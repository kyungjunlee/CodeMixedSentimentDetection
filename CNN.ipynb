{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To get an ID of an available GPU\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "\n",
    "ACCEPTABLE_AVAILABLE_MEMORY = 11167\n",
    "\n",
    "# https://github.com/yselivonchyk/TensorFlow_DCIGN/blob/master/utils.py\n",
    "def _output_to_list(output):\n",
    "  return output.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "\n",
    "def get_idle_gpu(leave_unmasked=1, random=True):\n",
    "  try:\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(command.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    available_gpus = [i for i, x in enumerate(memory_free_values) if x > ACCEPTABLE_AVAILABLE_MEMORY]\n",
    "\n",
    "    if len(available_gpus) <= leave_unmasked:\n",
    "      print('Found only %d usable GPUs in the system' % len(available_gpus))\n",
    "      return -1\n",
    "\n",
    "    if random:\n",
    "      available_gpus = np.asarray(available_gpus)\n",
    "      np.random.shuffle(available_gpus)\n",
    "\n",
    "    gpu_to_use = available_gpus[0]\n",
    "    print(\"Using GPU: \", gpu_to_use)\n",
    "    \n",
    "    return int(gpu_to_use)\n",
    "    \"\"\"\n",
    "    # update CUDA variable\n",
    "    gpus = available_gpus[:leave_unmasked]\n",
    "    setting = ','.join(map(str, gpus))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = setting\n",
    "    print('Left next %d GPU(s) unmasked: [%s] (from %s available)'\n",
    "          % (leave_unmasked, setting, str(available_gpus)))\n",
    "    \"\"\"\n",
    "  except FileNotFoundError as e:\n",
    "    print('\"nvidia-smi\" is probably not installed. GPUs are not masked')\n",
    "    print(e)\n",
    "    return -1\n",
    "  except sp.CalledProcessError as e:\n",
    "    print(\"Error on GPU masking:\\n\", e.output)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:  7\n",
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/train_conll_hinglish.csv'\n",
    "if torch.cuda.is_available():\n",
    "    gpu_id = \"cuda:{}\".format(get_idle_gpu(leave_unmasked=0))\n",
    "\n",
    "device = torch.device(gpu_id if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "tweet_max_len = 280\n",
    "\n",
    "import torchtext\n",
    "\n",
    "def label2int(label):\n",
    "    if label=='positive':\n",
    "        return 1\n",
    "    elif label=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def label2float(label):\n",
    "    if label=='positive':\n",
    "        return 1.\n",
    "    elif label=='negative':\n",
    "        return 0.\n",
    "    else:\n",
    "        return 2.\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
    "                                  include_lengths=True, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  fix_length=tweet_max_len, # 280 characters\n",
    "                                  lower=True, # lower characters\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "label_field = torchtext.data.Field(sequential=False,    # not a sequence\n",
    "                                   use_vocab=False,     # don't need to track vocabulary\n",
    "                                   is_target=True,\n",
    "                                   batch_first=True,\n",
    "                                   preprocessing=lambda x: label2int(x)) # convert text to 0 and 1\n",
    "\n",
    "fields = [('id', None),('text', text_field), ('label', label_field)]\n",
    "dataset = torchtext.data.TabularDataset(filename, # name of the file\n",
    "                                        \"tsv\",               # fields are separated by a tab\n",
    "                                        fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ adilnisarbutt pakistan ka ghra tauq he pakistan israel ko tasleem nahein kerta isko palestine kehta he- occupied palestine --- 0\n",
      "madarchod mulle ye mathura me nahi dikha tha jab mullo ne hindu ko iss liye mara ki vo lasse ki paise mag liye theâ€¦ https// t. co/ oxf8tr3bly --- 0\n",
      "@ narendramodi manya pradhan mantri mahoday shriman narendra modi ji pradhanmantri banne par hardik badhai tahe dilâ€¦ https// t. co/ prnomskkn1 --- 1\n",
      "@ atheist_ krishna jcb full trend me chal rahi aa --- 1\n",
      "@ abhisharsharma_@ ravishkumarblog loksabha me janta sirf modi ko vote de rahi thi na ki kisi mp or bjp ko without mâ€¦ https// t. co/ shtbwcb7fm --- 1\n",
      "@ noirnaveed@ angelahana6@ cricketworldcup bhosdike tum pechvade ki tatti hi rahoge bc --- 0\n",
      "love u bhaijan...â™¥â™¥ father+ son..# bharat# iambharat# bharatthiseid best pic from entire# promotions... mashallahâ€¦ https// t. co/ s2xhwu6lud --- 1\n",
      "@ manojgajjar111 tumhara pass abh deemagh hai nahi islea google ko apna deemagh banaya hua hai. har koi tumhari tarhâ€¦ https// t. co/ bxueug3xsn --- 0\n",
      "@ mahlogo_ nolo weni ankere o gae this weekendðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ --- 1\n",
      "@ aurangzeb_ aimim@ sachins40805591 lage raho mullo tumhre issi quran faad gyan ki kayal hain duniya allah bhi khusâ€¦ https// t co/ gha9dwnz6u --- 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(dataset[i].text, \"---\", dataset[i].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(dataset)\n",
    "# text_field.vocab.stoi\n",
    "# text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x7f4cae2b1fd0>\n"
     ]
    }
   ],
   "source": [
    "len(text_field.vocab)\n",
    "print(label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=32,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                           repeat=False, # repeat the iterator for multiple epochs\n",
    "                                           device=device)\n",
    "val_iter = torchtext.data.BucketIterator(val,\n",
    "                                         batch_size=32,\n",
    "                                         sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                         sort_within_batch=True,        # sort within each batch\n",
    "                                         repeat=False, # repeat the iterator for multiple epochs\n",
    "                                         device=device)\n",
    "test_iter = torchtext.data.BucketIterator(test,\n",
    "                                          batch_size=32,\n",
    "                                          sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                          sort_within_batch=True,        # sort within each batch\n",
    "                                          repeat=False, # repeat the iterator for multiple epochs\n",
    "                                          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, batch in enumerate(train_iter):\\n    if i >= 2:\\n        break\\n    print(batch.text)\\n#     print(batch.text[0].shape)\\n    print(batch.label)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(batch.text)\n",
    "#     print(batch.text[0].shape)\n",
    "    print(batch.label)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sklearn\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.svm import SVC\\nimport os\\n\\nINPUT_PATH = \"data/train_conll_spanglish.csv\"\\nMAX_TWEET = 280\\n\\nchar_to_ind = {}\\nind_to_char = {}\\n\\nchar_to_ind.update({\"UNK\":0})\\nind_to_char.update({0:\"UNK\"})\\n\\ncount = 1\\n\\nwith open(INPUT_PATH, \\'r\\') as f:\\n    for line in f:\\n        for char in line.split(\\'\\t\\')[1]:\\n            if char.lower() not in char_to_ind:\\n                char_to_ind.update({char.lower():count})\\n                ind_to_char.update({count:char.lower()})\\n                count += 1\\n\\n#print(char_to_ind)\\n#print(ind_to_char)\\n\\nn_letters = len(char_to_ind)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Another version of preprocessing data\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "INPUT_PATH = \"data/train_conll_spanglish.csv\"\n",
    "MAX_TWEET = 280\n",
    "\n",
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "\n",
    "char_to_ind.update({\"UNK\":0})\n",
    "ind_to_char.update({0:\"UNK\"})\n",
    "\n",
    "count = 1\n",
    "\n",
    "with open(INPUT_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        for char in line.split('\\t')[1]:\n",
    "            if char.lower() not in char_to_ind:\n",
    "                char_to_ind.update({char.lower():count})\n",
    "                ind_to_char.update({count:char.lower()})\n",
    "                count += 1\n",
    "\n",
    "#print(char_to_ind)\n",
    "#print(ind_to_char)\n",
    "\n",
    "n_letters = len(char_to_ind)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef letterToTensor(letter, n_letters):\\n    tensor = torch.zeros(1, n_letters)\\n    tensor[0][char_to_ind[letter]] = 1\\n    return tensor\\n\\ndef lineToTensor(line, n_letters):\\n    tensor = torch.zeros(len(line), n_letters)\\n    for li, letter in enumerate(line):\\n        tensor[li][char_to_ind[letter]] = 1\\n    return tensor\\n\\ndef batchToTensor(batch, n_letters):\\n    tensor = torch.zeros(len(batch),MAX_TWEET,n_letters)\\n    for sentence, line in enumerate(batch):\\n        for li, letter in enumerate(line):\\n            tensor[sentence][li][char_to_ind[letter.lower()]] = 1\\n    return tensor\\n\\n\\n#print(letterToTensor('o'))\\nprint(lineToTensor('hello how are tou', n_letters).shape)\\nprint(batchToTensor(['hello friend', 'linear svm is better'], n_letters))\\nprint(batchToTensor(['hello friend', 'linear svm is better'], n_letters).shape)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def letterToTensor(letter, n_letters):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][char_to_ind[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def lineToTensor(line, n_letters):\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][char_to_ind[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def batchToTensor(batch, n_letters):\n",
    "    tensor = torch.zeros(len(batch),MAX_TWEET,n_letters)\n",
    "    for sentence, line in enumerate(batch):\n",
    "        for li, letter in enumerate(line):\n",
    "            tensor[sentence][li][char_to_ind[letter.lower()]] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#print(letterToTensor('o'))\n",
    "print(lineToTensor('hello how are tou', n_letters).shape)\n",
    "print(batchToTensor(['hello friend', 'linear svm is better'], n_letters))\n",
    "print(batchToTensor(['hello friend', 'linear svm is better'], n_letters).shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainpath = INPUT_PATH\\ntrain = pd.read_csv(trainpath, sep=\\'\\\\t\\', names=[\"ID\",\"SENTENCE\",\"LABEL\"])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trainpath = INPUT_PATH\n",
    "train = pd.read_csv(trainpath, sep='\\\\t', names=[\"ID\",\"SENTENCE\",\"LABEL\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(train['SENTENCE'][0].lower())\\ntrain_char_features = batchToTensor(train['SENTENCE'], n_letters)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(train['SENTENCE'][0].lower())\n",
    "train_char_features = batchToTensor(train['SENTENCE'], n_letters)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(train_char_features.shape)\\nfor in_tensor in train_char_features:\\n    print(in_tensor.shape)\\n    break\\n# train_char_features[0].shape\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(train_char_features.shape)\n",
    "for in_tensor in train_char_features:\n",
    "    print(in_tensor.shape)\n",
    "    break\n",
    "# train_char_features[0].shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextShallowCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN implementation based on\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 conv0_f_nums, conv0_f_sizes,\n",
    "                 output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # one-hot vector, https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/11\n",
    "#         self.embedding.weight.data = torch.eye(vocab_size)\n",
    "        # make embedding untrainable\n",
    "#         self.embedding.weight.requires_grad=False\n",
    "        # first convolutional layer (three layers)\n",
    "        self.conv_0 = nn.ModuleList([\n",
    "                nn.Conv2d(in_channels = 1,\n",
    "                          out_channels = conv0_f_nums,\n",
    "                          kernel_size = (fs, embed_dim))\n",
    "                for fs in conv0_f_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(conv0_f_sizes) * conv0_f_nums, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text = (tensor of input, tensor of input length)\n",
    "#         print(text)\n",
    "        # convert input to embeddings\n",
    "        in_data = text[0]\n",
    "        # in_data = [batch_size, sentence_length]\n",
    "        embedded = self.embedding(in_data)\n",
    "#         print(embedded)\n",
    "        # embedded = [batch_size, sentence_length, embedding_dimension]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded = [batch_size, 1, sentence_length, embedding_dimension]\n",
    "        conved_0 = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_0]\n",
    "#         for each in conved_0:\n",
    "#             print(each.shape)        \n",
    "\n",
    "        cnn_output = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved_0]\n",
    "        \n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "#         cat = self.dropout(torch.cat(cnn_output, dim=1))\n",
    "        cat = torch.cat(cnn_output, dim=1)\n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDeepCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN implementation based on\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 conv0_f_nums, conv0_f_sizes,\n",
    "                 conv1_f_nums, conv1_f_sizes,\n",
    "                 output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # one-hot vector, https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/11\n",
    "#         self.embedding.weight.data = torch.eye(vocab_size)\n",
    "        # make embedding untrainable\n",
    "#         self.embedding.weight.requires_grad=False\n",
    "        # first convolutional layer (three layers)\n",
    "        self.conv_0 = nn.ModuleList([\n",
    "                nn.Conv2d(in_channels = 1,\n",
    "                          out_channels = conv0_f_nums,\n",
    "                          kernel_size = (fs, embed_dim))\n",
    "                for fs in conv0_f_sizes\n",
    "        ])\n",
    "        conv_0_out_dims = [280 - fs -1 for fs in conv0_f_sizes]\n",
    "        self.conv_1 = nn.ModuleList([\n",
    "                nn.Conv1d(in_channels = conv0_f_nums,\n",
    "                          out_channels = conv1_f_nums,\n",
    "                          kernel_size = fs)\n",
    "                for fs in conv1_f_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(conv1_f_sizes) * conv1_f_nums, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text = (tensor of input, tensor of input length)\n",
    "#         print(text)\n",
    "        # convert input to embeddings\n",
    "        in_data = text[0]\n",
    "        # in_data = [batch_size, sentence_length]\n",
    "        embedded = self.embedding(in_data)\n",
    "#         print(embedded)\n",
    "        # embedded = [batch_size, sentence_length, embedding_dimension]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded = [batch_size, 1, sentence_length, embedding_dimension]\n",
    "        conved_0 = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_0]\n",
    "#         for each in conved_0:\n",
    "#             print(each.shape)        \n",
    "\n",
    "        print(each.shape for each in pooled_0)\n",
    "        conved_1 = [F.relu(conv1(conv0)) for conv1, conv0 in zip(self.conv_1, conved_0)]\n",
    "        for each in conved_1:\n",
    "            print(each.shape)\n",
    "        # pooled output\n",
    "        cnn_output = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved_1]\n",
    "        \n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "#         cat = self.dropout(torch.cat(cnn_output, dim=1))\n",
    "        cat = torch.cat(cnn_output, dim=1)\n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_char_features = [num_of_tweets, max. length of each tweet, embedding_size]\n",
    "\"\"\"\n",
    "num_features = list(train_char_features.shape)\n",
    "print(num_features)\n",
    "max_tweet_length = num_features[1]\n",
    "embedding_dim = num_features[2] # vocab_size\n",
    "\"\"\"\n",
    "input_dim = embedding_dim = len(text_field.vocab)\n",
    "conv0_filter_sizes = [3, 4, 5, 6] # like character 3-gram, 4-gram, 5-gram, 6-gram\n",
    "conv0_filter_nums = 5 # number of filters\n",
    "conv1_filter_sizes = [3, 3, 3, 3]\n",
    "conv1_filter_nums = 5\n",
    "output_dim = 3\n",
    "dropout = 0.5\n",
    "\n",
    "model = TextShallowCNN(input_dim, embedding_dim,\n",
    "                conv0_filter_nums, conv0_filter_sizes,\n",
    "                output_dim, dropout)\n",
    "\n",
    "# model = TextDeepCNN(input_dim, embedding_dim,\n",
    "#                 conv0_filter_nums, conv0_filter_sizes,\n",
    "#                 conv1_filter_nums, conv1_filter_sizes,\n",
    "#                 output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 889194 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# checking the parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('The model has {} trainable parameters'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, labels):\n",
    "    correct, total = 0, 0\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "#     print(predicted, labels)\n",
    "#     print(predicted.shape, labels.shape)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "#         print(predictions, predictions.shape)\n",
    "#         print(batch.label, batch.label.shape)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = get_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \"\"\"\n",
    "        if i % 10 == 0:\n",
    "            print(\"batch: {}, loss: {}, acc: {}\".format(i, loss, acc*100))\n",
    "        \"\"\"\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = get_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.5124360588735515 | Train Acc: 44.234645265318086%\n",
      "\tVal. Loss: 1.2832356194655101 |  Val. Acc: 38.93952546296296%\n",
      "Epoch: 2 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9800451957770576 | Train Acc: 49.89097771914394%\n",
      "\tVal. Loss: 0.9699447515110174 |  Val. Acc: 49.660011574074076%\n",
      "Epoch: 3 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9587001022059559 | Train Acc: 52.0100410436822%\n",
      "\tVal. Loss: 0.9814755531648794 |  Val. Acc: 50.86082175925925%\n",
      "Epoch: 4 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9556486954160605 | Train Acc: 51.52631193198476%\n",
      "\tVal. Loss: 0.9893438580135504 |  Val. Acc: 50.47743055555556%\n",
      "Epoch: 5 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9802465331900403 | Train Acc: 50.950967458223396%\n",
      "\tVal. Loss: 1.0503458008170128 |  Val. Acc: 48.90769675925925%\n",
      "Epoch: 6 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9924766295503501 | Train Acc: 49.17362943418352%\n",
      "\tVal. Loss: 1.0863170213997364 |  Val. Acc: 45.20399305555556%\n",
      "Epoch: 7 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0072979236655626 | Train Acc: 47.901092055115804%\n",
      "\tVal. Loss: 1.1297076269984245 |  Val. Acc: 44.83506944444445%\n",
      "Epoch: 8 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0130920257606104 | Train Acc: 47.60700674289065%\n",
      "\tVal. Loss: 1.1774572792152564 |  Val. Acc: 43.800636574074076%\n",
      "Epoch: 9 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.069575590005336 | Train Acc: 44.44352829082381%\n",
      "\tVal. Loss: 1.1449298957983653 |  Val. Acc: 42.715567129629626%\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0455904305767572 | Train Acc: 44.2841175608326%\n",
      "\tVal. Loss: 1.2210041706760724 |  Val. Acc: 42.93981481481482%\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.08610771073515 | Train Acc: 42.75047639988273%\n",
      "\tVal. Loss: 1.303997952491045 |  Val. Acc: 42.324942129629626%\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0524994790711315 | Train Acc: 43.65013925535033%\n",
      "\tVal. Loss: 1.370793528854847 |  Val. Acc: 43.23640046296296%\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0540716152707001 | Train Acc: 44.21448988566403%\n",
      "\tVal. Loss: 1.4946641251444817 |  Val. Acc: 37.8978587962963%\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.1428319674997658 | Train Acc: 42.51136030489592%\n",
      "\tVal. Loss: 1.379397672911485 |  Val. Acc: 39.29398148148148%\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.090648067972591 | Train Acc: 43.00608326004104%\n",
      "\tVal. Loss: 1.3227394682665665 |  Val. Acc: 43.72106481481482%\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0533048946184345 | Train Acc: 43.35971855760774%\n",
      "\tVal. Loss: 1.582193021972974 |  Val. Acc: 36.88512731481482%\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0630341963277328 | Train Acc: 42.15589270008795%\n",
      "\tVal. Loss: 1.644880640010039 |  Val. Acc: 43.72106481481482%\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.1399274724769088 | Train Acc: 39.737247141600704%\n",
      "\tVal. Loss: 2.0090776905417442 |  Val. Acc: 37.637442129629626%\n",
      "Epoch: 19 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0870124067039793 | Train Acc: 40.614006156552335%\n",
      "\tVal. Loss: 1.7836426831781864 |  Val. Acc: 40.603298611111114%\n",
      "Epoch: 20 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0616295595596836 | Train Acc: 41.43121518616241%\n",
      "\tVal. Loss: 1.7741307069857915 |  Val. Acc: 39.749710648148145%\n",
      "Epoch: 21 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0912870925145917 | Train Acc: 40.66347845206684%\n",
      "\tVal. Loss: 1.673012802998225 |  Val. Acc: 37.051504629629626%\n",
      "Epoch: 22 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0877371979263024 | Train Acc: 38.216432131339786%\n",
      "\tVal. Loss: 1.541548877954483 |  Val. Acc: 37.18171296296296%\n",
      "Epoch: 23 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.106939215458792 | Train Acc: 38.199941366168275%\n",
      "\tVal. Loss: 3.032723401983579 |  Val. Acc: 37.28298611111111%\n",
      "Epoch: 24 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.103383835355965 | Train Acc: 38.15413368513632%\n",
      "\tVal. Loss: 2.573834595580896 |  Val. Acc: 36.98640046296296%\n",
      "Epoch: 25 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.1101635604860915 | Train Acc: 38.16146291410144%\n",
      "\tVal. Loss: 2.652811147272587 |  Val. Acc: 36.820023148148145%\n",
      "Epoch: 26 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0854532402549382 | Train Acc: 38.0341175608326%\n",
      "\tVal. Loss: 2.880361944437027 |  Val. Acc: 36.95023148148148%\n",
      "Epoch: 27 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0972362959605093 | Train Acc: 38.2778144239226%\n",
      "\tVal. Loss: 2.881528690457344 |  Val. Acc: 37.051504629629626%\n",
      "Epoch: 28 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.077724143508871 | Train Acc: 38.36942978598651%\n",
      "\tVal. Loss: 3.0286258459091187 |  Val. Acc: 37.051504629629626%\n",
      "Epoch: 29 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.1039485899942847 | Train Acc: 38.37767516857227%\n",
      "\tVal. Loss: 2.6163373179733753 |  Val. Acc: 37.08043981481482%\n",
      "Epoch: 30 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.1199935402593386 | Train Acc: 37.94708296687189%\n",
      "\tVal. Loss: 2.7039877846837044 |  Val. Acc: 37.1166087962963%\n",
      "Epoch: 31 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0845957989743014 | Train Acc: 38.024956024626206%\n",
      "\tVal. Loss: 2.6273499007026353 |  Val. Acc: 37.1166087962963%\n",
      "Epoch: 32 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.093398721520064 | Train Acc: 38.22467751392553%\n",
      "\tVal. Loss: 2.26585711290439 |  Val. Acc: 37.246817129629626%\n",
      "Epoch: 33 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0814035731443943 | Train Acc: 38.19902521254764%\n",
      "\tVal. Loss: 2.3028911277651787 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 34 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0836249868284744 | Train Acc: 38.26590442685429%\n",
      "\tVal. Loss: 2.2324309361477694 |  Val. Acc: 37.478298611111114%\n",
      "Epoch: 35 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0811021545002517 | Train Acc: 38.2026898270302%\n",
      "\tVal. Loss: 2.3769000669320426 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 36 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0793051617441203 | Train Acc: 38.25307827616535%\n",
      "\tVal. Loss: 2.3764357504745326 |  Val. Acc: 37.478298611111114%\n",
      "Epoch: 37 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0794579438609624 | Train Acc: 38.00113603048959%\n",
      "\tVal. Loss: 2.377192445099354 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 38 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0793794497336435 | Train Acc: 38.27414980944005%\n",
      "\tVal. Loss: 2.3771108215053878 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 39 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0792687378017758 | Train Acc: 38.23200674289065%\n",
      "\tVal. Loss: 2.3774978605409465 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 40 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0792171193930593 | Train Acc: 38.27414980944005%\n",
      "\tVal. Loss: 2.376754472653071 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 41 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0796048373848908 | Train Acc: 38.25307827616535%\n",
      "\tVal. Loss: 2.3772914359966912 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 42 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.079633739975952 | Train Acc: 38.253078276165354%\n",
      "\tVal. Loss: 2.377933129668236 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 43 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0795791215192045 | Train Acc: 38.31629287598945%\n",
      "\tVal. Loss: 2.376695640385151 |  Val. Acc: 37.47829861111111%\n",
      "Epoch: 44 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0793888968339382 | Train Acc: 38.25307827616535%\n",
      "\tVal. Loss: 2.3770769958694777 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 45 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0788565690095946 | Train Acc: 38.27414980944005%\n",
      "\tVal. Loss: 2.377154049774011 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 46 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0795287387666728 | Train Acc: 38.29522134271475%\n",
      "\tVal. Loss: 2.376447405666113 |  Val. Acc: 37.478298611111114%\n",
      "Epoch: 47 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.079482826991572 | Train Acc: 38.12939753737907%\n",
      "\tVal. Loss: 2.377389003833135 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 48 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0790094819735725 | Train Acc: 37.96082527118147%\n",
      "\tVal. Loss: 2.3763467334210873 |  Val. Acc: 37.47829861111111%\n",
      "Epoch: 49 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0791848150910048 | Train Acc: 38.29522134271475%\n",
      "\tVal. Loss: 2.377385408927997 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 50 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0795591253403938 | Train Acc: 38.27414980944005%\n",
      "\tVal. Loss: 2.3770149586101375 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 51 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0792219140284294 | Train Acc: 38.23200674289065%\n",
      "\tVal. Loss: 2.3770782314240932 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 52 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0795609219722193 | Train Acc: 38.06801524479624%\n",
      "\tVal. Loss: 2.37747123837471 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 53 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0792661685113227 | Train Acc: 38.27414980944005%\n",
      "\tVal. Loss: 2.376901414245367 |  Val. Acc: 37.14554398148148%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0795943096948488 | Train Acc: 38.25307827616535%\n",
      "\tVal. Loss: 2.376678249488274 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 55 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0797695110215362 | Train Acc: 38.23200674289065%\n",
      "\tVal. Loss: 2.3771719709038734 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 56 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0790884619023366 | Train Acc: 38.253078276165354%\n",
      "\tVal. Loss: 2.3773137057820954 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 57 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0792842262023987 | Train Acc: 38.25307827616535%\n",
      "\tVal. Loss: 2.378100593884786 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 58 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0793218601662125 | Train Acc: 38.27414980944005%\n",
      "\tVal. Loss: 2.3765408247709274 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 59 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0794609612084944 | Train Acc: 38.25307827616535%\n",
      "\tVal. Loss: 2.3782654826839766 |  Val. Acc: 37.14554398148148%\n",
      "Epoch: 60 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.079507201831385 | Train Acc: 38.253078276165354%\n",
      "\tVal. Loss: 2.3767969297866025 |  Val. Acc: 37.47829861111111%\n",
      "Epoch: 61 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0794038575997769 | Train Acc: 38.142223688068015%\n",
      "\tVal. Loss: 2.377419592191776 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 62 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0797376778949848 | Train Acc: 38.210935209615954%\n",
      "\tVal. Loss: 2.3766099313894906 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 63 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0795625594486347 | Train Acc: 38.253078276165354%\n",
      "\tVal. Loss: 2.3763833617170653 |  Val. Acc: 37.478298611111114%\n",
      "Epoch: 64 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.079670029296724 | Train Acc: 38.23200674289065%\n",
      "\tVal. Loss: 2.3767672230799994 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 65 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0794388250813949 | Train Acc: 38.29522134271475%\n",
      "\tVal. Loss: 2.3789558534820876 |  Val. Acc: 37.3119212962963%\n",
      "Epoch: 66 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0794814326517814 | Train Acc: 38.29522134271475%\n",
      "\tVal. Loss: 2.3773043851057687 |  Val. Acc: 37.3119212962963%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print('Epoch: {} | Epoch Time: {}m {}s'.format(epoch+1, epoch_mins, epoch_secs))\n",
    "    print('\\tTrain Loss: {} | Train Acc: {}%'.format(train_loss, train_acc*100))\n",
    "    print('\\tVal. Loss: {} |  Val. Acc: {}%'.format(valid_loss, valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
