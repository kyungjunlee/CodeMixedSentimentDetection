{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To get an ID of an available GPU\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "\n",
    "ACCEPTABLE_AVAILABLE_MEMORY = 11167\n",
    "\n",
    "# https://github.com/yselivonchyk/TensorFlow_DCIGN/blob/master/utils.py\n",
    "def _output_to_list(output):\n",
    "  return output.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "\n",
    "def get_idle_gpu(leave_unmasked=1, random=True):\n",
    "  try:\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(command.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    available_gpus = [i for i, x in enumerate(memory_free_values) if x > ACCEPTABLE_AVAILABLE_MEMORY]\n",
    "\n",
    "    if len(available_gpus) <= leave_unmasked:\n",
    "      print('Found only %d usable GPUs in the system' % len(available_gpus))\n",
    "      return -1\n",
    "\n",
    "    if random:\n",
    "      available_gpus = np.asarray(available_gpus)\n",
    "      np.random.shuffle(available_gpus)\n",
    "\n",
    "    gpu_to_use = available_gpus[0]\n",
    "    print(\"Using GPU: \", gpu_to_use)\n",
    "    \n",
    "    return int(gpu_to_use)\n",
    "    \"\"\"\n",
    "    # update CUDA variable\n",
    "    gpus = available_gpus[:leave_unmasked]\n",
    "    setting = ','.join(map(str, gpus))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = setting\n",
    "    print('Left next %d GPU(s) unmasked: [%s] (from %s available)'\n",
    "          % (leave_unmasked, setting, str(available_gpus)))\n",
    "    \"\"\"\n",
    "  except FileNotFoundError as e:\n",
    "    print('\"nvidia-smi\" is probably not installed. GPUs are not masked')\n",
    "    print(e)\n",
    "    return -1\n",
    "  except sp.CalledProcessError as e:\n",
    "    print(\"Error on GPU masking:\\n\", e.output)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "is_character_level = True\n",
    "\n",
    "if is_character_level:\n",
    "    # character level\n",
    "    tweet_max_len = 280\n",
    "    tokenizer = lambda x: x\n",
    "else:\n",
    "    # word level\n",
    "    tweet_max_len = 200\n",
    "    tokenizer = torchtext.data.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:  1\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/train_conll_spanglish.csv'\n",
    "if torch.cuda.is_available():\n",
    "    gpu_id = \"cuda:{}\".format(get_idle_gpu(leave_unmasked=0))\n",
    "\n",
    "device = torch.device(gpu_id if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def label2int(label):\n",
    "    if label=='positive':\n",
    "        return 1\n",
    "    elif label=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def label2float(label):\n",
    "    if label=='positive':\n",
    "        return 1.\n",
    "    elif label=='negative':\n",
    "        return 0.\n",
    "    else:\n",
    "        return 2.\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=tokenizer, # because are building a character-RNN\n",
    "                                  include_lengths=True, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  fix_length=tweet_max_len, # 280 characters if character-level; else 200\n",
    "                                  lower=True, # lower characters\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "label_field = torchtext.data.Field(sequential=False,    # not a sequence\n",
    "                                   use_vocab=False,     # don't need to track vocabulary\n",
    "                                   is_target=True,\n",
    "                                   batch_first=True,\n",
    "                                   preprocessing=lambda x: label2int(x)) # convert text to 0 and 1\n",
    "\n",
    "fields = [('id', None),('text', text_field), ('label', label_field)]\n",
    "dataset = torchtext.data.TabularDataset(filename, # name of the file\n",
    "                                        \"tsv\",               # fields are separated by a tab\n",
    "                                        fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so that means tomorrow cruda segura lol --- 1\n",
      "tonight peda segura --- 2\n",
      "eres tan mala vieja bruja interesada#jamming --- 0\n",
      "yo kiero pretzels lol --- 2\n",
      "fuck that ni ke el me vaya a mantener toda la vida lol --- 0\n",
      "i always tell my dad ke me kiero kasar con una vieja rika and me regaÃ±a telling me ke no sea interesada ha --- 0\n",
      "ke me compre un carrito pa irme con mis friends and party lol --- 2\n",
      "why can i just find a rich bitch ke me mantenga y ya ha --- 2\n",
      "since i started working ya ni disfruto la vida lol --- 0\n",
      "my dad me regano cuzs i was telling that to my brother and lo andaba molestando lol --- 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(dataset[i].text, \"---\", dataset[i].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(dataset)\n",
    "# text_field.vocab.stoi\n",
    "# text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n",
      "<unk>\n",
      "<pad>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(len(text_field.vocab))\n",
    "\"\"\"\n",
    "i = 0\n",
    "for word in text_field.vocab.itos:\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(word)\n",
    "    i += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36140 36140 36140\n"
     ]
    }
   ],
   "source": [
    "if not is_character_level:\n",
    "    from torchnlp.word_to_vector import FastText\n",
    "    # create weights matrix for word level\n",
    "    lang1 = \"en\"\n",
    "    lang2 = \"es\"\n",
    "    lang3 = \"hi\"\n",
    "    lang1_vectors = FastText(language=lang1, aligned=True, cache=\"wiki.{}.align.vec\".format(lang1))\n",
    "    lang2_vectors = FastText(language=lang2, aligned=True, cache=\"wiki.{}.align.vec\".format(lang2))\n",
    "    lang3_vectors = FastText(language=lang3, aligned=True, cache=\"wiki.{}.align.vec\".format(lang3))\n",
    "    en_embeddings = [lang1_vectors[word] for word in text_field.vocab.itos]\n",
    "    es_embeddings = [lang2_vectors[word] for word in text_field.vocab.itos]\n",
    "    hi_embeddings = [lang3_vectors[word] for word in text_field.vocab.itos]\n",
    "    print(len(en_embeddings), len(es_embeddings), len(hi_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=32,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                           repeat=False, # repeat the iterator for multiple epochs\n",
    "                                           device=device)\n",
    "val_iter = torchtext.data.BucketIterator(val,\n",
    "                                         batch_size=32,\n",
    "                                         sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                         sort_within_batch=True,        # sort within each batch\n",
    "                                         repeat=False, # repeat the iterator for multiple epochs\n",
    "                                         device=device)\n",
    "test_iter = torchtext.data.BucketIterator(test,\n",
    "                                          batch_size=32,\n",
    "                                          sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                          sort_within_batch=True,        # sort within each batch\n",
    "                                          repeat=False, # repeat the iterator for multiple epochs\n",
    "                                          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, batch in enumerate(train_iter):\\n    if i >= 2:\\n        break\\n    print(batch.text)\\n#     print(batch.text[0].shape)\\n    print(batch.label)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(batch.text)\n",
    "#     print(batch.text[0].shape)\n",
    "    print(batch.label)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sklearn\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.svm import SVC\\nimport os\\n\\nINPUT_PATH = \"data/train_conll_spanglish.csv\"\\nMAX_TWEET = 280\\n\\nchar_to_ind = {}\\nind_to_char = {}\\n\\nchar_to_ind.update({\"UNK\":0})\\nind_to_char.update({0:\"UNK\"})\\n\\ncount = 1\\n\\nwith open(INPUT_PATH, \\'r\\') as f:\\n    for line in f:\\n        for char in line.split(\\'\\t\\')[1]:\\n            if char.lower() not in char_to_ind:\\n                char_to_ind.update({char.lower():count})\\n                ind_to_char.update({count:char.lower()})\\n                count += 1\\n\\n#print(char_to_ind)\\n#print(ind_to_char)\\n\\nn_letters = len(char_to_ind)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Another version of preprocessing data\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "INPUT_PATH = \"data/train_conll_spanglish.csv\"\n",
    "MAX_TWEET = 280\n",
    "\n",
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "\n",
    "char_to_ind.update({\"UNK\":0})\n",
    "ind_to_char.update({0:\"UNK\"})\n",
    "\n",
    "count = 1\n",
    "\n",
    "with open(INPUT_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        for char in line.split('\\t')[1]:\n",
    "            if char.lower() not in char_to_ind:\n",
    "                char_to_ind.update({char.lower():count})\n",
    "                ind_to_char.update({count:char.lower()})\n",
    "                count += 1\n",
    "\n",
    "#print(char_to_ind)\n",
    "#print(ind_to_char)\n",
    "\n",
    "n_letters = len(char_to_ind)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef letterToTensor(letter, n_letters):\\n    tensor = torch.zeros(1, n_letters)\\n    tensor[0][char_to_ind[letter]] = 1\\n    return tensor\\n\\ndef lineToTensor(line, n_letters):\\n    tensor = torch.zeros(len(line), n_letters)\\n    for li, letter in enumerate(line):\\n        tensor[li][char_to_ind[letter]] = 1\\n    return tensor\\n\\ndef batchToTensor(batch, n_letters):\\n    tensor = torch.zeros(len(batch),MAX_TWEET,n_letters)\\n    for sentence, line in enumerate(batch):\\n        for li, letter in enumerate(line):\\n            tensor[sentence][li][char_to_ind[letter.lower()]] = 1\\n    return tensor\\n\\n\\n#print(letterToTensor('o'))\\nprint(lineToTensor('hello how are tou', n_letters).shape)\\nprint(batchToTensor(['hello friend', 'linear svm is better'], n_letters))\\nprint(batchToTensor(['hello friend', 'linear svm is better'], n_letters).shape)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def letterToTensor(letter, n_letters):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][char_to_ind[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def lineToTensor(line, n_letters):\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][char_to_ind[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def batchToTensor(batch, n_letters):\n",
    "    tensor = torch.zeros(len(batch),MAX_TWEET,n_letters)\n",
    "    for sentence, line in enumerate(batch):\n",
    "        for li, letter in enumerate(line):\n",
    "            tensor[sentence][li][char_to_ind[letter.lower()]] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#print(letterToTensor('o'))\n",
    "print(lineToTensor('hello how are tou', n_letters).shape)\n",
    "print(batchToTensor(['hello friend', 'linear svm is better'], n_letters))\n",
    "print(batchToTensor(['hello friend', 'linear svm is better'], n_letters).shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainpath = INPUT_PATH\\ntrain = pd.read_csv(trainpath, sep=\\'\\\\t\\', names=[\"ID\",\"SENTENCE\",\"LABEL\"])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trainpath = INPUT_PATH\n",
    "train = pd.read_csv(trainpath, sep='\\\\t', names=[\"ID\",\"SENTENCE\",\"LABEL\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(train['SENTENCE'][0].lower())\\ntrain_char_features = batchToTensor(train['SENTENCE'], n_letters)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(train['SENTENCE'][0].lower())\n",
    "train_char_features = batchToTensor(train['SENTENCE'], n_letters)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(train_char_features.shape)\\nfor in_tensor in train_char_features:\\n    print(in_tensor.shape)\\n    break\\n# train_char_features[0].shape\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(train_char_features.shape)\n",
    "for in_tensor in train_char_features:\n",
    "    print(in_tensor.shape)\n",
    "    break\n",
    "# train_char_features[0].shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharShallowCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN implementation based on\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 conv0_f_nums, conv0_f_sizes,\n",
    "                 output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # one-hot vector, https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/11\n",
    "        self.embedding.weight.data = torch.eye(vocab_size)\n",
    "        # make embedding untrainable\n",
    "#         self.embedding.weight.requires_grad=False\n",
    "        # first convolutional layer (three layers)\n",
    "        self.conv_0 = nn.ModuleList([\n",
    "                nn.Conv2d(in_channels = 1,\n",
    "                          out_channels = conv0_f_nums,\n",
    "                          kernel_size = (fs, embed_dim))\n",
    "                for fs in conv0_f_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(conv0_f_sizes) * conv0_f_nums, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text = (tensor of input, tensor of input length)\n",
    "#         print(text)\n",
    "        # convert input to embeddings\n",
    "        in_data = text[0]\n",
    "        # in_data = [batch_size, sentence_length]\n",
    "        embedded = self.embedding(in_data)\n",
    "#         print(embedded)\n",
    "        # embedded = [batch_size, sentence_length, embedding_dimension]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded = [batch_size, 1, sentence_length, embedding_dimension]\n",
    "        conved_0 = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_0]\n",
    "#         for each in conved_0:\n",
    "#             print(each.shape)        \n",
    "\n",
    "        cnn_output = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved_0]\n",
    "        \n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "        cat = self.dropout(torch.cat(cnn_output, dim=1))\n",
    "#         cat = torch.cat(cnn_output, dim=1)\n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordShallowCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN implementation based on\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 conv0_f_nums, conv0_f_sizes,\n",
    "                 output_dim, dropout,\n",
    "                 lang1_weights=None, lang2_weights=None):\n",
    "        super().__init__()\n",
    "        self.lang1_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lang2_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # one-hot vector, https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/11\n",
    "        if lang1_weights is not None:\n",
    "            self.lang1_embedding.weight = nn.Parameter(lang1_weights)\n",
    "        if lang2_weights is not None:\n",
    "            self.lang2_embedding.weight = nn.Parameter(lang2_weights)\n",
    "        # make embedding untrainable\n",
    "        self.lang1_embedding.weight.requires_grad=False\n",
    "        self.lang2_embedding.weight.requires_grad=False\n",
    "        # first convolutional layer (three layers)\n",
    "        self.conv_0 = nn.ModuleList([\n",
    "                nn.Conv2d(in_channels = 2,\n",
    "                          out_channels = conv0_f_nums,\n",
    "                          kernel_size = (fs, embed_dim))\n",
    "                for fs in conv0_f_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(conv0_f_sizes) * conv0_f_nums, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text = (tensor of input, tensor of input length)\n",
    "#         print(text)\n",
    "        # convert input to embeddings\n",
    "        in_data = text[0]\n",
    "        # in_data = [batch_size, sentence_length]\n",
    "        lang1_embedded = self.lang1_embedding(in_data)\n",
    "        lang1_embedded = lang1_embedded.unsqueeze(1)\n",
    "        lang2_embedded = self.lang2_embedding(in_data)\n",
    "        lang2_embedded = lang2_embedded.unsqueeze(1)\n",
    "        # embedded = [batch_size, 2, sentence_length, embedding_dimension]\n",
    "        embedded = torch.cat((lang1_embedded, lang2_embedded), dim=1)\n",
    "#         print(embedded)\n",
    "        # embedded = [batch_size, sentence_length, embedding_dimension]\n",
    "#         embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        conved_0 = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_0]\n",
    "#         for each in conved_0:\n",
    "#             print(each.shape)        \n",
    "\n",
    "        cnn_output = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved_0]\n",
    "        \n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "        # prevent overfitting\n",
    "        cat = self.dropout(torch.cat(cnn_output, dim=1))\n",
    "#         cat = torch.cat(cnn_output, dim=1)\n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDeepCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN implementation based on\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 conv0_f_nums, conv0_f_sizes,\n",
    "                 conv1_f_nums, conv1_f_sizes,\n",
    "                 output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # one-hot vector, https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/11\n",
    "        self.embedding.weight.data = torch.eye(vocab_size)\n",
    "        # make embedding untrainable\n",
    "#         self.embedding.weight.requires_grad=False\n",
    "        # first convolutional layer (three layers)\n",
    "        self.conv_0 = nn.ModuleList([\n",
    "                nn.Conv2d(in_channels = 1,\n",
    "                          out_channels = conv0_f_nums,\n",
    "                          kernel_size = (fs, embed_dim))\n",
    "                for fs in conv0_f_sizes\n",
    "        ])\n",
    "        conv_0_out_dims = [280 - fs -1 for fs in conv0_f_sizes]\n",
    "        self.conv_1 = nn.ModuleList([\n",
    "                nn.Conv1d(in_channels = conv0_f_nums,\n",
    "                          out_channels = conv1_f_nums,\n",
    "                          kernel_size = fs)\n",
    "                for fs in conv1_f_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(conv1_f_sizes) * conv1_f_nums, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text = (tensor of input, tensor of input length)\n",
    "#         print(text)\n",
    "        # convert input to embeddings\n",
    "        in_data = text[0]\n",
    "        # in_data = [batch_size, sentence_length]\n",
    "        embedded = self.embedding(in_data)\n",
    "#         print(embedded)\n",
    "        # embedded = [batch_size, sentence_length, embedding_dimension]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded = [batch_size, 1, sentence_length, embedding_dimension]\n",
    "        conved_0 = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_0]\n",
    "#         for each in conved_0:\n",
    "#             print(each.shape)        \n",
    "\n",
    "#         print(each.shape for each in pooled_0)\n",
    "        conved_1 = [F.relu(conv1(conv0)) for conv1, conv0 in zip(self.conv_1, conved_0)]\n",
    "#         for each in conved_1:\n",
    "#             print(each.shape)\n",
    "        # pooled output\n",
    "        cnn_output = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved_1]\n",
    "        \n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "        cat = self.dropout(torch.cat(cnn_output, dim=1))\n",
    "#         cat = torch.cat(cnn_output, dim=1)\n",
    "#         cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDeepCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN implementation based on\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 conv0_f_nums, conv0_f_sizes,\n",
    "                 conv1_f_nums, conv1_f_sizes,\n",
    "                 output_dim, dropout,\n",
    "                 lang1_weights=None, lang2_weights=None):\n",
    "        super().__init__()\n",
    "        self.lang1_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lang2_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        if lang1_weights is not None:\n",
    "            self.lang1_embedding.weight = nn.Parameter(lang1_weights)\n",
    "        if lang2_weights is not None:\n",
    "            self.lang2_embedding.weight = nn.Parameter(lang2_weights)\n",
    "        # make embedding untrainable\n",
    "        self.lang1_embedding.weight.requires_grad=False\n",
    "        self.lang2_embedding.weight.requires_grad=False\n",
    "        # first convolutional layer (three layers)\n",
    "        self.conv_0 = nn.ModuleList([\n",
    "                nn.Conv2d(in_channels = 2,\n",
    "                          out_channels = conv0_f_nums,\n",
    "                          kernel_size = (fs, embed_dim))\n",
    "                for fs in conv0_f_sizes\n",
    "        ])\n",
    "        conv_0_out_dims = [280 - fs -1 for fs in conv0_f_sizes]\n",
    "        self.conv_1 = nn.ModuleList([\n",
    "                nn.Conv1d(in_channels = conv0_f_nums,\n",
    "                          out_channels = conv1_f_nums,\n",
    "                          kernel_size = fs)\n",
    "                for fs in conv1_f_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(conv1_f_sizes) * conv1_f_nums, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text = (tensor of input, tensor of input length)\n",
    "#         print(text)\n",
    "        # convert input to embeddings\n",
    "        in_data = text[0]\n",
    "        # in_data = [batch_size, sentence_length]\n",
    "        lang1_embedded = self.lang1_embedding(in_data)\n",
    "        lang1_embedded = lang1_embedded.unsqueeze(1)\n",
    "        lang2_embedded = self.lang2_embedding(in_data)\n",
    "        lang2_embedded = lang2_embedded.unsqueeze(1)\n",
    "        # embedded = [batch_size, 2, sentence_length, embedding_dimension]\n",
    "        embedded = torch.cat((lang1_embedded, lang2_embedded), dim=1)\n",
    "#         print(embedded)\n",
    "        conved_0 = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_0]\n",
    "#         for each in conved_0:\n",
    "#             print(each.shape)        \n",
    "\n",
    "        conved_1 = [F.relu(conv1(conv0)) for conv1, conv0 in zip(self.conv_1, conved_0)]\n",
    "#         for each in conved_1:\n",
    "#             print(each.shape)\n",
    "        # pooled output\n",
    "        cnn_output = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved_1]\n",
    "        \n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "        cat = self.dropout(torch.cat(cnn_output, dim=1))\n",
    "#         cat = torch.cat(cnn_output, dim=1)\n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_char_features = [num_of_tweets, max. length of each tweet, embedding_size]\n",
    "\"\"\"\n",
    "num_features = list(train_char_features.shape)\n",
    "print(num_features)\n",
    "max_tweet_length = num_features[1]\n",
    "embedding_dim = num_features[2] # vocab_size\n",
    "\"\"\"\n",
    "if is_character_level:\n",
    "    input_dim = embedding_dim = len(text_field.vocab)\n",
    "else:\n",
    "    # en_embeddings: English\n",
    "    # es_embeddings: Spanish\n",
    "    # hi_embeddings: Hindi\n",
    "    lang1_weights = torch.stack(en_embeddings)\n",
    "    lang2_weights = torch.stack(es_embeddings) if \"spanglish\" in filename else torch.stack(hi_embeddings)\n",
    "    print(lang1_weights.shape, lang2_weights.shape)\n",
    "    input_dim = lang1_weights.shape[0]\n",
    "    embedding_dim = lang1_weights.shape[1]\n",
    "    \n",
    "conv0_filter_sizes = [3, 4, 5, 6] # like character 3-gram, 4-gram, 5-gram, 6-gram\n",
    "conv0_filter_nums = 5 # number of filters\n",
    "conv1_filter_sizes = [3, 3, 3, 3]\n",
    "conv1_filter_nums = 5\n",
    "output_dim = 3\n",
    "dropout = 0.5\n",
    "\n",
    "is_shallow = False\n",
    "\n",
    "if is_shallow:\n",
    "    # shallow CNNs\n",
    "    if is_character_level:\n",
    "        model = CharShallowCNN(input_dim, embedding_dim,\n",
    "                               conv0_filter_nums, conv0_filter_sizes,\n",
    "                               output_dim, dropout)\n",
    "    else:\n",
    "        model = WordShallowCNN(input_dim, embedding_dim,\n",
    "                               conv0_filter_nums, conv0_filter_sizes,\n",
    "                               output_dim, dropout,\n",
    "                               lang1_weights=lang1_weights, lang2_weights=lang2_weights)\n",
    "else:\n",
    "    # deep CNNs --- two hidden layers as of now\n",
    "    if is_character_level:\n",
    "        model = CharDeepCNN(input_dim, embedding_dim,\n",
    "                            conv0_filter_nums, conv0_filter_sizes,\n",
    "                            conv1_filter_nums, conv1_filter_sizes,\n",
    "                            output_dim, dropout)\n",
    "    else:\n",
    "        model = WordDeepCNN(input_dim, embedding_dim,\n",
    "                            conv0_filter_nums, conv0_filter_sizes,\n",
    "                            conv1_filter_nums, conv1_filter_sizes,\n",
    "                            output_dim, dropout,\n",
    "                            lang1_weights=lang1_weights, lang2_weights=lang2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 358378 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# checking the parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('The model has {} trainable parameters'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def get_results(logits, labels):\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "    # move CUDA variables to CPU for numpy\n",
    "    preds = predicted.cpu().numpy()\n",
    "    gts = labels.cpu().numpy()\n",
    "    ret = {}\n",
    "    ret['accuracy'] = accuracy_score(gts, preds)\n",
    "    ret['precision'], ret['recall'], ret['f1'], _ = precision_recall_fscore_support(gts, preds, average='macro')\n",
    "    return ret\n",
    "    \n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "#     print(predicted, labels)\n",
    "#     print(predicted.shape, labels.shape)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "#         print(predictions, predictions.shape)\n",
    "#         print(batch.label, batch.label.shape)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        ret = get_results(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += ret['accuracy']\n",
    "        epoch_f1 += ret['f1']\n",
    "        epoch_prec += ret['precision']\n",
    "        epoch_recall += ret['recall']\n",
    "        \"\"\"\n",
    "        if i % 10 == 0:\n",
    "            print(\"batch: {}, loss: {}, acc: {}\".format(i, loss, acc*100))\n",
    "        \"\"\"\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
    "            epoch_f1 / len(iterator), epoch_prec / len(iterator), epoch_recall / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_f1 = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            ret = get_results(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += ret['accuracy']\n",
    "            epoch_f1 += ret['f1']\n",
    "            epoch_prec += ret['precision']\n",
    "            epoch_recall += ret['recall']\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
    "            epoch_f1 / len(iterator), epoch_prec / len(iterator), epoch_recall / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved into deepCharCNN_spanglish.pt\n",
      "Epoch: 1 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.0148 | Acc: 48.9083% | F1: 0.2479 | Prec.: 0.2325 | Recall: 0.3365\n",
      "\tVal. Loss: 0.9834 | Acc: 49.8005% | F1: 0.2202 | Prec.: 0.1660 | Recall: 0.3333\n",
      "Epoch: 2 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9788 | Acc: 50.1500% | F1: 0.2391 | Prec.: 0.2221 | Recall: 0.3419\n",
      "\tVal. Loss: 0.9615 | Acc: 50.5319% | F1: 0.2589 | Prec.: 0.2585 | Recall: 0.3579\n",
      "Epoch: 3 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9627 | Acc: 50.8917% | F1: 0.2620 | Prec.: 0.2708 | Recall: 0.3568\n",
      "\tVal. Loss: 0.9539 | Acc: 51.0543% | F1: 0.2775 | Prec.: 0.3039 | Recall: 0.3665\n",
      "Epoch: 4 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9531 | Acc: 50.7833% | F1: 0.2752 | Prec.: 0.2999 | Recall: 0.3631\n",
      "\tVal. Loss: 0.9491 | Acc: 52.1847% | F1: 0.3253 | Prec.: 0.3735 | Recall: 0.3996\n",
      "Epoch: 5 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9436 | Acc: 51.4167% | F1: 0.3067 | Prec.: 0.3607 | Recall: 0.3817\n",
      "\tVal. Loss: 0.9548 | Acc: 51.4533% | F1: 0.2883 | Prec.: 0.3363 | Recall: 0.3728\n",
      "Epoch: 6 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9387 | Acc: 51.5750% | F1: 0.3125 | Prec.: 0.3851 | Recall: 0.3845\n",
      "\tVal. Loss: 0.9505 | Acc: 51.8522% | F1: 0.3064 | Prec.: 0.3188 | Recall: 0.3867\n",
      "Epoch: 7 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9299 | Acc: 52.0500% | F1: 0.3234 | Prec.: 0.3976 | Recall: 0.3911\n",
      "\tVal. Loss: 0.9587 | Acc: 52.1467% | F1: 0.3195 | Prec.: 0.3687 | Recall: 0.3967\n",
      "Epoch: 8 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9316 | Acc: 52.1167% | F1: 0.3414 | Prec.: 0.4298 | Recall: 0.3985\n",
      "\tVal. Loss: 0.9577 | Acc: 52.3556% | F1: 0.3226 | Prec.: 0.3893 | Recall: 0.3956\n",
      "Epoch: 9 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9237 | Acc: 52.4000% | F1: 0.3425 | Prec.: 0.4386 | Recall: 0.4013\n",
      "\tVal. Loss: 0.9561 | Acc: 52.4411% | F1: 0.3419 | Prec.: 0.4100 | Recall: 0.4081\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.9194 | Acc: 52.6583% | F1: 0.3539 | Prec.: 0.4447 | Recall: 0.4053\n",
      "\tVal. Loss: 0.9606 | Acc: 50.9403% | F1: 0.3490 | Prec.: 0.4172 | Recall: 0.3968\n"
     ]
    }
   ],
   "source": [
    "is_spanglish = \"spanglish\" in filename\n",
    "model_name = \"shallow\" if is_shallow else \"deep\"\n",
    "model_name += \"CharCNN\" if is_character_level else \"WordCNN\"\n",
    "model_name += \"_spanglish\" if is_spanglish else \"_higlish\"\n",
    "model_name += \".pt\"\n",
    "print(\"Model will be saved into {}\".format(model_name))\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1, train_prec, train_recall = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_prec, valid_recall = evaluate(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # add losses for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print('Epoch: {} | Epoch Time: {}m {}s'.format(epoch+1, epoch_mins, epoch_secs))\n",
    "    print('\\tTrain Loss: {0:.4f} | Acc: {1:.4f}% | F1: {2:.4f} | Prec.: {3:.4f} | Recall: {4:.4f}'.format(train_loss, train_acc*100, train_f1, train_prec, train_recall))\n",
    "    print('\\tVal. Loss: {0:.4f} | Acc: {1:.4f}% | F1: {2:.4f} | Prec.: {3:.4f} | Recall: {4:.4f}'.format(valid_loss, valid_acc*100, valid_f1, valid_prec, valid_recall))\n",
    "    \n",
    "# testing time\n",
    "_, test_acc, test_f1, test_prec, test_recall = evaluate(model, test_iter, criterion)\n",
    "print('============================')\n",
    "print('Test results: Acc: {0:.4f}% | F1: {1:.4f} | Prec.: {2:.4f} | Recall: {3:.4f}'.format(test_acc*100, test_f1, test_prec, test_recall))\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
