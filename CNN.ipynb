{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To get an ID of an available GPU\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import subprocess as sp\n",
    "\n",
    "ACCEPTABLE_AVAILABLE_MEMORY = 11167\n",
    "\n",
    "# https://github.com/yselivonchyk/TensorFlow_DCIGN/blob/master/utils.py\n",
    "def _output_to_list(output):\n",
    "  return output.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "\n",
    "def get_idle_gpu(leave_unmasked=1, random=True):\n",
    "  try:\n",
    "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(command.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    available_gpus = [i for i, x in enumerate(memory_free_values) if x > ACCEPTABLE_AVAILABLE_MEMORY]\n",
    "\n",
    "    if len(available_gpus) <= leave_unmasked:\n",
    "      print('Found only %d usable GPUs in the system' % len(available_gpus))\n",
    "      return -1\n",
    "\n",
    "    if random:\n",
    "      available_gpus = np.asarray(available_gpus)\n",
    "      np.random.shuffle(available_gpus)\n",
    "\n",
    "    gpu_to_use = available_gpus[0]\n",
    "    print(\"Using GPU: \", gpu_to_use)\n",
    "    \n",
    "    return int(gpu_to_use)\n",
    "    \"\"\"\n",
    "    # update CUDA variable\n",
    "    gpus = available_gpus[:leave_unmasked]\n",
    "    setting = ','.join(map(str, gpus))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = setting\n",
    "    print('Left next %d GPU(s) unmasked: [%s] (from %s available)'\n",
    "          % (leave_unmasked, setting, str(available_gpus)))\n",
    "    \"\"\"\n",
    "  except FileNotFoundError as e:\n",
    "    print('\"nvidia-smi\" is probably not installed. GPUs are not masked')\n",
    "    print(e)\n",
    "    return -1\n",
    "  except sp.CalledProcessError as e:\n",
    "    print(\"Error on GPU masking:\\n\", e.output)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:  0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/train_conll_spanglish.csv'\n",
    "if torch.cuda.is_available():\n",
    "    gpu_id = \"cuda:{}\".format(get_idle_gpu(leave_unmasked=0))\n",
    "\n",
    "device = torch.device(gpu_id if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "import torchtext\n",
    "\n",
    "def label2int(label):\n",
    "    if label=='positive':\n",
    "        return 1\n",
    "    elif label=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def label2float(label):\n",
    "    if label=='positive':\n",
    "        return 1.\n",
    "    elif label=='negative':\n",
    "        return 0.\n",
    "    else:\n",
    "        return 2.\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
    "                                  include_lengths=True, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "label_field = torchtext.data.Field(sequential=False,    # not a sequence\n",
    "                                   use_vocab=False,     # don't need to track vocabulary\n",
    "                                   is_target=True,\n",
    "                                   batch_first=True,\n",
    "                                   preprocessing=lambda x: label2int(x)) # convert text to 0 and 1\n",
    "\n",
    "fields = [('id', None),('text', text_field), ('label', label_field)]\n",
    "dataset = torchtext.data.TabularDataset(filename, # name of the file\n",
    "                                        \"tsv\",               # fields are separated by a tab\n",
    "                                        fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So that means tomorrow cruda segura lol --- 1\n",
      "Tonight peda segura --- 2\n",
      "Eres tan mala vieja bruja interesada#jamming --- 0\n",
      "Yo kiero Pretzels lol --- 2\n",
      "Fuck that ni ke el me vaya a mantener toda la vida lol --- 0\n",
      "I always tell my dad ke me kiero kasar con una vieja rika and me regaÃ±a telling me ke no sea interesada ha --- 0\n",
      "Ke me compre un carrito pa irme con mis friends and party lol --- 2\n",
      "Why can I just find a rich bitch ke me mantenga y ya ha --- 2\n",
      "Since I started working ya ni disfruto la vida lol --- 0\n",
      "My dad me regano cuzs I was telling that to my brother and lo andaba molestando lol --- 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(dataset[i].text, \"---\", dataset[i].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(dataset)\n",
    "# text_field.vocab.stoi\n",
    "# text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.field.Field object at 0x7f079e364668>\n"
     ]
    }
   ],
   "source": [
    "len(text_field.vocab)\n",
    "print(label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=32,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                           repeat=False, # repeat the iterator for multiple epochs\n",
    "                                           device=device)\n",
    "val_iter = torchtext.data.BucketIterator(val,\n",
    "                                         batch_size=32,\n",
    "                                         sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                         sort_within_batch=True,        # sort within each batch\n",
    "                                         repeat=False, # repeat the iterator for multiple epochs\n",
    "                                         device=device)\n",
    "test_iter = torchtext.data.BucketIterator(test,\n",
    "                                          batch_size=32,\n",
    "                                          sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                          sort_within_batch=True,        # sort within each batch\n",
    "                                          repeat=False, # repeat the iterator for multiple epochs\n",
    "                                          device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, batch in enumerate(train_iter):\\n    if i >= 2:\\n        break\\n    print(batch.text)\\n#     print(batch.text[0].shape)\\n    print(batch.label)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(batch.text)\n",
    "#     print(batch.text[0].shape)\n",
    "    print(batch.label)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Another version of preprocessing data\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "INPUT_PATH = \"data/train_conll_spanglish.csv\"\n",
    "MAX_TWEET = 280\n",
    "\n",
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "\n",
    "char_to_ind.update({\"UNK\":0})\n",
    "ind_to_char.update({0:\"UNK\"})\n",
    "\n",
    "count = 1\n",
    "\n",
    "with open(INPUT_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        for char in line.split('\\t')[1]:\n",
    "            if char.lower() not in char_to_ind:\n",
    "                char_to_ind.update({char.lower():count})\n",
    "                ind_to_char.update({count:char.lower()})\n",
    "                count += 1\n",
    "\n",
    "#print(char_to_ind)\n",
    "#print(ind_to_char)\n",
    "\n",
    "n_letters = len(char_to_ind)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def letterToTensor(letter, n_letters):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][char_to_ind[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def lineToTensor(line, n_letters):\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][char_to_ind[letter]] = 1\n",
    "    return tensor\n",
    "\n",
    "def batchToTensor(batch, n_letters):\n",
    "    tensor = torch.zeros(len(batch),MAX_TWEET,n_letters)\n",
    "    for sentence, line in enumerate(batch):\n",
    "        for li, letter in enumerate(line):\n",
    "            tensor[sentence][li][char_to_ind[letter.lower()]] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#print(letterToTensor('o'))\n",
    "print(lineToTensor('hello how are tou').shape)\n",
    "print(batchToTensor(['hello friend', 'linear svm is better']))\n",
    "print(batchToTensor(['hello friend', 'linear svm is better']).shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "trainpath = INPUT_PATH\n",
    "train = pd.read_csv(trainpath, sep='\\\\t', names=[\"ID\",\"SENTENCE\",\"LABEL\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(train['SENTENCE'][0].lower())\n",
    "train_char_features = batchToTensor(train['SENTENCE'], n_letters)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(train_char_features.shape)\n",
    "for in_tensor in train_char_features:\n",
    "    print(in_tensor.shape)\n",
    "    break\n",
    "# train_char_features[0].shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    TextCNN implementation based on\n",
    "    https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # first convolutional layer (three layers)\n",
    "        self.conv_0 = nn.ModuleList([\n",
    "                nn.Conv2d(in_channels = 1,\n",
    "                          out_channels = n_filters,\n",
    "                          kernel_size = (fs, embed_dim))\n",
    "                for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text = (tensor of input, tensor of input length)\n",
    "#         print(text[0].shape)\n",
    "        # convert input to embeddings\n",
    "        in_data = text[0]\n",
    "        # in_data = [batch_size, sentence_length]\n",
    "        embedded = self.embedding(in_data)\n",
    "        # embedded = [batch_size, sentence_length, embedding_dimension]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded = [batch_size, 1, sentence_length, embedding_dimension]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv_0]\n",
    "        # conved_n = [batch_size, n_filters, sentence_length - filter_size[n] - 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled_n = [batch_size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat = [batch_size, n_filters * len(filter_sizes)]\n",
    "        logit = self.fc(cat)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_char_features = [num_of_tweets, max. length of each tweet, embedding_size]\n",
    "\"\"\"\n",
    "num_features = list(train_char_features.shape)\n",
    "print(num_features)\n",
    "max_tweet_length = num_features[1]\n",
    "embedding_dim = num_features[2] # vocab_size\n",
    "\"\"\"\n",
    "input_dim = embedding_dim = len(text_field.vocab)\n",
    "n_filters = 3 # number of filters\n",
    "filter_sizes = [3, 4, 5] # like character 3-gram, 4-gram, 5-gram\n",
    "output_dim = 3\n",
    "dropout = 0.5\n",
    "\n",
    "model = TextCNN(input_dim, embedding_dim, n_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 368164 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# checking the parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('The model has {} trainable parameters'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, labels):\n",
    "    correct, total = 0, 0\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "#     print(predicted, labels)\n",
    "#     print(predicted.shape, labels.shape)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "#         print(predictions, predictions.shape)\n",
    "#         print(batch.label, batch.label.shape)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = get_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        \"\"\"\n",
    "        if i % 10 == 0:\n",
    "            print(\"batch: {}, loss: {}, acc: {}\".format(i, loss, acc*100))\n",
    "        \"\"\"\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = get_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.10106595993042 | Train Acc: 38.608333333333334%\n",
      "\tVal. Loss: 1.0006959400278457 |  Val. Acc: 49.85752279635259%\n",
      "Epoch: 2 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.0448423498471577 | Train Acc: 42.4%\n",
      "\tVal. Loss: 0.9915222398778225 |  Val. Acc: 49.40159574468085%\n",
      "Epoch: 3 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.0358972846666972 | Train Acc: 42.95833333333333%\n",
      "\tVal. Loss: 0.9916330182805975 |  Val. Acc: 49.1451367781155%\n",
      "Epoch: 4 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.0148214836120606 | Train Acc: 44.983333333333334%\n",
      "\tVal. Loss: 0.9804358355542446 |  Val. Acc: 49.2686170212766%\n",
      "Epoch: 5 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 1.0044216448465983 | Train Acc: 46.19166666666666%\n",
      "\tVal. Loss: 0.9784309787953154 |  Val. Acc: 49.7815349544073%\n",
      "Epoch: 6 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9945557249387106 | Train Acc: 47.575%\n",
      "\tVal. Loss: 0.9755334017124582 |  Val. Acc: 50.39893617021277%\n",
      "Epoch: 7 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.9911596272786458 | Train Acc: 48.391666666666666%\n",
      "\tVal. Loss: 0.9763575487948478 |  Val. Acc: 49.67705167173252%\n",
      "Epoch: 8 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.9875047216415406 | Train Acc: 48.28333333333333%\n",
      "\tVal. Loss: 0.9702124570278411 |  Val. Acc: 50.92135258358663%\n",
      "Epoch: 9 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.9803291314442952 | Train Acc: 49.31666666666666%\n",
      "\tVal. Loss: 0.9666088045911586 |  Val. Acc: 50.71238601823709%\n",
      "Epoch: 10 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.976359694480896 | Train Acc: 49.233333333333334%\n",
      "\tVal. Loss: 0.9676591525686548 |  Val. Acc: 51.120820668693014%\n",
      "Epoch: 11 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.9728606198628743 | Train Acc: 49.75%\n",
      "\tVal. Loss: 0.9687737074304135 |  Val. Acc: 50.446428571428584%\n",
      "Epoch: 12 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9706759174664815 | Train Acc: 49.983333333333334%\n",
      "\tVal. Loss: 0.9657850722049145 |  Val. Acc: 51.120820668693014%\n",
      "Epoch: 13 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9639114584922791 | Train Acc: 49.833333333333336%\n",
      "\tVal. Loss: 0.9661809307463626 |  Val. Acc: 50.98784194528876%\n",
      "Epoch: 14 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.959247347831726 | Train Acc: 50.224999999999994%\n",
      "\tVal. Loss: 0.962033582494614 |  Val. Acc: 51.44376899696048%\n",
      "Epoch: 15 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9623745239575704 | Train Acc: 50.81666666666666%\n",
      "\tVal. Loss: 0.967949315588525 |  Val. Acc: 51.23480243161094%\n",
      "Epoch: 16 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9548398157755534 | Train Acc: 51.891666666666666%\n",
      "\tVal. Loss: 0.9671801823250791 |  Val. Acc: 51.05433130699089%\n",
      "Epoch: 17 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9528389458656311 | Train Acc: 52.26666666666666%\n",
      "\tVal. Loss: 0.967260387349636 |  Val. Acc: 51.18731003039514%\n",
      "Epoch: 18 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9514555007616678 | Train Acc: 51.94166666666666%\n",
      "\tVal. Loss: 0.964395449516621 |  Val. Acc: 50.86436170212766%\n",
      "Epoch: 19 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9506903333663941 | Train Acc: 52.00833333333333%\n",
      "\tVal. Loss: 0.967252254486084 |  Val. Acc: 51.58624620060791%\n",
      "Epoch: 20 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9449399326642355 | Train Acc: 52.208333333333336%\n",
      "\tVal. Loss: 0.9655691892542737 |  Val. Acc: 51.120820668693014%\n",
      "Epoch: 21 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9415105619430542 | Train Acc: 52.29166666666667%\n",
      "\tVal. Loss: 0.9662341792532738 |  Val. Acc: 50.930851063829785%\n",
      "Epoch: 22 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9393213391304016 | Train Acc: 52.31666666666667%\n",
      "\tVal. Loss: 0.968052180523568 |  Val. Acc: 50.83586626139818%\n",
      "Epoch: 23 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9406263063748678 | Train Acc: 52.258333333333326%\n",
      "\tVal. Loss: 0.965683899027236 |  Val. Acc: 50.83586626139818%\n",
      "Epoch: 24 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.94001047706604 | Train Acc: 52.44166666666666%\n",
      "\tVal. Loss: 0.964978481860871 |  Val. Acc: 51.93768996960486%\n",
      "Epoch: 25 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9372694171269734 | Train Acc: 52.45833333333333%\n",
      "\tVal. Loss: 0.9677863387351341 |  Val. Acc: 51.50075987841946%\n",
      "Epoch: 26 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9315829407374064 | Train Acc: 52.83333333333333%\n",
      "\tVal. Loss: 0.9642357433095892 |  Val. Acc: 51.84270516717325%\n",
      "Epoch: 27 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9286688853899638 | Train Acc: 52.666666666666664%\n",
      "\tVal. Loss: 0.9676779812954842 |  Val. Acc: 51.434270516717326%\n",
      "Epoch: 28 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9326009353001913 | Train Acc: 52.75%\n",
      "\tVal. Loss: 0.9674538094946679 |  Val. Acc: 51.26329787234043%\n",
      "Epoch: 29 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.930330829779307 | Train Acc: 52.69166666666667%\n",
      "\tVal. Loss: 0.9715556527705903 |  Val. Acc: 50.31344984802432%\n",
      "Epoch: 30 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.924193928082784 | Train Acc: 53.041666666666664%\n",
      "\tVal. Loss: 0.9671549074193264 |  Val. Acc: 51.85220364741642%\n",
      "Epoch: 31 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9239099496205648 | Train Acc: 53.05833333333333%\n",
      "\tVal. Loss: 0.9721728284308251 |  Val. Acc: 50.902355623100306%\n",
      "Epoch: 32 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9239495147069295 | Train Acc: 53.59166666666667%\n",
      "\tVal. Loss: 0.967752752151895 |  Val. Acc: 51.13031914893617%\n",
      "Epoch: 33 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9189885600407919 | Train Acc: 53.24166666666667%\n",
      "\tVal. Loss: 0.9689514117037996 |  Val. Acc: 51.196808510638306%\n",
      "Epoch: 34 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.917560626188914 | Train Acc: 53.083333333333336%\n",
      "\tVal. Loss: 0.9705605697124562 |  Val. Acc: 51.51975683890577%\n",
      "Epoch: 35 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9187398161888123 | Train Acc: 53.6%\n",
      "\tVal. Loss: 0.9752651199381402 |  Val. Acc: 51.31079027355623%\n",
      "Epoch: 36 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9211798400878907 | Train Acc: 53.19166666666667%\n",
      "\tVal. Loss: 0.9720284748584667 |  Val. Acc: 51.04483282674772%\n",
      "Epoch: 37 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9182406946818034 | Train Acc: 53.641666666666666%\n",
      "\tVal. Loss: 0.9722341438557239 |  Val. Acc: 51.18731003039514%\n",
      "Epoch: 38 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9103073124885559 | Train Acc: 53.733333333333334%\n",
      "\tVal. Loss: 0.9753256541617373 |  Val. Acc: 51.006838905775076%\n",
      "Epoch: 39 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9182312925656636 | Train Acc: 53.474999999999994%\n",
      "\tVal. Loss: 0.9718871205411059 |  Val. Acc: 51.18731003039514%\n",
      "Epoch: 40 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9132446727752686 | Train Acc: 53.6%\n",
      "\tVal. Loss: 0.9737210907834641 |  Val. Acc: 50.53191489361703%\n",
      "Epoch: 41 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9173198369344076 | Train Acc: 53.28333333333334%\n",
      "\tVal. Loss: 0.97345706503442 |  Val. Acc: 51.11132218844985%\n",
      "Epoch: 42 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9126491379737854 | Train Acc: 53.69166666666667%\n",
      "\tVal. Loss: 0.9739886864702753 |  Val. Acc: 50.9783434650456%\n",
      "Epoch: 43 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9091800764401754 | Train Acc: 53.65833333333333%\n",
      "\tVal. Loss: 0.9791381917101272 |  Val. Acc: 50.57940729483283%\n",
      "Epoch: 44 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9103541366259257 | Train Acc: 53.833333333333336%\n",
      "\tVal. Loss: 0.9754538713617528 |  Val. Acc: 50.96884498480243%\n",
      "Epoch: 45 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.909009751478831 | Train Acc: 53.849999999999994%\n",
      "\tVal. Loss: 0.9743641564186584 |  Val. Acc: 51.71922492401217%\n",
      "Epoch: 46 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9059824991226196 | Train Acc: 53.98333333333334%\n",
      "\tVal. Loss: 0.9749426156916516 |  Val. Acc: 50.930851063829785%\n",
      "Epoch: 47 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9067234921455384 | Train Acc: 54.0%\n",
      "\tVal. Loss: 0.9762797507833927 |  Val. Acc: 50.86436170212766%\n",
      "Epoch: 48 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9027740597724915 | Train Acc: 54.00833333333333%\n",
      "\tVal. Loss: 0.9768973525534285 |  Val. Acc: 50.92135258358663%\n",
      "Epoch: 49 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9006489988962809 | Train Acc: 54.333333333333336%\n",
      "\tVal. Loss: 0.979562796176748 |  Val. Acc: 50.797872340425535%\n",
      "Epoch: 50 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9035978666941324 | Train Acc: 53.86666666666666%\n",
      "\tVal. Loss: 0.980514924576942 |  Val. Acc: 50.256458966565354%\n",
      "Epoch: 51 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.904853683312734 | Train Acc: 53.53333333333333%\n",
      "\tVal. Loss: 0.9809655382278117 |  Val. Acc: 50.18996960486323%\n",
      "Epoch: 52 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9028718717892965 | Train Acc: 53.949999999999996%\n",
      "\tVal. Loss: 0.9849443549805499 |  Val. Acc: 50.256458966565354%\n",
      "Epoch: 53 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9044393935203552 | Train Acc: 54.00833333333333%\n",
      "\tVal. Loss: 0.9832480042538745 |  Val. Acc: 50.65539513677811%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9013598254521688 | Train Acc: 54.25833333333333%\n",
      "\tVal. Loss: 0.9803611598116286 |  Val. Acc: 51.05433130699089%\n",
      "Epoch: 55 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.899087917804718 | Train Acc: 54.25%\n",
      "\tVal. Loss: 0.9809864125353225 |  Val. Acc: 50.98784194528876%\n",
      "Epoch: 56 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9023212135632833 | Train Acc: 54.15833333333333%\n",
      "\tVal. Loss: 0.9824108164361183 |  Val. Acc: 50.33244680851063%\n",
      "Epoch: 57 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8985627559026083 | Train Acc: 54.108333333333334%\n",
      "\tVal. Loss: 0.9822953216573025 |  Val. Acc: 50.379939209726444%\n",
      "Epoch: 58 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9007677076657613 | Train Acc: 53.925%\n",
      "\tVal. Loss: 0.9860163153486049 |  Val. Acc: 50.56990881458966%\n",
      "Epoch: 59 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8955888768831889 | Train Acc: 54.44166666666666%\n",
      "\tVal. Loss: 0.9838657899105803 |  Val. Acc: 50.256458966565354%\n",
      "Epoch: 60 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8915649959246318 | Train Acc: 54.55833333333333%\n",
      "\tVal. Loss: 0.9859238064035456 |  Val. Acc: 50.39893617021277%\n",
      "Epoch: 61 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8971335161526998 | Train Acc: 54.25%\n",
      "\tVal. Loss: 0.9864571487649958 |  Val. Acc: 50.87386018237082%\n",
      "Epoch: 62 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8930195023218791 | Train Acc: 54.516666666666666%\n",
      "\tVal. Loss: 0.9832562055993588 |  Val. Acc: 50.066489361702125%\n",
      "Epoch: 63 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8968473779360453 | Train Acc: 54.30833333333334%\n",
      "\tVal. Loss: 0.9826884776987928 |  Val. Acc: 50.740881458966555%\n",
      "Epoch: 64 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8966757650375367 | Train Acc: 54.30833333333334%\n",
      "\tVal. Loss: 0.9851067991966896 |  Val. Acc: 49.86702127659575%\n",
      "Epoch: 65 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8946650396982829 | Train Acc: 54.44166666666666%\n",
      "\tVal. Loss: 0.9868987938191028 |  Val. Acc: 50.54141337386018%\n",
      "Epoch: 66 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.9008655440012614 | Train Acc: 54.233333333333334%\n",
      "\tVal. Loss: 0.9875073128558219 |  Val. Acc: 50.62689969604863%\n",
      "Epoch: 67 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8906394284566244 | Train Acc: 54.474999999999994%\n",
      "\tVal. Loss: 0.9872350515203273 |  Val. Acc: 50.21846504559271%\n",
      "Epoch: 68 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8929013579686482 | Train Acc: 54.30833333333334%\n",
      "\tVal. Loss: 0.9896013381633353 |  Val. Acc: 50.18047112462006%\n",
      "Epoch: 69 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8950306420326233 | Train Acc: 54.45833333333333%\n",
      "\tVal. Loss: 0.990917092942177 |  Val. Acc: 50.18996960486323%\n",
      "Epoch: 70 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8886475501060486 | Train Acc: 54.425000000000004%\n",
      "\tVal. Loss: 0.992872843082915 |  Val. Acc: 49.933510638297875%\n",
      "Epoch: 71 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8896544466018677 | Train Acc: 54.68333333333333%\n",
      "\tVal. Loss: 0.9903860180935962 |  Val. Acc: 50.256458966565354%\n",
      "Epoch: 72 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8851000291506449 | Train Acc: 54.94166666666666%\n",
      "\tVal. Loss: 0.9920220780879894 |  Val. Acc: 50.902355623100306%\n",
      "Epoch: 73 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8919887922604879 | Train Acc: 54.55%\n",
      "\tVal. Loss: 0.9877473204693896 |  Val. Acc: 50.3419452887538%\n",
      "Epoch: 74 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8893318691253662 | Train Acc: 54.516666666666666%\n",
      "\tVal. Loss: 0.9905852497892177 |  Val. Acc: 50.32294832826748%\n",
      "Epoch: 75 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8913135601679484 | Train Acc: 54.175%\n",
      "\tVal. Loss: 0.9913446915910599 |  Val. Acc: 50.389437689969604%\n",
      "Epoch: 76 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8898975892066956 | Train Acc: 54.266666666666666%\n",
      "\tVal. Loss: 0.9884422243909633 |  Val. Acc: 50.930851063829785%\n",
      "Epoch: 77 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8872126561800638 | Train Acc: 54.7%\n",
      "\tVal. Loss: 0.9937762782928792 |  Val. Acc: 50.31344984802432%\n",
      "Epoch: 78 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8913964678446452 | Train Acc: 54.574999999999996%\n",
      "\tVal. Loss: 0.9904309891639872 |  Val. Acc: 50.6363981762918%\n",
      "Epoch: 79 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.886294763247172 | Train Acc: 54.79166666666667%\n",
      "\tVal. Loss: 0.9912524489646263 |  Val. Acc: 50.455927051671736%\n",
      "Epoch: 80 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8815866901079814 | Train Acc: 55.208333333333336%\n",
      "\tVal. Loss: 0.9898767255722208 |  Val. Acc: 50.04749240121581%\n",
      "Epoch: 81 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8860593209266663 | Train Acc: 55.025%\n",
      "\tVal. Loss: 0.9932645419810681 |  Val. Acc: 50.32294832826748%\n",
      "Epoch: 82 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8877659511566162 | Train Acc: 54.733333333333334%\n",
      "\tVal. Loss: 0.9936286413923223 |  Val. Acc: 50.218465045592694%\n",
      "Epoch: 83 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8806997706095377 | Train Acc: 54.87499999999999%\n",
      "\tVal. Loss: 0.9980227858462232 |  Val. Acc: 49.88601823708206%\n",
      "Epoch: 84 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8841496005058288 | Train Acc: 54.6%\n",
      "\tVal. Loss: 0.9997353490362776 |  Val. Acc: 49.32560790273556%\n",
      "Epoch: 85 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8816973298390707 | Train Acc: 54.36666666666666%\n",
      "\tVal. Loss: 0.9993871754788338 |  Val. Acc: 50.455927051671736%\n",
      "Epoch: 86 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8799500487645467 | Train Acc: 54.98333333333333%\n",
      "\tVal. Loss: 0.9975834488868713 |  Val. Acc: 50.19946808510638%\n",
      "Epoch: 87 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8840204242070516 | Train Acc: 54.87499999999999%\n",
      "\tVal. Loss: 0.9993656505929663 |  Val. Acc: 50.78837386018237%\n",
      "Epoch: 88 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8785220651626587 | Train Acc: 55.208333333333336%\n",
      "\tVal. Loss: 1.005507788759597 |  Val. Acc: 50.31344984802432%\n",
      "Epoch: 89 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8834448992411296 | Train Acc: 54.900000000000006%\n",
      "\tVal. Loss: 1.0007070581963722 |  Val. Acc: 50.05699088145897%\n",
      "Epoch: 90 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8826498923301697 | Train Acc: 54.775%\n",
      "\tVal. Loss: 1.0050579842100753 |  Val. Acc: 50.1234802431611%\n",
      "Epoch: 91 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8818867685000101 | Train Acc: 54.900000000000006%\n",
      "\tVal. Loss: 1.0056481627707785 |  Val. Acc: 48.774696048632215%\n",
      "Epoch: 92 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8790322249730428 | Train Acc: 55.05833333333333%\n",
      "\tVal. Loss: 1.0043389822574371 |  Val. Acc: 49.83852583586626%\n",
      "Epoch: 93 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8770050633748372 | Train Acc: 55.025%\n",
      "\tVal. Loss: 1.0031358168480244 |  Val. Acc: 50.503419452887535%\n",
      "Epoch: 94 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8751195948918661 | Train Acc: 55.08333333333333%\n",
      "\tVal. Loss: 1.0053523081414244 |  Val. Acc: 49.24012158054711%\n",
      "Epoch: 95 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8796986753145853 | Train Acc: 54.641666666666666%\n",
      "\tVal. Loss: 1.0051860606416743 |  Val. Acc: 49.98100303951368%\n",
      "Epoch: 96 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8813577486673991 | Train Acc: 55.18333333333333%\n",
      "\tVal. Loss: 1.001184490132839 |  Val. Acc: 50.446428571428584%\n",
      "Epoch: 97 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8788669544855754 | Train Acc: 55.21666666666667%\n",
      "\tVal. Loss: 1.0055903120243803 |  Val. Acc: 49.66755319148936%\n",
      "Epoch: 98 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8788640913963318 | Train Acc: 54.833333333333336%\n",
      "\tVal. Loss: 1.0039587693011507 |  Val. Acc: 49.914513677811556%\n",
      "Epoch: 99 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8769325728416443 | Train Acc: 55.125%\n",
      "\tVal. Loss: 1.0071340279376253 |  Val. Acc: 50.31344984802432%\n",
      "Epoch: 100 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.8804151539802552 | Train Acc: 54.766666666666666%\n",
      "\tVal. Loss: 1.0092735100299755 |  Val. Acc: 49.99050151975684%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print('Epoch: {} | Epoch Time: {}m {}s'.format(epoch+1, epoch_mins, epoch_secs))\n",
    "    print('\\tTrain Loss: {} | Train Acc: {}%'.format(train_loss, train_acc*100))\n",
    "    print('\\tVal. Loss: {} |  Val. Acc: {}%'.format(valid_loss, valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
