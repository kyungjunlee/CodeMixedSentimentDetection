{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "random.seed(0)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/train_conll_hinglish.csv'\n",
    "import torchtext\n",
    "\n",
    "def label2int(label):\n",
    "    if label=='positive':\n",
    "        return 1\n",
    "    elif label=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x.lower(), # because are building a character-RNN\n",
    "                                  include_lengths=False, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "label_field = torchtext.data.Field(sequential=False,    # not a sequence\n",
    "                                   use_vocab=False,     # don't need to track vocabulary\n",
    "                                   is_target=True,      \n",
    "                                   batch_first=True,\n",
    "                                   preprocessing=lambda x: label2int(x)) # convert text to 0 and 1\n",
    "\n",
    "fields = [('id', None),('text', text_field), ('label', label_field)]\n",
    "dataset = torchtext.data.TabularDataset(filename, # name of the file\n",
    "                                        \"tsv\",               # fields are separated by a tab\n",
    "                                        fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "@ adilnisarbutt pakistan ka ghra tauq he pakistan israel ko tasleem nahein kerta isko palestine kehta he- occupied palestine --- 0\nmadarchod mulle ye mathura me nahi dikha tha jab mullo ne hindu ko iss liye mara ki vo lasse ki paise mag liye the… https// t. co/ oxf8tr3bly --- 0\n@ narendramodi manya pradhan mantri mahoday shriman narendra modi ji pradhanmantri banne par hardik badhai tahe dil… https// t. co/ prnomskkn1 --- 1\n@ atheist_ krishna jcb full trend me chal rahi aa --- 1\n@ abhisharsharma_@ ravishkumarblog loksabha me janta sirf modi ko vote de rahi thi na ki kisi mp or bjp ko without m… https// t. co/ shtbwcb7fm --- 1\n@ noirnaveed@ angelahana6@ cricketworldcup bhosdike tum pechvade ki tatti hi rahoge bc --- 0\nlove u bhaijan...♥♥ father+ son..# bharat# iambharat# bharatthiseid best pic from entire# promotions... mashallah… https// t. co/ s2xhwu6lud --- 1\n@ manojgajjar111 tumhara pass abh deemagh hai nahi islea google ko apna deemagh banaya hua hai. har koi tumhari tarh… https// t. co/ bxueug3xsn --- 0\n@ mahlogo_ nolo weni ankere o gae this weekend😂😂😂😂😂 --- 1\n@ aurangzeb_ aimim@ sachins40805591 lage raho mullo tumhre issi quran faad gyan ki kayal hain duniya allah bhi khus… https// t co/ gha9dwnz6u --- 0\n"
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(dataset[i].text, \"---\", dataset[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "12105 1513 1513 15131\n"
    }
   ],
   "source": [
    "print(len(train), len(val), len(test), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "47,\n             \"'\": 48,\n             '-': 49,\n             '’': 50,\n             '️': 51,\n             '🙏': 52,\n             '❤': 53,\n             '🤣': 54,\n             '&': 55,\n             '😍': 56,\n             ')': 57,\n             '(': 58,\n             '😭': 59,\n             '*': 60,\n             '😘': 61,\n             '🇮': 62,\n             '🇳': 63,\n             '😊': 64,\n             '🌹': 65,\n             '💜': 66,\n             '🎂': 67,\n             '“': 68,\n             '💕': 69,\n             'ा': 70,\n             '😁': 71,\n             '👏': 72,\n             '🎉': 73,\n             '💖': 74,\n             '|': 75,\n             '👍': 76,\n             '”': 77,\n             '👌': 78,\n             '✌': 79,\n             '%': 80,\n             'र': 81,\n             '😜': 82,\n             '💐': 83,\n             '😆': 84,\n             '🏻': 85,\n             '+': 86,\n             '♥': 87,\n             '्': 88,\n             '🤗': 89,\n             '😎': 90,\n             '😡': 91,\n             '🙄': 92,\n             '🔥': 93,\n             '~': 94,\n             '🤔': 95,\n             '\\u200d': 96,\n             '💙': 97,\n             '😉': 98,\n             '😠': 99,\n             '😅': 100,\n             '🙌': 101,\n             '🤪': 102,\n             'क': 103,\n             'न': 104,\n             'म': 105,\n             '😀': 106,\n             ';': 107,\n             '🌸': 108,\n             'ी': 109,\n             '💓': 110,\n             'स': 111,\n             '💔': 112,\n             '😋': 113,\n             '‘': 114,\n             '☺': 115,\n             '😌': 116,\n             '😢': 117,\n             'े': 118,\n             '—': 119,\n             '💞': 120,\n             '🥰': 121,\n             '💪': 122,\n             'त': 123,\n             'ह': 124,\n             '😔': 125,\n             '😝': 126,\n             '💗': 127,\n             '💯': 128,\n             '😄': 129,\n             '😑': 130,\n             '😒': 131,\n             '[': 132,\n             '🙈': 133,\n             '🌷': 134,\n             '🎊': 135,\n             '🚩': 136,\n             'ं': 137,\n             '😇': 138,\n             '=': 139,\n             '👇': 140,\n             '😪': 141,\n             ']': 142,\n             'é': 143,\n             'य': 144,\n             '🌺': 145,\n             '😹': 146,\n             'ि': 147,\n             '🏼': 148,\n             '😐': 149,\n             '😛': 150,\n             '>': 151,\n             '🙂': 152,\n             '😬': 153,\n             'ग': 154,\n             'प': 155,\n             'ब': 156,\n             '🎁': 157,\n             '😃': 158,\n             '💚': 159,\n             '😤': 160,\n             '🤩': 161,\n             '$': 162,\n             'á': 163,\n             '🎈': 164,\n             '🏽': 165,\n             '😏': 166,\n             '😕': 167,\n             'ا': 168,\n             '🏆': 169,\n             '💏': 170,\n             '\"': 171,\n             '<': 172,\n             'द': 173,\n             '🇰': 174,\n             '🇵': 175,\n             '😩': 176,\n             '🤦': 177,\n             'ā': 178,\n             'व': 179,\n             '♂': 180,\n             'ज': 181,\n             'ल': 182,\n             'ु': 183,\n             '☹': 184,\n             '♀': 185,\n             '💃': 186,\n             '🙊': 187,\n             '🤨': 188,\n             'ो': 189,\n             '🏾': 190,\n             '🖕': 191,\n             '🤧': 192,\n             '🥺': 193,\n             'ू': 194,\n             '✨': 195,\n             '💝': 196,\n             'ै': 197,\n             'ل': 198,\n             'श': 199,\n             '✅': 200,\n             '❣': 201,\n             '💋': 202,\n             '🤬': 203,\n             'í': 204,\n             'م': 205,\n             '🌈': 206,\n             '🌚': 207,\n             'ر': 208,\n             '👉': 209,\n             '💘': 210,\n             '💛': 211,\n             '😈': 212,\n             '😶': 213,\n             '😷': 214,\n             'ع': 215,\n             'भ': 216,\n             '–': 217,\n             '💮': 218,\n             '🔯': 219,\n             '😣': 220,\n             '😳': 221,\n             'ı': 222,\n             'ی': 223,\n             '👋': 224,\n             '😞': 225,\n             '😨': 226,\n             '🤓': 227,\n             '🥳': 228,\n             '`': 229,\n             'ş': 230,\n             'و': 231,\n             'अ': 232,\n             'ᴇ': 233,\n             '⚫': 234,\n             '🌼': 235,\n             '🏵': 236,\n             '👀': 237,\n             '👑': 238,\n             '🖤': 239,\n             '😚': 240,\n             '😥': 241,\n             '😴': 242,\n             '🤘': 243,\n             '\\U000e0067': 244,\n             'ç': 245,\n             'د': 246,\n             'फ': 247,\n             '్': 248,\n             '‼': 249,\n             '🌱': 250,\n             '🌲': 251,\n             '🎶': 252,\n             '🏏': 253,\n             '👫': 254,\n             '💥': 255,\n             '😓': 256,\n             '😻': 257,\n             '🙉': 258,\n             'å': 259,\n             'ñ': 260,\n             'ü': 261,\n             'ट': 262,\n             'ध': 263,\n             '🐍': 264,\n             '👻': 265,\n             '💫': 266,\n             '🤲': 267,\n             '🤷': 268,\n             '🦋': 269,\n             '🧡': 270,\n             'ê': 271,\n             'ख': 272,\n             'च': 273,\n             'ᴀ': 274,\n             '✊': 275,\n             '✖': 276,\n             '➡': 277,\n             '🍫': 278,\n             '🎵': 279,\n             '🎼': 280,\n             '👈': 281,\n             '👦': 282,\n             '👩': 283,\n             '💑': 284,\n             '😫': 285,\n             '🙃': 286,\n             '🙇': 287,\n             '🤞': 288,\n             '🤭': 289,\n             '´': 290,\n             'ã': 291,\n             'ة': 292,\n             'ج': 293,\n             'ق': 294,\n             'ए': 295,\n             'ड': 296,\n             'थ': 297,\n             'ర': 298,\n             'ᴛ': 299,\n             '•': 300,\n             '☝': 301,\n             '⛳': 302,\n             '✔': 303,\n             '🇦': 304,\n             '🌵': 305,\n             '🌿': 306,\n             '🏴': 307,\n             '🗿': 308,\n             '😯': 309,\n             '😱': 310,\n             '🙆': 311,\n             '🤙': 312,\n             '\\U000e0062': 313,\n             '\\U000e007f': 314,\n             '^': 315,\n             '{': 316,\n             '}': 317,\n             '¡': 318,\n             'ɩ': 319,\n             'ɪ': 320,\n             'ɴ': 321,\n             'ت': 322,\n             'س': 323,\n             'ک': 324,\n             'ई': 325,\n             'ా': 326,\n             'ి': 327,\n             '่': 328,\n             'ᴍ': 329,\n             'ᵉ': 330,\n             'ᵒ': 331,\n             '₹': 332,\n             '╭': 333,\n             '╮': 334,\n             '✋': 335,\n             '✍': 336,\n             '❓': 337,\n             '❗': 338,\n             '⬇': 339,\n             'の': 340,\n             '𝗲': 341,\n             '🌙': 342,\n             '🍧': 343,\n             '🍰': 344,\n             '🎓': 345,\n             '🏳': 346,\n             '🐱': 347,\n             '👎': 348,\n             '👨': 349,\n             '👭': 350,\n             '💩': 351,\n             '📄': 352,\n             '🔫': 353,\n             '🖐': 354,\n             '😟': 355,\n             '😮': 356,\n             '😰': 357,\n             '🤝': 358,\n             '🤯': 359,\n             '\\U000e0065': 360,\n             '\\U000e006e': 361,\n             'ä': 362,\n             'ô': 363,\n             'ý': 364,\n             'č': 365,\n             'ł': 366,\n             'ʀ': 367,\n             'ˢ': 368,\n             'ن': 369,\n             'ے': 370,\n             'आ': 371,\n             'इ': 372,\n             'ष': 373,\n             '़': 374,\n             'ौ': 375,\n             '।': 376,\n             'ਂ': 377,\n             'ం': 378,\n             'క': 379,\n             'డ': 380,\n             'ప': 381,\n             'స': 382,\n             'ు': 383,\n             'ค': 384,\n             '\\u200b': 385,\n             '\\u2066': 386,\n             '❌': 387,\n             '⬆': 388,\n             'ン': 389,\n             'ー': 390,\n             '𝄞': 391,\n             '𝘀': 392,\n             '🇩': 393,\n             '🇫': 394,\n             '🇬': 395,\n             '🇸': 396,\n             '🇺': 397,\n             '🇿': 398,\n             '🌟': 399,\n             '🌻': 400,\n             '🍾': 401,\n             '🎀': 402,\n             '🎇': 403,\n             '🎤': 404,\n             '🏿': 405,\n             '🐷': 406,\n             '👊': 407,\n             '👠': 408,\n             '👰': 409,\n             '👸': 410,\n             '👿': 411,\n             '💎': 412,\n             '💸': 413,\n             '📃': 414,\n             '🔝': 415,\n             '🔴': 416,\n             '🗣': 417,\n             '😗': 418,\n             '😙': 419,\n             '🙁': 420,\n             '🚨': 421,\n             '🤟': 422,\n             '🧐': 423,\n             '🧚': 424,\n             '®': 425,\n             '¯': 426,\n             '·': 427,\n             'º': 428,\n             '×': 429,\n             'î': 430,\n             'ó': 431,\n             'ö': 432,\n             'ú': 433,\n             'đ': 434,\n             'ē': 435,\n             'ī': 436,\n             'ř': 437,\n             'ů': 438,\n             'ż': 439,\n             'ɣ': 440,\n             'ɲ': 441,\n             'ʸ': 442,\n             '˘': 443,\n             '̩': 444,\n             'а': 445,\n             'є': 446,\n             'ғ': 447,\n             'լ': 448,\n             'ب': 449,\n             'ش': 450,\n             'ي': 451,\n             'ۃ': 452,\n             'ँ': 453,\n             'ः': 454,\n             'छ': 455,\n             'ण': 456,\n             'ृ': 457,\n             'ਤ': 458,\n             'ਰ': 459,\n             'ਹ': 460,\n             'ੀ': 461,\n             'ੁ': 462,\n             'ఉ': 463,\n             'త': 464,\n             'ద': 465,\n             'న': 466,\n             'మ': 467,\n             'య': 468,\n             'ల': 469,\n             'ష': 470,\n             'ో': 471,\n             'ก': 472,\n             'ท': 473,\n             'น': 474,\n             'บ': 475,\n             'ป': 476,\n             'ผ': 477,\n             'อ': 478,\n             'ี': 479,\n             'ุ': 480,\n             'ᴅ': 481,\n             'ᴋ': 482,\n             'ᴏ': 483,\n             'ᴘ': 484,\n             'ᵃ': 485,\n             'ᵍ': 486,\n             'ᵐ': 487,\n             'ᵘ': 488,\n             'ᶦ': 489,\n             'ạ': 490,\n             'ẽ': 491,\n             '‿': 492,\n             '\\u2069': 493,\n             '⃑': 494,\n             '⃣': 495,\n             '►': 496,\n             '◉': 497,\n             '♡': 498,\n             '✈': 499,\n             '⠀': 500,\n             '⤴': 501,\n             '〣': 502,\n             'す': 503,\n             'っ': 504,\n             'で': 505,\n             'ダ': 506,\n             'ヒ': 507,\n             'ョ': 508,\n             'ㅠ': 509,\n             '定': 510,\n             '日': 511,\n             '限': 512,\n             'ꕊ': 513,\n             '기': 514,\n             '다': 515,\n             '스': 516,\n             'ﷺ': 517,\n             'ｓ': 518,\n             '｡': 519,\n             '𝐞': 520,\n             '𝐡': 521,\n             '𝗰': 522,\n             '𝗶': 523,\n             '🇧': 524,\n             '🇭': 525,\n             '🇱': 526,\n             '🌍': 527,\n             '🌎': 528,\n             '🌏': 529,\n             '🌘': 530,\n             '🌝': 531,\n             '🌴': 532,\n             '🌶': 533,\n             '🍃': 534,\n             '🍍': 535,\n             '🍎': 536,\n             '🍏': 537,\n             '🍦': 538,\n             '🍨': 539,\n             '🍬': 540,\n             '🎋': 541,\n             '🎧': 542,\n             '🎬': 543,\n             '🐂': 544,\n             '🐖': 545,\n             '🐯': 546,\n             '🐼': 547,\n             '👧': 548,\n             '👺': 549,\n             '👾': 550,\n             '💀': 551,\n             '💁': 552,\n             '💅': 553,\n             '💌': 554,\n             '💡': 555,\n             '💬': 556,\n             '📷': 557,\n             '😖': 558,\n             '🙋': 559,\n             '🚬': 560,\n             '🚴': 561,\n             '🚶': 562,\n             '🤒': 563,\n             '🤤': 564,\n             '🥵': 565,\n             '🦉': 566,\n             '🦍': 567,\n             '£': 568,\n             '¥': 569,\n             '°': 570,\n             '¿': 571,\n             'à': 572,\n             'ì': 573,\n             'ò': 574,\n             'ć': 575,\n             'ę': 576,\n             'ě': 577,\n             'ğ': 578,\n             'ň': 579,\n             'ś': 580,\n             'š': 581,\n             'ư': 582,\n             'ɢ': 583,\n             'ʏ': 584,\n             'ʜ': 585,\n             'ʟ': 586,\n             'ʰ': 587,\n             'ʳ': 588,\n             'δ': 589,\n             'μ': 590,\n             'д': 591,\n             'к': 592,\n             'м': 593,\n             'о': 594,\n             'р': 595,\n             'т': 596,\n             'ئ': 597,\n             'خ': 598,\n             'ز': 599,\n             'ص': 600,\n             'ط': 601,\n             'غ': 602,\n             'ه': 603,\n             'گ': 604,\n             'ں': 605,\n             'ہ': 606,\n             'ऑ': 607,\n             'औ': 608,\n             'ठ': 609,\n             'ॉ': 610,\n             'ঈ': 611,\n             'ক': 612,\n             'দ': 613,\n             'ব': 614,\n             'ম': 615,\n             'র': 616,\n             'া': 617,\n             'ো': 618,\n             'ਕ': 619,\n             'ਗ': 620,\n             'ਣ': 621,\n             'ਨ': 622,\n             'ਫ': 623,\n             'ਸ': 624,\n             'ਾ': 625,\n             'ਿ': 626,\n             'ੇ': 627,\n             'ੋ': 628,\n             'అ': 629,\n             'ఇ': 630,\n             'చ': 631,\n             'ధ': 632,\n             'బ': 633,\n             'వ': 634,\n             'ూ': 635,\n             'ే': 636,\n             'ข': 637,\n             'ง': 638,\n             'ณ': 639,\n             'ย': 640,\n             'ร': 641,\n             'ล': 642,\n             'ว': 643,\n             'ั': 644,\n             'า': 645,\n             'ึ': 646,\n             'ู': 647,\n             'แ': 648,\n             'ไ': 649,\n             '้': 650,\n             'ᴄ': 651,\n             'ᴜ': 652,\n             'ᴠ': 653,\n             'ᵈ': 654,\n             'ᵏ': 655,\n             'ᵗ': 656,\n             'ᵛ': 657,\n             'ᶜ': 658,\n             'ế': 659,\n             'ề': 660,\n             'ỉ': 661,\n             'ố': 662,\n             'ồ': 663,\n             'ờ': 664,\n             'ủ': 665,\n             '―': 666,\n             '⁉': 667,\n             '€': 668,\n             '℅': 669,\n             '™': 670,\n             '↑': 671,\n             '⇒': 672,\n             '⌚': 673,\n             '▂': 674,\n             '☀': 675,\n             '★': 676,\n             '☔': 677,\n             '☪': 678,\n             '⚔': 679,\n             '⚕': 680,\n             '⚘': 681,\n             '⚪': 682,\n             '⚽': 683,\n             '⛲': 684,\n             '⛽': 685,\n             '✉': 686,\n             '✴': 687,\n             '❇': 688,\n             '❔': 689,\n             '➕': 690,\n             '⭐': 691,\n             '、': 692,\n             'い': 693,\n             'が': 694,\n             'こ': 695,\n             'つ': 696,\n             'て': 697,\n             'と': 698,\n             'に': 699,\n             'ね': 700,\n             'は': 701,\n             'べ': 702,\n             'ま': 703,\n             'る': 704,\n             'を': 705,\n             'ア': 706,\n             'イ': 707,\n             'ク': 708,\n             'コ': 709,\n             'ジ': 710,\n             'ス': 711,\n             'ズ': 712,\n             'テ': 713,\n             'ト': 714,\n             'ネ': 715,\n             'パ': 716,\n             'ビ': 717,\n             'ベ': 718,\n             'ム': 719,\n             'リ': 720,\n             'ル': 721,\n             '当': 722,\n             '放': 723,\n             '猫': 724,\n             '生': 725,\n             '誕': 726,\n             '開': 727,\n             '경': 728,\n             '계': 729,\n             '국': 730,\n             '께': 731,\n             '나': 732,\n             '는': 733,\n             '니': 734,\n             '댄': 735,\n             '러': 736,\n             '립': 737,\n             '만': 738,\n             '머': 739,\n             '멀': 740,\n             '뷰': 741,\n             '생': 742,\n             '시': 743,\n             '실': 744,\n             '약': 745,\n             '와': 746,\n             '은': 747,\n             '이': 748,\n             '일': 749,\n             '자': 750,\n             '제': 751,\n             '중': 752,\n             '즐': 753,\n             '터': 754,\n             '티': 755,\n             '팀': 756,\n             '한': 757,\n             '함': 758,\n             '행': 759,\n             '황': 760,\n             'ﷻ': 761,\n             '︎': 762,\n             'ｅ': 763,\n             'ｎ': 764,\n             'ｔ': 765,\n             'ｕ': 766,\n             '𝐂': 767,\n             '𝐉': 768,\n             '𝐖': 769,\n             '𝐚': 770,\n             '𝐢': 771,\n             '𝐧': 772,\n             '𝐩': 773,\n             '𝐫': 774,\n             '𝐬': 775,\n             '𝐭': 776,\n             '𝐮': 777,\n             '𝐹': 778,\n             '𝑔': 779,\n             '𝑜': 780,\n             '𝒻': 781,\n             '𝒽': 782,\n             '𝒾': 783,\n             '𝓇': 784,\n             '𝓈': 785,\n             '𝓉': 786,\n             '𝓊': 787,\n             '𝗣': 788,\n             '𝗮': 789,\n             '𝗯': 790,\n             '𝗴': 791,\n             '𝗹': 792,\n             '𝗺': 793,\n             '𝗿': 794,\n             '𝘂': 795,\n             '𝘃': 796,\n             '𝟏': 797,\n             '𝟐': 798,\n             '𝟔': 799,\n             '🇨': 800,\n             '🇪': 801,\n             '🇲': 802,\n             '🇷': 803,\n             '🇾': 804,\n             '🌄': 805,\n             '🌊': 806,\n             '🌌': 807,\n             '🌳': 808,\n             '🍀': 809,\n             '🍂': 810,\n             '🍇': 811,\n             '🍟': 812,\n             '🍱': 813,\n             '🎆': 814,\n             '🎗': 815,\n             '🎟': 816,\n             '🎥': 817,\n             '🎩': 818,\n             '🏁': 819,\n             '🏋': 820,\n             '🏖': 821,\n             '🏡': 822,\n             '🐈': 823,\n             '🐐': 824,\n             '🐒': 825,\n             '🐓': 826,\n             '🐕': 827,\n             '🐗': 828,\n             '🐥': 829,\n             '🐮': 830,\n             '🐶': 831,\n             '🐸': 832,\n             '🐿': 833,\n             '👅': 834,\n             '👆': 835,\n             '👐': 836,\n             '👒': 837,\n             '👞': 838,\n             '👟': 839,\n             '👥': 840,\n             '👶': 841,\n             '👹': 842,\n             '👼': 843,\n             '💍': 844,\n             '💟': 845,\n             '💣': 846,\n             '💾': 847,\n             '📈': 848,\n             '📝': 849,\n             '📣': 850,\n             '📻': 851,\n             '📿': 852,\n             '🔁': 853,\n             '🔈': 854,\n             '🔐': 855,\n             '🔔': 856,\n             '🔚': 857,\n             '🔞': 858,\n             '🔟': 859,\n             '🔨': 860,\n             '🔪': 861,\n             '🔶': 862,\n             '🔷': 863,\n             '🕋': 864,\n             '🕌': 865,\n             '🕯': 866,\n             '🕺': 867,\n             '😦': 868,\n             '😲': 869,\n             '😵': 870,\n             '😼': 871,\n             '😿': 872,\n             '🙀': 873,\n             '🙅': 874,\n             '🛍': 875,\n             '🛐': 876,\n             '🤐': 877,\n             '🤑': 878,\n             '🤕': 879,\n             '🤚': 880,\n             '🤠': 881,\n             '🤡': 882,\n             '🤮': 883,\n             '🥂': 884,\n             '🥃': 885,\n             '🥇': 886,\n             '🥭': 887,\n             '🥴': 888,\n             '🥾': 889,\n             '🦁': 890,\n             '🦄': 891,\n             '🦅': 892,\n             '🦳': 893,\n             '🧁': 894,\n             '🧸': 895,\n             '\\U000e0063': 896,\n             '\\U000e0073': 897,\n             '\\U000e0074': 898})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(dataset)\n",
    "text_field.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['<unk>',\n '<pad>',\n ' ',\n 'a',\n 'i',\n 'e',\n 'h',\n 't',\n 'o',\n 'r',\n 's',\n 'n',\n 'k',\n 'm',\n 'l',\n 'd',\n 'u',\n 'p',\n 'b',\n 'y',\n 'c',\n 'g',\n '/',\n '@',\n '.',\n 'j',\n 'w',\n 'f',\n 'v',\n '…',\n 'z',\n '_',\n '1',\n '0',\n 'q',\n '2',\n 'x',\n '3',\n '9',\n '5',\n '4',\n '7',\n '8',\n '6',\n '#',\n '!',\n '😂',\n '?',\n \"'\",\n '-',\n '’',\n '️',\n '🙏',\n '❤',\n '🤣',\n '&',\n '😍',\n ')',\n '(',\n '😭',\n '*',\n '😘',\n '🇮',\n '🇳',\n '😊',\n '🌹',\n '💜',\n '🎂',\n '“',\n '💕',\n 'ा',\n '😁',\n '👏',\n '🎉',\n '💖',\n '|',\n '👍',\n '”',\n '👌',\n '✌',\n '%',\n 'र',\n '😜',\n '💐',\n '😆',\n '🏻',\n '+',\n '♥',\n '्',\n '🤗',\n '😎',\n '😡',\n '🙄',\n '🔥',\n '~',\n '🤔',\n '\\u200d',\n '💙',\n '😉',\n '😠',\n '😅',\n '🙌',\n '🤪',\n 'क',\n 'न',\n 'म',\n '😀',\n ';',\n '🌸',\n 'ी',\n '💓',\n 'स',\n '💔',\n '😋',\n '‘',\n '☺',\n '😌',\n '😢',\n 'े',\n '—',\n '💞',\n '🥰',\n '💪',\n 'त',\n 'ह',\n '😔',\n '😝',\n '💗',\n '💯',\n '😄',\n '😑',\n '😒',\n '[',\n '🙈',\n '🌷',\n '🎊',\n '🚩',\n 'ं',\n '😇',\n '=',\n '👇',\n '😪',\n ']',\n 'é',\n 'य',\n '🌺',\n '😹',\n 'ि',\n '🏼',\n '😐',\n '😛',\n '>',\n '🙂',\n '😬',\n 'ग',\n 'प',\n 'ब',\n '🎁',\n '😃',\n '💚',\n '😤',\n '🤩',\n '$',\n 'á',\n '🎈',\n '🏽',\n '😏',\n '😕',\n 'ا',\n '🏆',\n '💏',\n '\"',\n '<',\n 'द',\n '🇰',\n '🇵',\n '😩',\n '🤦',\n 'ā',\n 'व',\n '♂',\n 'ज',\n 'ल',\n 'ु',\n '☹',\n '♀',\n '💃',\n '🙊',\n '🤨',\n 'ो',\n '🏾',\n '🖕',\n '🤧',\n '🥺',\n 'ू',\n '✨',\n '💝',\n 'ै',\n 'ل',\n 'श',\n '✅',\n '❣',\n '💋',\n '🤬',\n 'í',\n 'م',\n '🌈',\n '🌚',\n 'ر',\n '👉',\n '💘',\n '💛',\n '😈',\n '😶',\n '😷',\n 'ع',\n 'भ',\n '–',\n '💮',\n '🔯',\n '😣',\n '😳',\n 'ı',\n 'ی',\n '👋',\n '😞',\n '😨',\n '🤓',\n '🥳',\n '`',\n 'ş',\n 'و',\n 'अ',\n 'ᴇ',\n '⚫',\n '🌼',\n '🏵',\n '👀',\n '👑',\n '🖤',\n '😚',\n '😥',\n '😴',\n '🤘',\n '\\U000e0067',\n 'ç',\n 'د',\n 'फ',\n '్',\n '‼',\n '🌱',\n '🌲',\n '🎶',\n '🏏',\n '👫',\n '💥',\n '😓',\n '😻',\n '🙉',\n 'å',\n 'ñ',\n 'ü',\n 'ट',\n 'ध',\n '🐍',\n '👻',\n '💫',\n '🤲',\n '🤷',\n '🦋',\n '🧡',\n 'ê',\n 'ख',\n 'च',\n 'ᴀ',\n '✊',\n '✖',\n '➡',\n '🍫',\n '🎵',\n '🎼',\n '👈',\n '👦',\n '👩',\n '💑',\n '😫',\n '🙃',\n '🙇',\n '🤞',\n '🤭',\n '´',\n 'ã',\n 'ة',\n 'ج',\n 'ق',\n 'ए',\n 'ड',\n 'थ',\n 'ర',\n 'ᴛ',\n '•',\n '☝',\n '⛳',\n '✔',\n '🇦',\n '🌵',\n '🌿',\n '🏴',\n '🗿',\n '😯',\n '😱',\n '🙆',\n '🤙',\n '\\U000e0062',\n '\\U000e007f',\n '^',\n '{',\n '}',\n '¡',\n 'ɩ',\n 'ɪ',\n 'ɴ',\n 'ت',\n 'س',\n 'ک',\n 'ई',\n 'ా',\n 'ి',\n '่',\n 'ᴍ',\n 'ᵉ',\n 'ᵒ',\n '₹',\n '╭',\n '╮',\n '✋',\n '✍',\n '❓',\n '❗',\n '⬇',\n 'の',\n '𝗲',\n '🌙',\n '🍧',\n '🍰',\n '🎓',\n '🏳',\n '🐱',\n '👎',\n '👨',\n '👭',\n '💩',\n '📄',\n '🔫',\n '🖐',\n '😟',\n '😮',\n '😰',\n '🤝',\n '🤯',\n '\\U000e0065',\n '\\U000e006e',\n 'ä',\n 'ô',\n 'ý',\n 'č',\n 'ł',\n 'ʀ',\n 'ˢ',\n 'ن',\n 'ے',\n 'आ',\n 'इ',\n 'ष',\n '़',\n 'ौ',\n '।',\n 'ਂ',\n 'ం',\n 'క',\n 'డ',\n 'ప',\n 'స',\n 'ు',\n 'ค',\n '\\u200b',\n '\\u2066',\n '❌',\n '⬆',\n 'ン',\n 'ー',\n '𝄞',\n '𝘀',\n '🇩',\n '🇫',\n '🇬',\n '🇸',\n '🇺',\n '🇿',\n '🌟',\n '🌻',\n '🍾',\n '🎀',\n '🎇',\n '🎤',\n '🏿',\n '🐷',\n '👊',\n '👠',\n '👰',\n '👸',\n '👿',\n '💎',\n '💸',\n '📃',\n '🔝',\n '🔴',\n '🗣',\n '😗',\n '😙',\n '🙁',\n '🚨',\n '🤟',\n '🧐',\n '🧚',\n '®',\n '¯',\n '·',\n 'º',\n '×',\n 'î',\n 'ó',\n 'ö',\n 'ú',\n 'đ',\n 'ē',\n 'ī',\n 'ř',\n 'ů',\n 'ż',\n 'ɣ',\n 'ɲ',\n 'ʸ',\n '˘',\n '̩',\n 'а',\n 'є',\n 'ғ',\n 'լ',\n 'ب',\n 'ش',\n 'ي',\n 'ۃ',\n 'ँ',\n 'ः',\n 'छ',\n 'ण',\n 'ृ',\n 'ਤ',\n 'ਰ',\n 'ਹ',\n 'ੀ',\n 'ੁ',\n 'ఉ',\n 'త',\n 'ద',\n 'న',\n 'మ',\n 'య',\n 'ల',\n 'ష',\n 'ో',\n 'ก',\n 'ท',\n 'น',\n 'บ',\n 'ป',\n 'ผ',\n 'อ',\n 'ี',\n 'ุ',\n 'ᴅ',\n 'ᴋ',\n 'ᴏ',\n 'ᴘ',\n 'ᵃ',\n 'ᵍ',\n 'ᵐ',\n 'ᵘ',\n 'ᶦ',\n 'ạ',\n 'ẽ',\n '‿',\n '\\u2069',\n '⃑',\n '⃣',\n '►',\n '◉',\n '♡',\n '✈',\n '⠀',\n '⤴',\n '〣',\n 'す',\n 'っ',\n 'で',\n 'ダ',\n 'ヒ',\n 'ョ',\n 'ㅠ',\n '定',\n '日',\n '限',\n 'ꕊ',\n '기',\n '다',\n '스',\n 'ﷺ',\n 'ｓ',\n '｡',\n '𝐞',\n '𝐡',\n '𝗰',\n '𝗶',\n '🇧',\n '🇭',\n '🇱',\n '🌍',\n '🌎',\n '🌏',\n '🌘',\n '🌝',\n '🌴',\n '🌶',\n '🍃',\n '🍍',\n '🍎',\n '🍏',\n '🍦',\n '🍨',\n '🍬',\n '🎋',\n '🎧',\n '🎬',\n '🐂',\n '🐖',\n '🐯',\n '🐼',\n '👧',\n '👺',\n '👾',\n '💀',\n '💁',\n '💅',\n '💌',\n '💡',\n '💬',\n '📷',\n '😖',\n '🙋',\n '🚬',\n '🚴',\n '🚶',\n '🤒',\n '🤤',\n '🥵',\n '🦉',\n '🦍',\n '£',\n '¥',\n '°',\n '¿',\n 'à',\n 'ì',\n 'ò',\n 'ć',\n 'ę',\n 'ě',\n 'ğ',\n 'ň',\n 'ś',\n 'š',\n 'ư',\n 'ɢ',\n 'ʏ',\n 'ʜ',\n 'ʟ',\n 'ʰ',\n 'ʳ',\n 'δ',\n 'μ',\n 'д',\n 'к',\n 'м',\n 'о',\n 'р',\n 'т',\n 'ئ',\n 'خ',\n 'ز',\n 'ص',\n 'ط',\n 'غ',\n 'ه',\n 'گ',\n 'ں',\n 'ہ',\n 'ऑ',\n 'औ',\n 'ठ',\n 'ॉ',\n 'ঈ',\n 'ক',\n 'দ',\n 'ব',\n 'ম',\n 'র',\n 'া',\n 'ো',\n 'ਕ',\n 'ਗ',\n 'ਣ',\n 'ਨ',\n 'ਫ',\n 'ਸ',\n 'ਾ',\n 'ਿ',\n 'ੇ',\n 'ੋ',\n 'అ',\n 'ఇ',\n 'చ',\n 'ధ',\n 'బ',\n 'వ',\n 'ూ',\n 'ే',\n 'ข',\n 'ง',\n 'ณ',\n 'ย',\n 'ร',\n 'ล',\n 'ว',\n 'ั',\n 'า',\n 'ึ',\n 'ู',\n 'แ',\n 'ไ',\n '้',\n 'ᴄ',\n 'ᴜ',\n 'ᴠ',\n 'ᵈ',\n 'ᵏ',\n 'ᵗ',\n 'ᵛ',\n 'ᶜ',\n 'ế',\n 'ề',\n 'ỉ',\n 'ố',\n 'ồ',\n 'ờ',\n 'ủ',\n '―',\n '⁉',\n '€',\n '℅',\n '™',\n '↑',\n '⇒',\n '⌚',\n '▂',\n '☀',\n '★',\n '☔',\n '☪',\n '⚔',\n '⚕',\n '⚘',\n '⚪',\n '⚽',\n '⛲',\n '⛽',\n '✉',\n '✴',\n '❇',\n '❔',\n '➕',\n '⭐',\n '、',\n 'い',\n 'が',\n 'こ',\n 'つ',\n 'て',\n 'と',\n 'に',\n 'ね',\n 'は',\n 'べ',\n 'ま',\n 'る',\n 'を',\n 'ア',\n 'イ',\n 'ク',\n 'コ',\n 'ジ',\n 'ス',\n 'ズ',\n 'テ',\n 'ト',\n 'ネ',\n 'パ',\n 'ビ',\n 'ベ',\n 'ム',\n 'リ',\n 'ル',\n '当',\n '放',\n '猫',\n '生',\n '誕',\n '開',\n '경',\n '계',\n '국',\n '께',\n '나',\n '는',\n '니',\n '댄',\n '러',\n '립',\n '만',\n '머',\n '멀',\n '뷰',\n '생',\n '시',\n '실',\n '약',\n '와',\n '은',\n '이',\n '일',\n '자',\n '제',\n '중',\n '즐',\n '터',\n '티',\n '팀',\n '한',\n '함',\n '행',\n '황',\n 'ﷻ',\n '︎',\n 'ｅ',\n 'ｎ',\n 'ｔ',\n 'ｕ',\n '𝐂',\n '𝐉',\n '𝐖',\n '𝐚',\n '𝐢',\n '𝐧',\n '𝐩',\n '𝐫',\n '𝐬',\n '𝐭',\n '𝐮',\n '𝐹',\n '𝑔',\n '𝑜',\n '𝒻',\n '𝒽',\n '𝒾',\n '𝓇',\n '𝓈',\n '𝓉',\n '𝓊',\n '𝗣',\n '𝗮',\n '𝗯',\n '𝗴',\n '𝗹',\n '𝗺',\n '𝗿',\n '𝘂',\n '𝘃',\n '𝟏',\n '𝟐',\n '𝟔',\n '🇨',\n '🇪',\n '🇲',\n '🇷',\n '🇾',\n '🌄',\n '🌊',\n '🌌',\n '🌳',\n '🍀',\n '🍂',\n '🍇',\n '🍟',\n '🍱',\n '🎆',\n '🎗',\n '🎟',\n '🎥',\n '🎩',\n '🏁',\n '🏋',\n '🏖',\n '🏡',\n '🐈',\n '🐐',\n '🐒',\n '🐓',\n '🐕',\n '🐗',\n '🐥',\n '🐮',\n '🐶',\n '🐸',\n '🐿',\n '👅',\n '👆',\n '👐',\n '👒',\n '👞',\n '👟',\n '👥',\n '👶',\n '👹',\n '👼',\n '💍',\n '💟',\n '💣',\n '💾',\n '📈',\n '📝',\n '📣',\n '📻',\n '📿',\n '🔁',\n '🔈',\n '🔐',\n '🔔',\n '🔚',\n '🔞',\n '🔟',\n '🔨',\n '🔪',\n '🔶',\n '🔷',\n '🕋',\n '🕌',\n '🕯',\n '🕺',\n '😦',\n '😲',\n '😵',\n '😼',\n '😿',\n '🙀',\n '🙅',\n '🛍',\n '🛐',\n '🤐',\n '🤑',\n '🤕',\n '🤚',\n '🤠',\n '🤡',\n '🤮',\n '🥂',\n '🥃',\n '🥇',\n '🥭',\n '🥴',\n '🥾',\n '🦁',\n '🦄',\n '🦅',\n '🦳',\n '🧁',\n '🧸',\n '\\U000e0063',\n '\\U000e0073',\n '\\U000e0074']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch = 128\n",
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                          )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "val_iter = torchtext.data.BucketIterator(val,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                        )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                         )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<torchtext.data.iterator.BucketIterator object at 0x7f8ee5833f10>\ntensor([[23,  2, 17,  ..., 27,  4,  7],\n        [15,  3, 10,  ...,  8, 16, 24],\n        [ 9,  7, 23,  ...,  4,  5, 10],\n        ...,\n        [ 9,  7, 23,  ..., 77,  1,  1],\n        [23,  2, 10,  ..., 21,  1,  1],\n        [ 9,  7, 23,  ..., 46,  1,  1]])\ntensor([1, 1, 1, 2, 1, 1, 2, 0, 1, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 2, 0, 1,\n        1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 0, 2, 2, 1,\n        1, 0, 1, 2, 1, 1, 2, 2, 2, 0, 0, 0, 2, 2, 1, 2, 2, 0, 1, 2, 2, 1, 2, 0,\n        2, 0, 1, 2, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 1, 1, 2,\n        0, 2, 2, 0, 1, 1, 2, 2, 1, 0, 1, 2, 2, 2, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1,\n        2, 2, 1, 1, 0, 2, 1, 0])\ntensor([[23,  2, 28,  ..., 20, 14,  5],\n        [23,  2, 18,  ...,  2, 17,  5],\n        [ 9,  7, 23,  ...,  3, 14,  4],\n        ...,\n        [23,  2, 10,  ...,  1,  1,  1],\n        [23,  2, 10,  ...,  1,  1,  1],\n        [23,  2, 12,  ...,  1,  1,  1]])\ntensor([2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2,\n        2, 2, 1, 1, 1, 2, 2, 0, 0, 1, 2, 0, 2, 1, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0,\n        2, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1,\n        0, 2, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0,\n        2, 2, 0, 0, 1, 2, 2, 1, 1, 2, 1, 1, 0, 1, 0, 2, 2, 0, 0, 1, 2, 1, 2, 1,\n        0, 0, 0, 1, 0, 1, 1, 2])\ntensor([[23,  2, 15,  ..., 15,  4,  3],\n        [10,  8, 13,  ..., 16, 17, 24],\n        [23,  2, 10,  ..., 21,  3,  1],\n        ...,\n        [23,  2,  3,  ...,  1,  1,  1],\n        [23,  2,  6,  ...,  1,  1,  1],\n        [ 9,  3, 25,  ...,  1,  1,  1]])\ntensor([0, 2, 0, 2, 2, 2, 2, 1, 1, 1, 0, 1, 1, 0, 2, 2, 2, 1, 1, 2, 0, 0, 2, 1,\n        2, 2, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1, 2, 2, 0, 1, 1, 0, 1, 1, 0, 2, 1, 0,\n        1, 1, 2, 0, 2, 2, 2, 0, 2, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 1, 1, 0, 2, 1,\n        2, 0, 0, 1, 1, 0, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2,\n        0, 1, 1, 0, 1, 0, 2, 2, 2, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 0, 0, 0, 2,\n        1, 2, 1, 1, 2, 1, 1, 1])\ntensor([[ 11,   2, 291,  ...,   9,  43,  10],\n        [ 10,   3,   9,  ...,  39,  13,  27],\n        [  9,   7,  23,  ...,  46,  46,  29],\n        ...,\n        [ 23,   2,   4,  ...,   9,  40,  42],\n        [ 23,   2,  12,  ...,   4,  21,  24],\n        [ 23,   2,  26,  ...,   5,   9,   4]])\ntensor([2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 0, 1, 2, 1, 2, 1, 1, 2, 2, 0, 2,\n        1, 1, 2, 1, 0, 2, 1, 2, 1, 2, 0, 0, 0, 2, 1, 1, 2, 0, 1, 1, 0, 1, 0, 0,\n        1, 1, 2, 1, 2, 1, 1, 0, 0, 2, 2, 1, 1, 0, 1, 2, 2, 2, 1, 0, 2, 2, 0, 2,\n        1, 1, 1, 1, 0, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2,\n        1, 0, 2, 2, 2, 1, 1, 2, 1, 0, 0, 1, 2, 2, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2,\n        0, 2, 1, 1, 0, 1, 0, 2])\ntensor([[23,  2,  3,  ...,  3,  6, 18],\n        [ 9,  7, 23,  ..., 20, 12, 29],\n        [ 9,  7, 23,  ..., 18,  3, 29],\n        ...,\n        [23,  2, 10,  ..., 27, 13, 17],\n        [23,  2,  3,  ..., 14,  5, 11],\n        [23,  2,  3,  ...,  9,  6,  3]])\ntensor([2, 1, 2, 1, 2, 2, 2, 0, 1, 1, 0, 1, 0, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 1,\n        1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2, 1, 1, 0, 1, 1, 2, 2, 1,\n        0, 1, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 1, 2, 1, 2, 1, 2, 1, 0, 0, 2, 0,\n        1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 1,\n        0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2, 2, 1, 1, 0, 2, 2, 1, 0, 2,\n        1, 0, 0, 1, 0, 1, 0, 0])\ntensor([[ 23,   2,  26,  ...,  19,   8,  16],\n        [ 23,   2,   6,  ...,   5, 175, 396],\n        [ 23,   2,  17,  ...,   8,  98,  46],\n        ...,\n        [ 23,   2,   3,  ...,   1,   1,   1],\n        [ 23,   2,   4,  ...,   1,   1,   1],\n        [ 23,   2,  10,  ...,   1,   1,   1]])\ntensor([2, 1, 2, 0, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 0, 0, 2, 1,\n        1, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 1, 0, 2, 2,\n        0, 0, 0, 2, 0, 1, 2, 2, 1, 1, 1, 2, 2, 0, 0, 2, 1, 2, 0, 2, 0, 2, 1, 2,\n        0, 2, 0, 0, 0, 2, 1, 2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 2, 1, 1, 0, 0, 2, 2,\n        2, 1, 2, 0, 2, 2, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 1, 0, 0, 1, 2, 2, 1, 2,\n        2, 2, 2, 1, 2, 1, 2, 0])\ntensor([[ 9,  7, 23,  ...,  7, 31, 29],\n        [28,  8,  7,  ..., 27, 16, 15],\n        [23, 31,  2,  ..., 10,  7, 36],\n        ...,\n        [ 9,  7, 23,  ..., 25,  3, 29],\n        [ 9,  7, 23,  ...,  6,  5, 29],\n        [21, 16, 19,  ...,  8, 34, 35]])\ntensor([2, 1, 2, 1, 1, 1, 2, 0, 2, 1, 2, 1, 1, 2, 0, 2, 1, 1, 1, 1, 2, 1, 0, 2,\n        2, 2, 2, 2, 1, 1, 0, 2, 1, 0, 2, 2, 2, 0, 2, 0, 2, 1, 1, 1, 0, 1, 1, 2,\n        1, 0, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, 2, 2, 2, 1, 2, 1, 1,\n        1, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0,\n        0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n        2, 0, 1, 1, 0, 1, 2, 2])\ntensor([[23,  2, 13,  ..., 19,  5, 52],\n        [23,  2, 13,  ..., 51, 53, 51],\n        [23,  2,  3,  ...,  3,  4, 24],\n        ...,\n        [23,  2, 18,  ..., 46,  1,  1],\n        [ 9,  7, 23,  ...,  3,  1,  1],\n        [23,  2, 15,  ...,  3,  1,  1]])\ntensor([1, 1, 0, 1, 1, 1, 0, 0, 2, 2, 1, 0, 0, 1, 0, 1, 1, 0, 2, 0, 0, 0, 2, 2,\n        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 2, 2, 2, 2, 1, 0, 0, 2, 0, 2,\n        0, 0, 2, 0, 1, 1, 2, 0, 0, 1, 2, 1, 2, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 0, 2, 0, 0, 0, 1, 2, 2, 0, 2, 1,\n        2, 2, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 2, 1, 0, 2, 1, 0, 2, 2, 1, 1, 1, 1,\n        0, 1, 0, 1, 2, 2, 2, 2])\ntensor([[23,  2,  3,  ..., 43, 19,  3],\n        [23,  2, 11,  ..., 40,  4, 25],\n        [23,  2, 11,  ...,  6, 36,  8],\n        ...,\n        [23,  2, 13,  ..., 12, 18,  1],\n        [ 9,  7, 23,  ...,  4, 29,  1],\n        [ 6,  5,  2,  ..., 43,  7,  1]])\ntensor([2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 1,\n        2, 2, 0, 1, 0, 1, 1, 2, 1, 1, 0, 2, 2, 2, 1, 0, 2, 1, 0, 1, 0, 2, 2, 2,\n        1, 1, 1, 1, 0, 1, 0, 0, 0, 2, 0, 2, 2, 1, 0, 1, 2, 2, 1, 1, 1, 0, 0, 0,\n        1, 2, 2, 1, 0, 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0,\n        1, 1, 0, 2, 2, 1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1,\n        0, 1, 0, 2, 2, 2, 0, 2])\ntensor([[23,  2, 10,  ...,  6,  5, 24],\n        [23,  2,  4,  ...,  9, 38, 15],\n        [23,  2, 10,  ...,  4, 13,  5],\n        ...,\n        [27, 16, 20,  ...,  1,  1,  1],\n        [23,  2, 14,  ...,  1,  1,  1],\n        [23,  2, 13,  ...,  1,  1,  1]])\ntensor([0, 2, 0, 1, 1, 2, 2, 1, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 2,\n        0, 0, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 2,\n        2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 0, 2,\n        1, 1, 2, 2, 2, 0, 2, 1, 1, 0, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 0, 2, 1, 0,\n        2, 0, 1, 2, 0, 0, 2, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 2, 2, 0, 2, 0, 1, 0,\n        2, 2, 0, 1, 2, 0, 1, 2])\n"
    }
   ],
   "source": [
    "print(train_iter)\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(batch.text)\n",
    "    print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, bidir=False, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(2*n_hidden, n_output)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out[:,-1])\n",
    "        log_probs = F.log_softmax(out)\n",
    "\n",
    "        # sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        # sigmoid_out = fc_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        # sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        return log_probs\n",
    "        # return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(logits, labels):\n",
    "    correct, total = 0, 0\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "#     print(predicted, labels)\n",
    "#     print(predicted.shape, labels.shape)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        h = model.init_hidden(len(batch))\n",
    "        batch.text = batch.text.to(device)\n",
    "        # predictions, _ = model(batch.text)\n",
    "        predictions = model(batch.text)\n",
    "        # predictions = predictions.squeeze(1)\n",
    "        # print(batch.text.shape, predictions.shape, batch.label.shape)\n",
    "\n",
    "        # target = torch.tensor(batch.label, dtype=torch.float, device=device)\n",
    "        loss = criterion(predictions, batch.label.to(device))\n",
    "\n",
    "        acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label.to(device))\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_metrics(model, config):\n",
    "    train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])\n",
    "    exp_name = config[\"NAME\"]\n",
    "    test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=len(test),\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True)        # sort within each batch\n",
    "    print(\"BEST METRICS VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "            conf = confusion_matrix(batch.label.cpu(), preds.cpu())\n",
    "            pp.pprint(conf)\n",
    "            \n",
    "\n",
    "    print(\"BEST METRICS TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-train.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "            conf = confusion_matrix(batch.label.cpu(), preds.cpu())\n",
    "            pp.pprint(conf)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, config):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    exp_name = config[\"NAME\"]\n",
    "    print(\"TEST VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    print(\"TEST TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-train.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(model, config):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = config[\"LR\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = config[\"N_EPOCHS\"]\n",
    "    exp_name = config[\"NAME\"]\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    print(\"BEGIN TRAINING\")\n",
    "    print(\"-\"*50)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(\"SAVED VALID\")\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-valid.pt'.format(exp_name))\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            print(\"SAVED TRAIN\")\n",
    "            best_train_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-train.pt'.format(exp_name))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning Sentiment Analysis, Random Embeddings, hinglish\n{   'BIDIR': True,\n    'DRPOUT': 0.5,\n    'EMB_TRAIN': True,\n    'LR': 0.01,\n    'NAME': 'LSTM_RandomEmbeddings_hinglish',\n    'N_EMBED': 400,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 130,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 899}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nSAVED VALID\nSAVED TRAIN\nEpoch: 01 | Epoch Time: 0m 4s\n\tTrain Loss: 1.331 | Train Acc: 35.82%\n\t Val. Loss: 1.129 |  Val. Acc: 36.18%\nSAVED VALID\nSAVED TRAIN\nEpoch: 02 | Epoch Time: 0m 4s\n\tTrain Loss: 1.099 | Train Acc: 37.69%\n\t Val. Loss: 1.101 |  Val. Acc: 36.63%\nSAVED VALID\nSAVED TRAIN\nEpoch: 03 | Epoch Time: 0m 4s\n\tTrain Loss: 1.093 | Train Acc: 38.13%\n\t Val. Loss: 1.084 |  Val. Acc: 38.56%\nEpoch: 04 | Epoch Time: 0m 4s\n\tTrain Loss: 1.090 | Train Acc: 38.06%\n\t Val. Loss: 1.104 |  Val. Acc: 36.46%\nSAVED VALID\nEpoch: 05 | Epoch Time: 0m 4s\n\tTrain Loss: 1.086 | Train Acc: 38.61%\n\t Val. Loss: 1.084 |  Val. Acc: 36.79%\nEpoch: 06 | Epoch Time: 0m 4s\n\tTrain Loss: 1.085 | Train Acc: 38.57%\n\t Val. Loss: 1.097 |  Val. Acc: 36.09%\nSAVED TRAIN\nEpoch: 07 | Epoch Time: 0m 4s\n\tTrain Loss: 1.080 | Train Acc: 39.38%\n\t Val. Loss: 1.089 |  Val. Acc: 37.98%\nSAVED VALID\nSAVED TRAIN\nEpoch: 08 | Epoch Time: 0m 4s\n\tTrain Loss: 1.083 | Train Acc: 39.05%\n\t Val. Loss: 1.077 |  Val. Acc: 39.10%\nEpoch: 09 | Epoch Time: 0m 4s\n\tTrain Loss: 1.079 | Train Acc: 39.30%\n\t Val. Loss: 1.080 |  Val. Acc: 37.10%\nSAVED VALID\nSAVED TRAIN\nEpoch: 10 | Epoch Time: 0m 4s\n\tTrain Loss: 1.069 | Train Acc: 40.23%\n\t Val. Loss: 1.043 |  Val. Acc: 42.61%\nSAVED VALID\nEpoch: 11 | Epoch Time: 0m 4s\n\tTrain Loss: 1.051 | Train Acc: 42.80%\n\t Val. Loss: 1.036 |  Val. Acc: 43.59%\nSAVED VALID\nSAVED TRAIN\nEpoch: 12 | Epoch Time: 0m 4s\n\tTrain Loss: 1.043 | Train Acc: 44.08%\n\t Val. Loss: 1.031 |  Val. Acc: 42.45%\nSAVED VALID\nSAVED TRAIN\nEpoch: 13 | Epoch Time: 0m 4s\n\tTrain Loss: 1.031 | Train Acc: 45.27%\n\t Val. Loss: 1.019 |  Val. Acc: 46.04%\nEpoch: 14 | Epoch Time: 0m 4s\n\tTrain Loss: 1.022 | Train Acc: 45.37%\n\t Val. Loss: 1.026 |  Val. Acc: 46.69%\nSAVED TRAIN\nEpoch: 15 | Epoch Time: 0m 4s\n\tTrain Loss: 1.019 | Train Acc: 45.90%\n\t Val. Loss: 1.025 |  Val. Acc: 45.03%\nSAVED TRAIN\nEpoch: 16 | Epoch Time: 0m 4s\n\tTrain Loss: 1.016 | Train Acc: 46.36%\n\t Val. Loss: 1.023 |  Val. Acc: 45.98%\nSAVED TRAIN\nEpoch: 17 | Epoch Time: 0m 4s\n\tTrain Loss: 1.013 | Train Acc: 46.66%\n\t Val. Loss: 1.029 |  Val. Acc: 44.42%\nSAVED TRAIN\nEpoch: 18 | Epoch Time: 0m 4s\n\tTrain Loss: 1.011 | Train Acc: 46.81%\n\t Val. Loss: 1.030 |  Val. Acc: 43.98%\nSAVED VALID\nSAVED TRAIN\nEpoch: 19 | Epoch Time: 0m 4s\n\tTrain Loss: 1.005 | Train Acc: 47.88%\n\t Val. Loss: 1.001 |  Val. Acc: 47.06%\nEpoch: 20 | Epoch Time: 0m 4s\n\tTrain Loss: 1.003 | Train Acc: 46.81%\n\t Val. Loss: 1.012 |  Val. Acc: 46.82%\nEpoch: 21 | Epoch Time: 0m 4s\n\tTrain Loss: 1.002 | Train Acc: 48.00%\n\t Val. Loss: 1.016 |  Val. Acc: 45.23%\nSAVED TRAIN\nEpoch: 22 | Epoch Time: 0m 4s\n\tTrain Loss: 0.998 | Train Acc: 48.36%\n\t Val. Loss: 1.018 |  Val. Acc: 45.73%\nSAVED TRAIN\nEpoch: 23 | Epoch Time: 0m 4s\n\tTrain Loss: 1.002 | Train Acc: 47.38%\n\t Val. Loss: 1.006 |  Val. Acc: 46.19%\nSAVED VALID\nSAVED TRAIN\nEpoch: 24 | Epoch Time: 0m 4s\n\tTrain Loss: 0.997 | Train Acc: 48.29%\n\t Val. Loss: 1.000 |  Val. Acc: 47.49%\nSAVED VALID\nSAVED TRAIN\nEpoch: 25 | Epoch Time: 0m 4s\n\tTrain Loss: 0.998 | Train Acc: 48.08%\n\t Val. Loss: 0.997 |  Val. Acc: 47.96%\nEpoch: 26 | Epoch Time: 0m 4s\n\tTrain Loss: 1.000 | Train Acc: 47.64%\n\t Val. Loss: 1.001 |  Val. Acc: 46.12%\nSAVED TRAIN\nEpoch: 27 | Epoch Time: 0m 4s\n\tTrain Loss: 0.992 | Train Acc: 48.43%\n\t Val. Loss: 1.014 |  Val. Acc: 45.93%\nSAVED TRAIN\nEpoch: 28 | Epoch Time: 0m 4s\n\tTrain Loss: 0.988 | Train Acc: 49.41%\n\t Val. Loss: 1.017 |  Val. Acc: 45.92%\nSAVED TRAIN\nEpoch: 29 | Epoch Time: 0m 4s\n\tTrain Loss: 0.990 | Train Acc: 49.05%\n\t Val. Loss: 1.005 |  Val. Acc: 47.38%\nSAVED TRAIN\nEpoch: 30 | Epoch Time: 0m 4s\n\tTrain Loss: 0.987 | Train Acc: 49.00%\n\t Val. Loss: 1.004 |  Val. Acc: 48.87%\nSAVED VALID\nSAVED TRAIN\nEpoch: 31 | Epoch Time: 0m 4s\n\tTrain Loss: 0.985 | Train Acc: 49.94%\n\t Val. Loss: 0.993 |  Val. Acc: 47.79%\nSAVED TRAIN\nEpoch: 32 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.37%\n\t Val. Loss: 0.995 |  Val. Acc: 48.06%\nSAVED TRAIN\nEpoch: 33 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.30%\n\t Val. Loss: 1.019 |  Val. Acc: 44.74%\nSAVED VALID\nSAVED TRAIN\nEpoch: 34 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.62%\n\t Val. Loss: 0.993 |  Val. Acc: 48.74%\nSAVED VALID\nSAVED TRAIN\nEpoch: 35 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.13%\n\t Val. Loss: 0.988 |  Val. Acc: 50.32%\nSAVED TRAIN\nEpoch: 36 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 49.76%\n\t Val. Loss: 0.993 |  Val. Acc: 49.32%\nSAVED TRAIN\nEpoch: 37 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.31%\n\t Val. Loss: 1.013 |  Val. Acc: 47.24%\nSAVED VALID\nSAVED TRAIN\nEpoch: 38 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.69%\n\t Val. Loss: 0.984 |  Val. Acc: 50.32%\nSAVED VALID\nSAVED TRAIN\nEpoch: 39 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.43%\n\t Val. Loss: 0.980 |  Val. Acc: 51.45%\nSAVED TRAIN\nEpoch: 40 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.87%\n\t Val. Loss: 0.993 |  Val. Acc: 50.13%\nSAVED TRAIN\nEpoch: 41 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 50.13%\n\t Val. Loss: 0.989 |  Val. Acc: 49.15%\nSAVED TRAIN\nEpoch: 42 | Epoch Time: 0m 4s\n\tTrain Loss: 0.971 | Train Acc: 50.48%\n\t Val. Loss: 0.991 |  Val. Acc: 48.73%\nSAVED TRAIN\nEpoch: 43 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 49.78%\n\t Val. Loss: 0.994 |  Val. Acc: 48.51%\nSAVED TRAIN\nEpoch: 44 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.75%\n\t Val. Loss: 0.991 |  Val. Acc: 49.86%\nSAVED TRAIN\nEpoch: 45 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.11%\n\t Val. Loss: 0.985 |  Val. Acc: 49.48%\nSAVED TRAIN\nEpoch: 46 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.64%\n\t Val. Loss: 0.987 |  Val. Acc: 47.63%\nSAVED TRAIN\nEpoch: 47 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.31%\n\t Val. Loss: 0.997 |  Val. Acc: 48.46%\nSAVED TRAIN\nEpoch: 48 | Epoch Time: 0m 4s\n\tTrain Loss: 0.982 | Train Acc: 49.25%\n\t Val. Loss: 1.002 |  Val. Acc: 47.44%\nSAVED VALID\nSAVED TRAIN\nEpoch: 49 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.88%\n\t Val. Loss: 0.972 |  Val. Acc: 49.73%\nEpoch: 50 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 50.54%\n\t Val. Loss: 0.982 |  Val. Acc: 49.47%\nEpoch: 51 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 50.21%\n\t Val. Loss: 0.972 |  Val. Acc: 50.61%\nEpoch: 52 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 50.37%\n\t Val. Loss: 1.016 |  Val. Acc: 46.95%\nEpoch: 53 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.78%\n\t Val. Loss: 0.986 |  Val. Acc: 51.01%\nEpoch: 54 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.80%\n\t Val. Loss: 0.982 |  Val. Acc: 50.71%\nEpoch: 55 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 50.19%\n\t Val. Loss: 0.995 |  Val. Acc: 47.42%\nEpoch: 56 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.66%\n\t Val. Loss: 0.985 |  Val. Acc: 50.50%\nEpoch: 57 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.36%\n\t Val. Loss: 0.989 |  Val. Acc: 49.39%\nEpoch: 58 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.64%\n\t Val. Loss: 0.987 |  Val. Acc: 51.07%\nEpoch: 59 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 50.11%\n\t Val. Loss: 0.996 |  Val. Acc: 49.45%\nEpoch: 60 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.89%\n\t Val. Loss: 0.980 |  Val. Acc: 50.31%\nEpoch: 61 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.58%\n\t Val. Loss: 0.983 |  Val. Acc: 50.01%\nEpoch: 62 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 49.79%\n\t Val. Loss: 0.992 |  Val. Acc: 47.33%\nEpoch: 63 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.07%\n\t Val. Loss: 0.984 |  Val. Acc: 49.16%\nEpoch: 64 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.43%\n\t Val. Loss: 0.980 |  Val. Acc: 48.06%\nEpoch: 65 | Epoch Time: 0m 4s\n\tTrain Loss: 0.982 | Train Acc: 49.62%\n\t Val. Loss: 0.992 |  Val. Acc: 49.35%\nEpoch: 66 | Epoch Time: 0m 4s\n\tTrain Loss: 0.972 | Train Acc: 49.96%\n\t Val. Loss: 0.984 |  Val. Acc: 47.29%\nEpoch: 67 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.89%\n\t Val. Loss: 1.005 |  Val. Acc: 48.56%\nEpoch: 68 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.76%\n\t Val. Loss: 0.987 |  Val. Acc: 49.60%\nEpoch: 69 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.88%\n\t Val. Loss: 0.983 |  Val. Acc: 48.83%\nSAVED TRAIN\nEpoch: 70 | Epoch Time: 0m 4s\n\tTrain Loss: 0.972 | Train Acc: 50.46%\n\t Val. Loss: 0.995 |  Val. Acc: 47.98%\nSAVED TRAIN\nEpoch: 71 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.40%\n\t Val. Loss: 0.981 |  Val. Acc: 50.08%\nSAVED TRAIN\nEpoch: 72 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.15%\n\t Val. Loss: 0.983 |  Val. Acc: 49.38%\nSAVED TRAIN\nEpoch: 73 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.17%\n\t Val. Loss: 0.977 |  Val. Acc: 49.48%\nSAVED TRAIN\nEpoch: 74 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 49.79%\n\t Val. Loss: 0.976 |  Val. Acc: 50.76%\nEpoch: 75 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.99%\n\t Val. Loss: 0.975 |  Val. Acc: 52.47%\nSAVED TRAIN\nEpoch: 76 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.82%\n\t Val. Loss: 0.978 |  Val. Acc: 50.62%\nSAVED TRAIN\nEpoch: 77 | Epoch Time: 0m 4s\n\tTrain Loss: 0.971 | Train Acc: 49.97%\n\t Val. Loss: 0.994 |  Val. Acc: 50.14%\nSAVED TRAIN\nEpoch: 78 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.00%\n\t Val. Loss: 0.989 |  Val. Acc: 48.36%\nSAVED TRAIN\nEpoch: 79 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 49.75%\n\t Val. Loss: 0.981 |  Val. Acc: 50.28%\nSAVED VALID\nSAVED TRAIN\nEpoch: 80 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.70%\n\t Val. Loss: 0.968 |  Val. Acc: 50.70%\nEpoch: 81 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.11%\n\t Val. Loss: 0.985 |  Val. Acc: 50.34%\nEpoch: 82 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.71%\n\t Val. Loss: 0.985 |  Val. Acc: 50.20%\nEpoch: 83 | Epoch Time: 0m 4s\n\tTrain Loss: 0.969 | Train Acc: 50.64%\n\t Val. Loss: 0.990 |  Val. Acc: 49.98%\nEpoch: 84 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.52%\n\t Val. Loss: 0.981 |  Val. Acc: 50.60%\nEpoch: 85 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.86%\n\t Val. Loss: 0.978 |  Val. Acc: 50.26%\nEpoch: 86 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 49.66%\n\t Val. Loss: 0.990 |  Val. Acc: 48.04%\nEpoch: 87 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.92%\n\t Val. Loss: 0.991 |  Val. Acc: 48.74%\nEpoch: 88 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.79%\n\t Val. Loss: 0.984 |  Val. Acc: 49.05%\nEpoch: 89 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 50.02%\n\t Val. Loss: 0.990 |  Val. Acc: 49.24%\nEpoch: 90 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.55%\n\t Val. Loss: 0.980 |  Val. Acc: 49.63%\nEpoch: 91 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 48.95%\n\t Val. Loss: 1.003 |  Val. Acc: 48.40%\nEpoch: 92 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.59%\n\t Val. Loss: 0.991 |  Val. Acc: 47.87%\nEpoch: 93 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.61%\n\t Val. Loss: 0.987 |  Val. Acc: 47.64%\nEpoch: 94 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.48%\n\t Val. Loss: 0.981 |  Val. Acc: 49.26%\nEpoch: 95 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 49.16%\n\t Val. Loss: 0.974 |  Val. Acc: 49.72%\nEpoch: 96 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.58%\n\t Val. Loss: 0.979 |  Val. Acc: 51.01%\nEpoch: 97 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.29%\n\t Val. Loss: 0.978 |  Val. Acc: 50.25%\nEpoch: 98 | Epoch Time: 0m 4s\n\tTrain Loss: 0.984 | Train Acc: 49.36%\n\t Val. Loss: 0.970 |  Val. Acc: 51.71%\nEpoch: 99 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.74%\n\t Val. Loss: 0.970 |  Val. Acc: 51.46%\nEpoch: 100 | Epoch Time: 0m 4s\n\tTrain Loss: 0.986 | Train Acc: 49.40%\n\t Val. Loss: 0.980 |  Val. Acc: 50.39%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTEST VALID\n\t Test. Loss: 0.957 |  Test. Acc: 51.46%\nTEST TRAIN\n\t Test. Loss: 0.961 |  Test. Acc: 51.05%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\nGET METRICS\n--------------------------------------------------\nBEST METRICS VALID\nTest f1: 0.510 | Test Prec: 51.20% | Test Recall: 51.20% \narray([[263,  34, 144],\n       [ 78, 260, 168],\n       [201, 124, 241]])\nBEST METRICS TRAIN\nTest f1: 0.510 | Test Prec: 51.20% | Test Recall: 51.20% \narray([[263,  34, 144],\n       [ 78, 260, 168],\n       [201, 124, 241]])\n"
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(\"-\"*50)\n",
    "print(\"Running Sentiment Analysis, Random Embeddings, hinglish\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_RandomEmbeddings_hinglish\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : 400,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_HIDDEN\" : 130,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"BIDIR\": True,\n",
    "    \"DRPOUT\": 0.5,\n",
    "    \"LR\": 0.01\n",
    "}\n",
    "model = SentimentLSTM(config[\"N_VOCAB\"],config[\"N_EMBED\"], config[\"N_HIDDEN\"], config[\"N_OUTPUT\"], config[\"N_LAYERS\"], config[\"BIDIR\"], config[\"DRPOUT\"])\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)\n",
    "print(\"GET METRICS\")\n",
    "print(\"-\"*50)\n",
    "get_metrics(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = config[\"N_VOCAB\"]     # number of unique words in vocabulary\n",
    "        self.n_layers = config[\"N_LAYERS\"]   # number of LSTM layers \n",
    "        self.n_hidden = config[\"N_HIDDEN\"]   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.n_vocab, config[\"N_EMBED\"])\n",
    "        self.embedding.weight.data = torch.eye(self.n_vocab)\n",
    "        # make embedding untrainable\n",
    "        if not config[\"EMB_TRAIN\"]:\n",
    "            self.embedding.weight.requires_grad=False\n",
    "        self.lstm = nn.LSTM(config[\"N_EMBED\"], self.n_hidden, self.n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(self.n_hidden, config[\"N_OUTPUT\"])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Untrainable\n{   'EMB_TRAIN': False,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Untrainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 1,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 49.86%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Untrainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Untrainable\",\n",
    "    \"N_EPOCHS\": 1,\n",
    "    \"EMB_TRAIN\": False,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Trainable\n{   'EMB_TRAIN': True,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Trainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 9s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.82%\nEpoch: 02 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 03 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 50.08%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 04 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.02%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 05 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.78%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 06 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.20%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 07 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 48.80%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 08 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 46.21%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 09 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 44.87%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 10 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 44.31%\n\t Val. Loss: 0.693 |  Val. Acc: 49.78%\nEpoch: 11 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 41.84%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 12 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 40.35%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 13 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.64%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 14 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.10%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 15 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 16 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.21%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 17 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 34.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 18 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.87%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 19 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.33%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 20 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.41%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 21 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.17%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 22 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.15%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 23 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.02%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 24 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.90%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 25 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.00%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 26 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 27 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.96%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 28 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.93%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 29 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 30 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.94%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 31 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 32 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 33 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 34 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 35 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 36 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 37 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 38 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 39 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 40 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 41 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 42 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 43 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 44 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 45 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 46 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 47 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 48 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 49 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 50 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 51 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 52 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 53 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 54 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 55 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 56 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 57 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 58 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 59 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 60 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 61 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 62 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 63 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 64 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 65 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 66 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 67 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 68 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 69 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 70 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 71 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 72 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 73 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 74 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 75 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 76 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 77 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.41%\nEpoch: 78 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 79 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 80 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 81 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 82 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 83 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 84 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 85 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 86 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 87 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 88 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 89 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 90 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 91 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 92 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 93 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 94 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 95 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 96 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 97 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 98 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 99 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 100 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 34.29%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Trainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Trainable\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}