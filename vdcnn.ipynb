{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "random.seed(0)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/train_conll_spanglish.csv'\n",
    "import torchtext\n",
    "\n",
    "def label2int(label):\n",
    "    if label=='positive':\n",
    "        return 1\n",
    "    elif label=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x.lower(), # because are building a character-RNN\n",
    "                                  include_lengths=False, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "label_field = torchtext.data.Field(sequential=False,    # not a sequence\n",
    "                                   use_vocab=False,     # don't need to track vocabulary\n",
    "                                   is_target=True,      \n",
    "                                   batch_first=True,\n",
    "                                   preprocessing=lambda x: label2int(x)) # convert text to 0 and 1\n",
    "\n",
    "fields = [('id', None),('text', text_field), ('label', label_field)]\n",
    "dataset = torchtext.data.TabularDataset(filename, # name of the file\n",
    "                                        \"tsv\",               # fields are separated by a tab\n",
    "                                        fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "so that means tomorrow cruda segura lol --- 1\ntonight peda segura --- 2\neres tan mala vieja bruja interesada#jamming --- 0\nyo kiero pretzels lol --- 2\nfuck that ni ke el me vaya a mantener toda la vida lol --- 0\ni always tell my dad ke me kiero kasar con una vieja rika and me regaña telling me ke no sea interesada ha --- 0\nke me compre un carrito pa irme con mis friends and party lol --- 2\nwhy can i just find a rich bitch ke me mantenga y ya ha --- 2\nsince i started working ya ni disfruto la vida lol --- 0\nmy dad me regano cuzs i was telling that to my brother and lo andaba molestando lol --- 0\n"
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(dataset[i].text, \"---\", dataset[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "12000 1500 1500 15000\n"
    }
   ],
   "source": [
    "print(len(train), len(val), len(test), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f42806ce4d0>>,\n            {'<unk>': 0,\n             '<pad>': 1,\n             ' ': 2,\n             'e': 3,\n             'a': 4,\n             'o': 5,\n             't': 6,\n             's': 7,\n             'n': 8,\n             'i': 9,\n             'r': 10,\n             'l': 11,\n             'c': 12,\n             'd': 13,\n             'u': 14,\n             'm': 15,\n             'p': 16,\n             'h': 17,\n             'y': 18,\n             'g': 19,\n             '.': 20,\n             'b': 21,\n             '/': 22,\n             'v': 23,\n             'f': 24,\n             'j': 25,\n             ':': 26,\n             '@': 27,\n             '!': 28,\n             'q': 29,\n             'k': 30,\n             'w': 31,\n             '#': 32,\n             'z': 33,\n             ',': 34,\n             'x': 35,\n             '1': 36,\n             '0': 37,\n             '2': 38,\n             '_': 39,\n             'í': 40,\n             '\"': 41,\n             '3': 42,\n             '7': 43,\n             '?': 44,\n             '6': 45,\n             '4': 46,\n             '5': 47,\n             '8': 48,\n             '9': 49,\n             'á': 50,\n             'ñ': 51,\n             \"'\": 52,\n             'é': 53,\n             '…': 54,\n             'ó': 55,\n             '-': 56,\n             '😂': 57,\n             ')': 58,\n             '(': 59,\n             '️': 60,\n             '|': 61,\n             '❤': 62,\n             '😍': 63,\n             'ú': 64,\n             '’': 65,\n             '“': 66,\n             '”': 67,\n             '^': 68,\n             '&': 69,\n             '¿': 70,\n             '😭': 71,\n             '*': 72,\n             '>': 73,\n             '$': 74,\n             ';': 75,\n             '😁': 76,\n             '¡': 77,\n             '👌': 78,\n             '・': 79,\n             '🔥': 80,\n             '🏼': 81,\n             '😱': 82,\n             '☺': 83,\n             '🚨': 84,\n             '🎉': 85,\n             '👏': 86,\n             '😘': 87,\n             '😝': 88,\n             '😎': 89,\n             '🏻': 90,\n             '🏽': 91,\n             '🙌': 92,\n             '💕': 93,\n             '😊': 94,\n             '~': 95,\n             '♥': 96,\n             '•': 97,\n             '<': 98,\n             '🎵': 99,\n             '😩': 100,\n             '💀': 101,\n             '+': 102,\n             '😜': 103,\n             '👍': 104,\n             '😋': 105,\n             '😒': 106,\n             '✌': 107,\n             '😏': 108,\n             '😡': 109,\n             '%': 110,\n             '👗': 111,\n             '💃': 112,\n             '💋': 113,\n             '💪': 114,\n             '=': 115,\n             '☀': 116,\n             '💁': 117,\n             '😔': 118,\n             '🎶': 119,\n             '😉': 120,\n             '😳': 121,\n             '😌': 122,\n             '🤔': 123,\n             '💯': 124,\n             '🍻': 125,\n             '👇': 126,\n             '😈': 127,\n             '🙄': 128,\n             '🙋': 129,\n             '👠': 130,\n             '🌸': 131,\n             '😅': 132,\n             '😫': 133,\n             '♡': 134,\n             '🎈': 135,\n             '💔': 136,\n             '😃': 137,\n             '🙈': 138,\n             '✨': 139,\n             '😞': 140,\n             '🌻': 141,\n             '💙': 142,\n             '✋': 143,\n             'ü': 144,\n             '🔫': 145,\n             '😬': 146,\n             '😻': 147,\n             '😐': 148,\n             '🌴': 149,\n             '💛': 150,\n             '😓': 151,\n             '🎊': 152,\n             '💜': 153,\n             '💥': 154,\n             '😛': 155,\n             '😆': 156,\n             '😴': 157,\n             '🙏': 158,\n             '🤘': 159,\n             '☁': 160,\n             '🌙': 161,\n             '⁰': 162,\n             '🍁': 163,\n             '🎁': 164,\n             '👊': 165,\n             '😑': 166,\n             '😢': 167,\n             ']': 168,\n             '💘': 169,\n             '😄': 170,\n             '😰': 171,\n             '😹': 172,\n             '⭐': 173,\n             '🌷': 174,\n             '🎂': 175,\n             '💄': 176,\n             '😕': 177,\n             '🤗': 178,\n             '🌼': 179,\n             '🍹': 180,\n             '🍺': 181,\n             '💓': 182,\n             '💖': 183,\n             '📲': 184,\n             '🙂': 185,\n             '🙆': 186,\n             '✈': 187,\n             '❄': 188,\n             '🍃': 189,\n             '🏿': 190,\n             '💆': 191,\n             '😠': 192,\n             '😷': 193,\n             '🎮': 194,\n             '💦': 195,\n             '💫': 196,\n             '😟': 197,\n             '°': 198,\n             '—': 199,\n             '─': 200,\n             '🌞': 201,\n             '🎤': 202,\n             '💰': 203,\n             '🔝': 204,\n             '🔴': 205,\n             '😤': 206,\n             '😪': 207,\n             '🙃': 208,\n             '❗': 209,\n             '🇵': 210,\n             '🇷': 211,\n             '👖': 212,\n             '👛': 213,\n             '💚': 214,\n             '💣': 215,\n             '💨': 216,\n             '😀': 217,\n             '😖': 218,\n             '[': 219,\n             '\\\\': 220,\n             'ɪ': 221,\n             '☎': 222,\n             '☝': 223,\n             '⚡': 224,\n             '✖': 225,\n             '🌲': 226,\n             '🌺': 227,\n             '🌾': 228,\n             '🍀': 229,\n             '🎀': 230,\n             '🎧': 231,\n             '🐶': 232,\n             '👀': 233,\n             '👅': 234,\n             '👉': 235,\n             '👔': 236,\n             '👶': 237,\n             '👿': 238,\n             '💗': 239,\n             '💤': 240,\n             '😗': 241,\n             '😚': 242,\n             '😨': 243,\n             '🙀': 244,\n             '🙊': 245,\n             '´': 246,\n             '÷': 247,\n             'ə': 248,\n             '☔': 249,\n             '☕': 250,\n             '⛄': 251,\n             '✊': 252,\n             '❣': 253,\n             '🌊': 254,\n             '🌚': 255,\n             '👋': 256,\n             '👑': 257,\n             '👙': 258,\n             '👜': 259,\n             '👭': 260,\n             '👯': 261,\n             '👼': 262,\n             '👾': 263,\n             '💌': 264,\n             '💟': 265,\n             '📩': 266,\n             '📱': 267,\n             '📸': 268,\n             '😇': 269,\n             '🙍': 270,\n             '£': 271,\n             '®': 272,\n             '–': 273,\n             '‘': 274,\n             '☹': 275,\n             '⚠': 276,\n             '⛵': 277,\n             '✅': 278,\n             '🌈': 279,\n             '🌌': 280,\n             '🌟': 281,\n             '🌤': 282,\n             '🌵': 283,\n             '🌿': 284,\n             '🍂': 285,\n             '🍇': 286,\n             '🍡': 287,\n             '🍾': 288,\n             '🎄': 289,\n             '🐀': 290,\n             '🐚': 291,\n             '🐤': 292,\n             '🐥': 293,\n             '🐸': 294,\n             '👰': 295,\n             '👻': 296,\n             '💎': 297,\n             '📍': 298,\n             '🖕': 299,\n             '😥': 300,\n             '😮': 301,\n             '🛍': 302,\n             '🦄': 303,\n             '¢': 304,\n             '×': 305,\n             'ç': 306,\n             'ˈ': 307,\n             '⌛': 308,\n             '╭': 309,\n             '╮': 310,\n             '⚽': 311,\n             '🌎': 312,\n             '🌏': 313,\n             '🌠': 314,\n             '🍒': 315,\n             '🍦': 316,\n             '🍧': 317,\n             '🍬': 318,\n             '🎋': 319,\n             '🎓': 320,\n             '🎥': 321,\n             '🏆': 322,\n             '🐝': 323,\n             '🐰': 324,\n             '🐱': 325,\n             '👎': 326,\n             '👚': 327,\n             '💍': 328,\n             '💐': 329,\n             '💞': 330,\n             '🔆': 331,\n             '🔙': 332,\n             '🔪': 333,\n             '🔺': 334,\n             '😦': 335,\n             '😶': 336,\n             '😽': 337,\n             '🙎': 338,\n             '«': 339,\n             '»': 340,\n             '½': 341,\n             'è': 342,\n             'ń': 343,\n             'ɛ': 344,\n             '͡': 345,\n             'ت': 346,\n             '€': 347,\n             '√': 348,\n             '⏰': 349,\n             '⏱': 350,\n             '▶': 351,\n             '☢': 352,\n             '⚓': 353,\n             '⚜': 354,\n             '⚪': 355,\n             '⛅': 356,\n             '✂': 357,\n             '✏': 358,\n             '✳': 359,\n             '⬇': 360,\n             '🌀': 361,\n             '🌍': 362,\n             '🌱': 363,\n             '🌶': 364,\n             '🍉': 365,\n             '🍊': 366,\n             '🍍': 367,\n             '🍕': 368,\n             '🍭': 369,\n             '🍴': 370,\n             '🍷': 371,\n             '🎃': 372,\n             '🎨': 373,\n             '🎩': 374,\n             '🎼': 375,\n             '🎾': 376,\n             '🏃': 377,\n             '🏊': 378,\n             '🐟': 379,\n             '🐣': 380,\n             '🐧': 381,\n             '🐳': 382,\n             '🐵': 383,\n             '🐷': 384,\n             '👐': 385,\n             '👕': 386,\n             '👢': 387,\n             '👨': 388,\n             '👫': 389,\n             '💊': 390,\n             '💏': 391,\n             '💩': 392,\n             '💳': 393,\n             '💴': 394,\n             '💸': 395,\n             '📚': 396,\n             '📢': 397,\n             '📥': 398,\n             '📨': 399,\n             '📪': 400,\n             '📷': 401,\n             '🔅': 402,\n             '🔮': 403,\n             '🔵': 404,\n             '🖖': 405,\n             '🙅': 406,\n             '🙉': 407,\n             '🚌': 408,\n             '🚕': 409,\n             '🚢': 410,\n             '🚶': 411,\n             '🤓': 412,\n             '{': 413,\n             'º': 414,\n             'à': 415,\n             'â': 416,\n             'ä': 417,\n             'ē': 418,\n             'ņ': 419,\n             'ś': 420,\n             'ʃ': 421,\n             'ʖ': 422,\n             '͜': 423,\n             '⃣': 424,\n             '⏳': 425,\n             '│': 426,\n             '▪': 427,\n             '►': 428,\n             '☂': 429,\n             '★': 430,\n             '♠': 431,\n             '♩': 432,\n             '♪': 433,\n             '♬': 434,\n             '⚫': 435,\n             '⚾': 436,\n             '✧': 437,\n             '❌': 438,\n             '〰': 439,\n             'ツ': 440,\n             '🆔': 441,\n             '🇨': 442,\n             '🇮': 443,\n             '🇱': 444,\n             '🇲': 445,\n             '🇸': 446,\n             '🇹': 447,\n             '🇺': 448,\n             '🇽': 449,\n             '🌂': 450,\n             '🌃': 451,\n             '🌄': 452,\n             '🌑': 453,\n             '🌝': 454,\n             '🌮': 455,\n             '🌯': 456,\n             '🌳': 457,\n             '🌹': 458,\n             '🍎': 459,\n             '🍐': 460,\n             '🍓': 461,\n             '🍔': 462,\n             '🍗': 463,\n             '🍘': 464,\n             '🍚': 465,\n             '🍠': 466,\n             '🍢': 467,\n             '🍣': 468,\n             '🍥': 469,\n             '🍫': 470,\n             '🍰': 471,\n             '🍱': 472,\n             '🍲': 473,\n             '🍸': 474,\n             '🍼': 475,\n             '🍿': 476,\n             '🎆': 477,\n             '🎌': 478,\n             '🎬': 479,\n             '🎸': 480,\n             '🏀': 481,\n             '🏁': 482,\n             '🏈': 483,\n             '🏐': 484,\n             '🏝': 485,\n             '🏠': 486,\n             '🏡': 487,\n             '🏥': 488,\n             '🏾': 489,\n             '🐖': 490,\n             '🐢': 491,\n             '🐦': 492,\n             '🐯': 493,\n             '🐻': 494,\n             '🐼': 495,\n             '👁': 496,\n             '👆': 497,\n             '👓': 498,\n             '👞': 499,\n             '👣': 500,\n             '👤': 501,\n             '👩': 502,\n             '👴': 503,\n             '👸': 504,\n             '👹': 505,\n             '👺': 506,\n             '👽': 507,\n             '💂': 508,\n             '💉': 509,\n             '💑': 510,\n             '💒': 511,\n             '💝': 512,\n             '💭': 513,\n             '💮': 514,\n             '💵': 515,\n             '💶': 516,\n             '💷': 517,\n             '💻': 518,\n             '💼': 519,\n             '📀': 520,\n             '📄': 521,\n             '📇': 522,\n             '📒': 523,\n             '📓': 524,\n             '📝': 525,\n             '📫': 526,\n             '📬': 527,\n             '📹': 528,\n             '📻': 529,\n             '🔄': 530,\n             '🔊': 531,\n             '🔘': 532,\n             '🔜': 533,\n             '🔞': 534,\n             '🔬': 535,\n             '🔭': 536,\n             '🕰': 537,\n             '🖊': 538,\n             '🖌': 539,\n             '🗻': 540,\n             '🗽': 541,\n             '😲': 542,\n             '😺': 543,\n             '😼': 544,\n             '😾': 545,\n             '🚖': 546,\n             '🚗': 547,\n             '🚙': 548,\n             '🚷': 549,\n             '🛳': 550,\n             '🤒': 551,\n             '🦃': 552,\n             '\\U000fe1d2': 553,\n             '\\U000fe343': 554})"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(dataset)\n",
    "text_field.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['<unk>',\n '<pad>',\n ' ',\n 'e',\n 'a',\n 'o',\n 't',\n 's',\n 'n',\n 'i',\n 'r',\n 'l',\n 'c',\n 'd',\n 'u',\n 'm',\n 'p',\n 'h',\n 'y',\n 'g',\n '.',\n 'b',\n '/',\n 'v',\n 'f',\n 'j',\n ':',\n '@',\n '!',\n 'q',\n 'k',\n 'w',\n '#',\n 'z',\n ',',\n 'x',\n '1',\n '0',\n '2',\n '_',\n 'í',\n '\"',\n '3',\n '7',\n '?',\n '6',\n '4',\n '5',\n '8',\n '9',\n 'á',\n 'ñ',\n \"'\",\n 'é',\n '…',\n 'ó',\n '-',\n '😂',\n ')',\n '(',\n '️',\n '|',\n '❤',\n '😍',\n 'ú',\n '’',\n '“',\n '”',\n '^',\n '&',\n '¿',\n '😭',\n '*',\n '>',\n '$',\n ';',\n '😁',\n '¡',\n '👌',\n '・',\n '🔥',\n '🏼',\n '😱',\n '☺',\n '🚨',\n '🎉',\n '👏',\n '😘',\n '😝',\n '😎',\n '🏻',\n '🏽',\n '🙌',\n '💕',\n '😊',\n '~',\n '♥',\n '•',\n '<',\n '🎵',\n '😩',\n '💀',\n '+',\n '😜',\n '👍',\n '😋',\n '😒',\n '✌',\n '😏',\n '😡',\n '%',\n '👗',\n '💃',\n '💋',\n '💪',\n '=',\n '☀',\n '💁',\n '😔',\n '🎶',\n '😉',\n '😳',\n '😌',\n '🤔',\n '💯',\n '🍻',\n '👇',\n '😈',\n '🙄',\n '🙋',\n '👠',\n '🌸',\n '😅',\n '😫',\n '♡',\n '🎈',\n '💔',\n '😃',\n '🙈',\n '✨',\n '😞',\n '🌻',\n '💙',\n '✋',\n 'ü',\n '🔫',\n '😬',\n '😻',\n '😐',\n '🌴',\n '💛',\n '😓',\n '🎊',\n '💜',\n '💥',\n '😛',\n '😆',\n '😴',\n '🙏',\n '🤘',\n '☁',\n '🌙',\n '⁰',\n '🍁',\n '🎁',\n '👊',\n '😑',\n '😢',\n ']',\n '💘',\n '😄',\n '😰',\n '😹',\n '⭐',\n '🌷',\n '🎂',\n '💄',\n '😕',\n '🤗',\n '🌼',\n '🍹',\n '🍺',\n '💓',\n '💖',\n '📲',\n '🙂',\n '🙆',\n '✈',\n '❄',\n '🍃',\n '🏿',\n '💆',\n '😠',\n '😷',\n '🎮',\n '💦',\n '💫',\n '😟',\n '°',\n '—',\n '─',\n '🌞',\n '🎤',\n '💰',\n '🔝',\n '🔴',\n '😤',\n '😪',\n '🙃',\n '❗',\n '🇵',\n '🇷',\n '👖',\n '👛',\n '💚',\n '💣',\n '💨',\n '😀',\n '😖',\n '[',\n '\\\\',\n 'ɪ',\n '☎',\n '☝',\n '⚡',\n '✖',\n '🌲',\n '🌺',\n '🌾',\n '🍀',\n '🎀',\n '🎧',\n '🐶',\n '👀',\n '👅',\n '👉',\n '👔',\n '👶',\n '👿',\n '💗',\n '💤',\n '😗',\n '😚',\n '😨',\n '🙀',\n '🙊',\n '´',\n '÷',\n 'ə',\n '☔',\n '☕',\n '⛄',\n '✊',\n '❣',\n '🌊',\n '🌚',\n '👋',\n '👑',\n '👙',\n '👜',\n '👭',\n '👯',\n '👼',\n '👾',\n '💌',\n '💟',\n '📩',\n '📱',\n '📸',\n '😇',\n '🙍',\n '£',\n '®',\n '–',\n '‘',\n '☹',\n '⚠',\n '⛵',\n '✅',\n '🌈',\n '🌌',\n '🌟',\n '🌤',\n '🌵',\n '🌿',\n '🍂',\n '🍇',\n '🍡',\n '🍾',\n '🎄',\n '🐀',\n '🐚',\n '🐤',\n '🐥',\n '🐸',\n '👰',\n '👻',\n '💎',\n '📍',\n '🖕',\n '😥',\n '😮',\n '🛍',\n '🦄',\n '¢',\n '×',\n 'ç',\n 'ˈ',\n '⌛',\n '╭',\n '╮',\n '⚽',\n '🌎',\n '🌏',\n '🌠',\n '🍒',\n '🍦',\n '🍧',\n '🍬',\n '🎋',\n '🎓',\n '🎥',\n '🏆',\n '🐝',\n '🐰',\n '🐱',\n '👎',\n '👚',\n '💍',\n '💐',\n '💞',\n '🔆',\n '🔙',\n '🔪',\n '🔺',\n '😦',\n '😶',\n '😽',\n '🙎',\n '«',\n '»',\n '½',\n 'è',\n 'ń',\n 'ɛ',\n '͡',\n 'ت',\n '€',\n '√',\n '⏰',\n '⏱',\n '▶',\n '☢',\n '⚓',\n '⚜',\n '⚪',\n '⛅',\n '✂',\n '✏',\n '✳',\n '⬇',\n '🌀',\n '🌍',\n '🌱',\n '🌶',\n '🍉',\n '🍊',\n '🍍',\n '🍕',\n '🍭',\n '🍴',\n '🍷',\n '🎃',\n '🎨',\n '🎩',\n '🎼',\n '🎾',\n '🏃',\n '🏊',\n '🐟',\n '🐣',\n '🐧',\n '🐳',\n '🐵',\n '🐷',\n '👐',\n '👕',\n '👢',\n '👨',\n '👫',\n '💊',\n '💏',\n '💩',\n '💳',\n '💴',\n '💸',\n '📚',\n '📢',\n '📥',\n '📨',\n '📪',\n '📷',\n '🔅',\n '🔮',\n '🔵',\n '🖖',\n '🙅',\n '🙉',\n '🚌',\n '🚕',\n '🚢',\n '🚶',\n '🤓',\n '{',\n 'º',\n 'à',\n 'â',\n 'ä',\n 'ē',\n 'ņ',\n 'ś',\n 'ʃ',\n 'ʖ',\n '͜',\n '⃣',\n '⏳',\n '│',\n '▪',\n '►',\n '☂',\n '★',\n '♠',\n '♩',\n '♪',\n '♬',\n '⚫',\n '⚾',\n '✧',\n '❌',\n '〰',\n 'ツ',\n '🆔',\n '🇨',\n '🇮',\n '🇱',\n '🇲',\n '🇸',\n '🇹',\n '🇺',\n '🇽',\n '🌂',\n '🌃',\n '🌄',\n '🌑',\n '🌝',\n '🌮',\n '🌯',\n '🌳',\n '🌹',\n '🍎',\n '🍐',\n '🍓',\n '🍔',\n '🍗',\n '🍘',\n '🍚',\n '🍠',\n '🍢',\n '🍣',\n '🍥',\n '🍫',\n '🍰',\n '🍱',\n '🍲',\n '🍸',\n '🍼',\n '🍿',\n '🎆',\n '🎌',\n '🎬',\n '🎸',\n '🏀',\n '🏁',\n '🏈',\n '🏐',\n '🏝',\n '🏠',\n '🏡',\n '🏥',\n '🏾',\n '🐖',\n '🐢',\n '🐦',\n '🐯',\n '🐻',\n '🐼',\n '👁',\n '👆',\n '👓',\n '👞',\n '👣',\n '👤',\n '👩',\n '👴',\n '👸',\n '👹',\n '👺',\n '👽',\n '💂',\n '💉',\n '💑',\n '💒',\n '💝',\n '💭',\n '💮',\n '💵',\n '💶',\n '💷',\n '💻',\n '💼',\n '📀',\n '📄',\n '📇',\n '📒',\n '📓',\n '📝',\n '📫',\n '📬',\n '📹',\n '📻',\n '🔄',\n '🔊',\n '🔘',\n '🔜',\n '🔞',\n '🔬',\n '🔭',\n '🕰',\n '🖊',\n '🖌',\n '🗻',\n '🗽',\n '😲',\n '😺',\n '😼',\n '😾',\n '🚖',\n '🚗',\n '🚙',\n '🚷',\n '🛳',\n '🤒',\n '🦃',\n '\\U000fe1d2',\n '\\U000fe343']"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch = 128\n",
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                          )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "val_iter = torchtext.data.BucketIterator(val,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                        )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                         )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<torchtext.data.iterator.BucketIterator object at 0x7f4281474110>\ntensor([[  4,  18,   2,  ...,   4,   6,   5],\n        [ 10,   6,  27,  ...,  21,  37,  54],\n        [ 10,   6,  27,  ...,   7, 220,  54],\n        ...,\n        [ 10,   6,  27,  ...,  12,   4,  20],\n        [ 66,  27,  12,  ...,   9,  10,  67],\n        [ 27,  24,   4,  ...,   7,  44,  28]])\ntensor([0, 1, 1, 2, 1, 2, 0, 2, 0, 1, 1, 2, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1,\n        1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2,\n        1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1,\n        2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 2, 0, 1, 1, 1,\n        0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 2, 0, 2, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 0,\n        1, 1, 1, 2, 2, 0, 1, 0])\ntensor([[24,  3, 11,  ..., 15,  5, 10],\n        [27, 35, 35,  ...,  2,  4, 21],\n        [10,  6, 27,  ...,  7, 18, 54],\n        ...,\n        [10,  6, 27,  ..., 11,  8,  1],\n        [18,  2, 23,  ..., 26, 58,  1],\n        [11,  4,  2,  ..., 16, 24,  1]])\ntensor([2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n        0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,\n        1, 1, 1, 1, 0, 2, 2, 2, 0, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2,\n        2, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 0, 2, 2,\n        1, 1, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 0, 2,\n        2, 1, 1, 1, 1, 1, 1, 2])\ntensor([[10,  6, 27,  ..., 46,  3, 24],\n        [ 7,  9,  2,  ...,  8,  5, 28],\n        [10,  6, 27,  ..., 15,  9, 35],\n        ...,\n        [32,  6, 17,  ..., 28, 28,  1],\n        [25, 14,  7,  ...,  5, 20,  1],\n        [ 5, 21, 11,  ...,  6,  4,  1]])\ntensor([2, 0, 1, 2, 0, 2, 1, 1, 1, 2, 0, 2, 0, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2,\n        1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1,\n        1, 2, 2, 1, 1, 0, 1, 1, 2, 1, 2, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 2,\n        1, 2, 1, 0, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 2, 2, 1, 0, 0,\n        2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1,\n        2, 2, 2, 2, 0, 1, 2, 1])\ntensor([[  8,   3,  12,  ..., 225,  58,  44],\n        [  5,  25,   4,  ...,  76,  62,  60],\n        [ 27,  15,   9,  ...,  23,   3,   7],\n        ...,\n        [ 15,  18,   2,  ...,  14,  13,   1],\n        [ 41,  27,  25,  ...,   7,  87,   1],\n        [ 10,   6,  27,  ...,   6, 123,   1]])\ntensor([2, 1, 2, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2,\n        1, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2, 2, 0, 1, 0, 1, 1, 1, 1,\n        2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0,\n        1, 0, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 1, 0, 1, 2, 1, 1, 0, 1,\n        0, 1, 1, 1, 2, 0, 0, 2, 1, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1,\n        1, 1, 0, 2, 1, 1, 1, 1])\ntensor([[27, 16,  4,  ..., 18, 18, 28],\n        [15,  3,  2,  ...,  5, 58, 20],\n        [27, 39,  9,  ..., 11,  5, 11],\n        ...,\n        [ 5, 30,  3,  ..., 19,  1,  1],\n        [24,  4, 15,  ..., 33,  1,  1],\n        [32, 12,  5,  ..., 33,  1,  1]])\ntensor([1, 2, 1, 1, 2, 2, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 2, 0, 2, 1, 0,\n        0, 1, 1, 0, 1, 2, 2, 1, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2,\n        1, 1, 1, 2, 1, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n        1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n        0, 2, 2, 1, 1, 1, 1, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1,\n        1, 1, 2, 1, 1, 2, 1, 2])\ntensor([[  3,   8,   4,  ..., 171, 171, 132],\n        [ 32,   6,  21,  ...,  45,  19,   5],\n        [  8,   3,   5,  ...,  18,  18,  18],\n        ...,\n        [  6,   3,   8,  ...,   3,  20,   1],\n        [ 18,   5,  14,  ...,   7,  20,   1],\n        [ 10,   6,  27,  ...,   8,   8,   1]])\ntensor([0, 1, 1, 0, 2, 1, 2, 1, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 0, 1, 2, 2,\n        1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 0, 0, 2, 1, 1, 2, 2, 2, 2, 0,\n        1, 1, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, 0, 2, 1, 2, 2, 1, 1, 2, 0,\n        1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 1, 2, 2, 1, 1, 2, 0, 1, 2, 1, 1, 0, 1,\n        1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1,\n        1, 2, 1, 0, 1, 0, 1, 1])\ntensor([[ 10,   5,   4,  ...,   7, 201, 141],\n        [ 27,  12,  10,  ...,  75,   2,  21],\n        [ 27,  24,   4,  ...,  11,   3,  18],\n        ...,\n        [ 27,  12,  10,  ...,   4,   1,   1],\n        [ 18,   4,   2,  ...,   7,   1,   1],\n        [  6,  17,   3,  ...,   8,   1,   1]])\ntensor([2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 0, 1, 1, 2, 2, 1, 2, 0, 1, 1, 1, 0,\n        1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 0, 1, 1, 0, 2, 2, 1,\n        2, 1, 1, 2, 1, 0, 2, 0, 1, 0, 1, 1, 2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0,\n        1, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n        1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 1, 0, 1, 1, 0, 2, 1, 1, 0, 2, 2, 1, 2, 1,\n        1, 1, 2, 2, 1, 1, 2, 1])\ntensor([[ 17,   5,  10,  ..., 175, 333, 333],\n        [ 27,  12,   4,  ...,   4,  19,   3],\n        [  6,  31,   2,  ...,  35,   6,  20],\n        ...,\n        [ 10,   6,  27,  ...,   8,   7,   1],\n        [ 32,   6,  10,  ...,   9,  12,   1],\n        [ 27,  25,   4,  ...,  18,   3,   1]])\ntensor([1, 1, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 2, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1,\n        2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 2, 1, 1, 2, 1, 2, 0, 1, 1,\n        1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 0, 1, 2, 2, 1, 2, 2, 2, 1, 1, 0, 1, 1, 2,\n        1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0,\n        1, 0, 2, 2, 1, 1, 0, 2, 2, 2, 0, 1, 1, 2, 0, 1, 1, 2, 2, 1, 2, 2, 0, 1,\n        2, 1, 0, 1, 0, 1, 1, 2])\ntensor([[ 9,  2, 31,  ..., 12,  3, 28],\n        [ 7,  9,  2,  ...,  5, 23, 31],\n        [ 8,  5,  2,  ..., 20, 20, 20],\n        ...,\n        [13,  5, 15,  ...,  6,  5,  1],\n        [27, 12, 17,  ..., 57, 78,  1],\n        [ 3, 11,  2,  ..., 35, 13,  1]])\ntensor([1, 2, 0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1, 2, 1,\n        0, 2, 0, 2, 2, 1, 1, 2, 2, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1,\n        2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 0, 1, 2, 2, 0, 1, 1, 2, 1, 0, 0, 0, 1,\n        0, 0, 1, 0, 2, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0, 2, 1, 0, 0, 1, 1, 1,\n        1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 2, 0, 2, 1, 1, 1,\n        1, 2, 2, 2, 1, 1, 1, 2])\ntensor([[ 15,   3,   2,  ...,   2,  19,   5],\n        [ 11,   3,   2,  ...,   4,  10, 106],\n        [  3,   8,   4,  ...,   5,   8,  20],\n        ...,\n        [ 15,   3,  13,  ...,  30,  30,   1],\n        [ 27,   4,   8,  ...,   3,   3,   1],\n        [ 15,   4,  51,  ...,  26,  58,   1]])\ntensor([1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 0, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1,\n        1, 0, 0, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2, 2, 2, 2, 2,\n        1, 2, 1, 1, 2, 2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n        1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1,\n        1, 2, 1, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1, 1,\n        2, 2, 0, 2, 2, 1, 2, 2])\n"
    }
   ],
   "source": [
    "print(train_iter)\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(batch.text)\n",
    "    print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, bidir=False, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(2*n_hidden, n_output)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out[:,-1])\n",
    "        log_probs = F.log_softmax(out)\n",
    "\n",
    "        # sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        # sigmoid_out = fc_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        # sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        return log_probs\n",
    "        # return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(logits, labels):\n",
    "    correct, total = 0, 0\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "#     print(predicted, labels)\n",
    "#     print(predicted.shape, labels.shape)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        h = model.init_hidden(len(batch))\n",
    "        batch.text = batch.text.to(device)\n",
    "        # predictions, _ = model(batch.text)\n",
    "        predictions = model(batch.text)\n",
    "        # predictions = predictions.squeeze(1)\n",
    "        # print(batch.text.shape, predictions.shape, batch.label.shape)\n",
    "\n",
    "        # target = torch.tensor(batch.label, dtype=torch.float, device=device)\n",
    "        loss = criterion(predictions, batch.label.to(device))\n",
    "\n",
    "        acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-121-358ba46187ca>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-121-358ba46187ca>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    predictions = model(batch.text))\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text))\n",
    "\n",
    "            loss = criterion(predictions, batch.label.to(device))\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, config):\n",
    "    train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])\n",
    "    exp_name = config[\"NAME\"]\n",
    "    test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=len(test),\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True)        # sort within each batch\n",
    "    print(\"BEST METRICS VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "\n",
    "    print(\"BEST METRICS TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, config):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    exp_name = config[\"NAME\"]\n",
    "    print(\"TEST VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    print(\"TEST TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(model, config):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = config[\"LR\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = config[\"N_EPOCHS\"]\n",
    "    exp_name = config[\"NAME\"]\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    print(\"BEGIN TRAINING\")\n",
    "    print(\"-\"*50)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(\"SAVED VALID\")\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-valid.pt'.format(exp_name))\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            print(\"SAVED TRAIN\")\n",
    "            best_train_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-train.pt'.format(exp_name))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning Sentiment Analysis, Random Embeddings, spanglish\n{   'BIDIR': True,\n    'DRPOUT': 0.5,\n    'EMB_TRAIN': True,\n    'LR': 0.01,\n    'NAME': 'LSTM_RandomEmbeddings_spanglish',\n    'N_EMBED': 300,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 100,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nSAVED VALID\nSAVED TRAIN\nEpoch: 01 | Epoch Time: 0m 2s\n\tTrain Loss: 1.158 | Train Acc: 46.76%\n\t Val. Loss: 1.017 |  Val. Acc: 49.70%\nSAVED VALID\nEpoch: 02 | Epoch Time: 0m 2s\n\tTrain Loss: 1.018 | Train Acc: 48.81%\n\t Val. Loss: 1.008 |  Val. Acc: 49.79%\nSAVED VALID\nSAVED TRAIN\nEpoch: 03 | Epoch Time: 0m 2s\n\tTrain Loss: 1.004 | Train Acc: 49.55%\n\t Val. Loss: 0.994 |  Val. Acc: 49.61%\nSAVED VALID\nEpoch: 04 | Epoch Time: 0m 2s\n\tTrain Loss: 0.995 | Train Acc: 49.42%\n\t Val. Loss: 0.978 |  Val. Acc: 50.53%\nSAVED VALID\nSAVED TRAIN\nEpoch: 05 | Epoch Time: 0m 2s\n\tTrain Loss: 0.983 | Train Acc: 49.81%\n\t Val. Loss: 0.974 |  Val. Acc: 49.96%\nEpoch: 06 | Epoch Time: 0m 2s\n\tTrain Loss: 0.978 | Train Acc: 50.02%\n\t Val. Loss: 0.980 |  Val. Acc: 49.13%\nSAVED VALID\nSAVED TRAIN\nEpoch: 07 | Epoch Time: 0m 2s\n\tTrain Loss: 0.974 | Train Acc: 49.74%\n\t Val. Loss: 0.970 |  Val. Acc: 49.10%\nSAVED TRAIN\nEpoch: 08 | Epoch Time: 0m 2s\n\tTrain Loss: 0.967 | Train Acc: 50.38%\n\t Val. Loss: 0.974 |  Val. Acc: 50.81%\nSAVED VALID\nSAVED TRAIN\nEpoch: 09 | Epoch Time: 0m 2s\n\tTrain Loss: 0.963 | Train Acc: 50.63%\n\t Val. Loss: 0.969 |  Val. Acc: 49.86%\nSAVED VALID\nSAVED TRAIN\nEpoch: 10 | Epoch Time: 0m 2s\n\tTrain Loss: 0.955 | Train Acc: 50.72%\n\t Val. Loss: 0.964 |  Val. Acc: 50.65%\nSAVED VALID\nSAVED TRAIN\nEpoch: 11 | Epoch Time: 0m 2s\n\tTrain Loss: 0.953 | Train Acc: 51.10%\n\t Val. Loss: 0.962 |  Val. Acc: 50.11%\nSAVED TRAIN\nEpoch: 12 | Epoch Time: 0m 2s\n\tTrain Loss: 0.955 | Train Acc: 50.76%\n\t Val. Loss: 0.973 |  Val. Acc: 48.10%\nSAVED VALID\nSAVED TRAIN\nEpoch: 13 | Epoch Time: 0m 2s\n\tTrain Loss: 0.956 | Train Acc: 50.90%\n\t Val. Loss: 0.959 |  Val. Acc: 51.12%\nSAVED VALID\nSAVED TRAIN\nEpoch: 14 | Epoch Time: 0m 2s\n\tTrain Loss: 0.952 | Train Acc: 50.83%\n\t Val. Loss: 0.954 |  Val. Acc: 50.82%\nSAVED TRAIN\nEpoch: 15 | Epoch Time: 0m 2s\n\tTrain Loss: 0.945 | Train Acc: 51.34%\n\t Val. Loss: 0.979 |  Val. Acc: 45.16%\nSAVED TRAIN\nEpoch: 16 | Epoch Time: 0m 2s\n\tTrain Loss: 0.947 | Train Acc: 51.12%\n\t Val. Loss: 0.959 |  Val. Acc: 48.26%\nSAVED TRAIN\nEpoch: 17 | Epoch Time: 0m 2s\n\tTrain Loss: 0.942 | Train Acc: 51.07%\n\t Val. Loss: 0.955 |  Val. Acc: 49.92%\nSAVED TRAIN\nEpoch: 18 | Epoch Time: 0m 2s\n\tTrain Loss: 0.945 | Train Acc: 51.11%\n\t Val. Loss: 0.955 |  Val. Acc: 51.45%\nSAVED VALID\nSAVED TRAIN\nEpoch: 19 | Epoch Time: 0m 2s\n\tTrain Loss: 0.940 | Train Acc: 51.74%\n\t Val. Loss: 0.953 |  Val. Acc: 49.58%\nSAVED VALID\nSAVED TRAIN\nEpoch: 20 | Epoch Time: 0m 2s\n\tTrain Loss: 0.940 | Train Acc: 51.34%\n\t Val. Loss: 0.948 |  Val. Acc: 51.34%\nSAVED TRAIN\nEpoch: 21 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 52.46%\n\t Val. Loss: 0.961 |  Val. Acc: 49.21%\nSAVED TRAIN\nEpoch: 22 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.93%\n\t Val. Loss: 0.948 |  Val. Acc: 50.52%\nSAVED TRAIN\nEpoch: 23 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.89%\n\t Val. Loss: 0.976 |  Val. Acc: 46.11%\nSAVED TRAIN\nEpoch: 24 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.35%\n\t Val. Loss: 0.963 |  Val. Acc: 48.91%\nSAVED TRAIN\nEpoch: 25 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 52.12%\n\t Val. Loss: 0.958 |  Val. Acc: 48.52%\nSAVED TRAIN\nEpoch: 26 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.68%\n\t Val. Loss: 0.965 |  Val. Acc: 50.51%\nSAVED TRAIN\nEpoch: 27 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.90%\n\t Val. Loss: 0.964 |  Val. Acc: 51.24%\nSAVED TRAIN\nEpoch: 28 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.52%\n\t Val. Loss: 0.970 |  Val. Acc: 49.58%\nSAVED TRAIN\nEpoch: 29 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.19%\n\t Val. Loss: 0.966 |  Val. Acc: 50.21%\nSAVED TRAIN\nEpoch: 30 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.80%\n\t Val. Loss: 0.974 |  Val. Acc: 50.85%\nSAVED TRAIN\nEpoch: 31 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.44%\n\t Val. Loss: 0.968 |  Val. Acc: 50.53%\nSAVED TRAIN\nEpoch: 32 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.64%\n\t Val. Loss: 0.965 |  Val. Acc: 49.67%\nSAVED TRAIN\nEpoch: 33 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.76%\n\t Val. Loss: 0.966 |  Val. Acc: 50.23%\nSAVED TRAIN\nEpoch: 34 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 52.14%\n\t Val. Loss: 0.963 |  Val. Acc: 50.50%\nSAVED TRAIN\nEpoch: 35 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.63%\n\t Val. Loss: 0.955 |  Val. Acc: 50.42%\nSAVED TRAIN\nEpoch: 36 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.11%\n\t Val. Loss: 0.962 |  Val. Acc: 47.81%\nSAVED TRAIN\nEpoch: 37 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 52.06%\n\t Val. Loss: 0.966 |  Val. Acc: 51.68%\nSAVED TRAIN\nEpoch: 38 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.63%\n\t Val. Loss: 0.971 |  Val. Acc: 51.46%\nSAVED TRAIN\nEpoch: 39 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.21%\n\t Val. Loss: 0.979 |  Val. Acc: 49.60%\nSAVED TRAIN\nEpoch: 40 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 52.78%\n\t Val. Loss: 0.967 |  Val. Acc: 50.05%\nSAVED TRAIN\nEpoch: 41 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.93%\n\t Val. Loss: 0.983 |  Val. Acc: 50.33%\nSAVED TRAIN\nEpoch: 42 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 52.39%\n\t Val. Loss: 0.964 |  Val. Acc: 49.34%\nSAVED TRAIN\nEpoch: 43 | Epoch Time: 0m 2s\n\tTrain Loss: 0.927 | Train Acc: 52.31%\n\t Val. Loss: 1.004 |  Val. Acc: 47.11%\nSAVED TRAIN\nEpoch: 44 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.87%\n\t Val. Loss: 0.958 |  Val. Acc: 50.90%\nSAVED TRAIN\nEpoch: 45 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.60%\n\t Val. Loss: 0.962 |  Val. Acc: 50.72%\nSAVED TRAIN\nEpoch: 46 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.60%\n\t Val. Loss: 0.961 |  Val. Acc: 51.21%\nSAVED TRAIN\nEpoch: 47 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.05%\n\t Val. Loss: 0.969 |  Val. Acc: 50.27%\nSAVED TRAIN\nEpoch: 48 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.85%\n\t Val. Loss: 0.990 |  Val. Acc: 49.24%\nSAVED TRAIN\nEpoch: 49 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 51.84%\n\t Val. Loss: 0.978 |  Val. Acc: 48.66%\nSAVED TRAIN\nEpoch: 50 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.76%\n\t Val. Loss: 0.974 |  Val. Acc: 49.85%\nSAVED TRAIN\nEpoch: 51 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.59%\n\t Val. Loss: 0.964 |  Val. Acc: 50.76%\nSAVED TRAIN\nEpoch: 52 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.04%\n\t Val. Loss: 0.986 |  Val. Acc: 50.12%\nSAVED TRAIN\nEpoch: 53 | Epoch Time: 0m 3s\n\tTrain Loss: 0.934 | Train Acc: 52.42%\n\t Val. Loss: 0.957 |  Val. Acc: 49.26%\nSAVED TRAIN\nEpoch: 54 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.48%\n\t Val. Loss: 0.983 |  Val. Acc: 47.70%\nSAVED TRAIN\nEpoch: 55 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.12%\n\t Val. Loss: 0.981 |  Val. Acc: 46.07%\nSAVED TRAIN\nEpoch: 56 | Epoch Time: 0m 2s\n\tTrain Loss: 0.924 | Train Acc: 52.47%\n\t Val. Loss: 0.969 |  Val. Acc: 47.75%\nSAVED TRAIN\nEpoch: 57 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 52.17%\n\t Val. Loss: 0.983 |  Val. Acc: 45.34%\nSAVED TRAIN\nEpoch: 58 | Epoch Time: 0m 2s\n\tTrain Loss: 0.927 | Train Acc: 52.29%\n\t Val. Loss: 0.976 |  Val. Acc: 46.82%\nSAVED TRAIN\nEpoch: 59 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.71%\n\t Val. Loss: 0.995 |  Val. Acc: 47.34%\nSAVED TRAIN\nEpoch: 60 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.29%\n\t Val. Loss: 0.975 |  Val. Acc: 50.04%\nSAVED TRAIN\nEpoch: 61 | Epoch Time: 0m 2s\n\tTrain Loss: 0.926 | Train Acc: 52.58%\n\t Val. Loss: 0.963 |  Val. Acc: 49.44%\nSAVED TRAIN\nEpoch: 62 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.08%\n\t Val. Loss: 0.997 |  Val. Acc: 46.55%\nSAVED TRAIN\nEpoch: 63 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.62%\n\t Val. Loss: 0.977 |  Val. Acc: 48.27%\nSAVED TRAIN\nEpoch: 64 | Epoch Time: 0m 2s\n\tTrain Loss: 0.926 | Train Acc: 52.12%\n\t Val. Loss: 0.974 |  Val. Acc: 49.68%\nSAVED TRAIN\nEpoch: 65 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 52.26%\n\t Val. Loss: 0.988 |  Val. Acc: 49.51%\nSAVED TRAIN\nEpoch: 66 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.76%\n\t Val. Loss: 0.982 |  Val. Acc: 48.22%\nSAVED TRAIN\nEpoch: 67 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 52.04%\n\t Val. Loss: 0.971 |  Val. Acc: 49.39%\nSAVED TRAIN\nEpoch: 68 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 51.89%\n\t Val. Loss: 0.979 |  Val. Acc: 47.89%\nSAVED TRAIN\nEpoch: 69 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 50.91%\n\t Val. Loss: 0.969 |  Val. Acc: 50.43%\nSAVED TRAIN\nEpoch: 70 | Epoch Time: 0m 3s\n\tTrain Loss: 0.929 | Train Acc: 51.61%\n\t Val. Loss: 0.987 |  Val. Acc: 50.55%\nSAVED TRAIN\nEpoch: 71 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 51.30%\n\t Val. Loss: 0.983 |  Val. Acc: 49.58%\nSAVED TRAIN\nEpoch: 72 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.66%\n\t Val. Loss: 0.975 |  Val. Acc: 48.51%\nSAVED TRAIN\nEpoch: 73 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.69%\n\t Val. Loss: 0.965 |  Val. Acc: 50.16%\nSAVED TRAIN\nEpoch: 74 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 50.93%\n\t Val. Loss: 0.972 |  Val. Acc: 46.93%\nSAVED TRAIN\nEpoch: 75 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.83%\n\t Val. Loss: 0.968 |  Val. Acc: 51.08%\nSAVED TRAIN\nEpoch: 76 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.18%\n\t Val. Loss: 0.986 |  Val. Acc: 45.89%\nSAVED TRAIN\nEpoch: 77 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.10%\n\t Val. Loss: 0.979 |  Val. Acc: 49.32%\nSAVED TRAIN\nEpoch: 78 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.84%\n\t Val. Loss: 0.965 |  Val. Acc: 49.71%\nSAVED TRAIN\nEpoch: 79 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.79%\n\t Val. Loss: 0.972 |  Val. Acc: 48.83%\nSAVED TRAIN\nEpoch: 80 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.77%\n\t Val. Loss: 0.957 |  Val. Acc: 50.96%\nSAVED TRAIN\nEpoch: 81 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.72%\n\t Val. Loss: 0.971 |  Val. Acc: 49.54%\nSAVED TRAIN\nEpoch: 82 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.86%\n\t Val. Loss: 0.982 |  Val. Acc: 47.81%\nSAVED TRAIN\nEpoch: 83 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.56%\n\t Val. Loss: 0.980 |  Val. Acc: 50.46%\nSAVED TRAIN\nEpoch: 84 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.22%\n\t Val. Loss: 0.976 |  Val. Acc: 50.26%\nSAVED TRAIN\nEpoch: 85 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.54%\n\t Val. Loss: 0.977 |  Val. Acc: 51.13%\nSAVED TRAIN\nEpoch: 86 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.02%\n\t Val. Loss: 0.971 |  Val. Acc: 49.48%\nSAVED TRAIN\nEpoch: 87 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.49%\n\t Val. Loss: 0.975 |  Val. Acc: 50.50%\nSAVED TRAIN\nEpoch: 88 | Epoch Time: 0m 2s\n\tTrain Loss: 0.938 | Train Acc: 51.45%\n\t Val. Loss: 0.977 |  Val. Acc: 49.67%\nSAVED TRAIN\nEpoch: 89 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 52.20%\n\t Val. Loss: 0.975 |  Val. Acc: 47.89%\nSAVED TRAIN\nEpoch: 90 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.36%\n\t Val. Loss: 0.982 |  Val. Acc: 51.30%\nSAVED TRAIN\nEpoch: 91 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.15%\n\t Val. Loss: 0.984 |  Val. Acc: 50.52%\nSAVED TRAIN\nEpoch: 92 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.67%\n\t Val. Loss: 0.973 |  Val. Acc: 50.35%\nSAVED TRAIN\nEpoch: 93 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 51.87%\n\t Val. Loss: 0.982 |  Val. Acc: 50.69%\nSAVED TRAIN\nEpoch: 94 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.99%\n\t Val. Loss: 0.976 |  Val. Acc: 51.11%\nSAVED TRAIN\nEpoch: 95 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.68%\n\t Val. Loss: 0.979 |  Val. Acc: 49.36%\nSAVED TRAIN\nEpoch: 96 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.68%\n\t Val. Loss: 0.994 |  Val. Acc: 47.72%\nSAVED TRAIN\nEpoch: 97 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.64%\n\t Val. Loss: 0.981 |  Val. Acc: 49.83%\nSAVED TRAIN\nEpoch: 98 | Epoch Time: 0m 2s\n\tTrain Loss: 0.938 | Train Acc: 51.69%\n\t Val. Loss: 0.982 |  Val. Acc: 49.68%\nSAVED TRAIN\nEpoch: 99 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 52.22%\n\t Val. Loss: 0.974 |  Val. Acc: 50.40%\nSAVED TRAIN\nEpoch: 100 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.72%\n\t Val. Loss: 0.979 |  Val. Acc: 49.21%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTEST VALID\n\t Test. Loss: 0.941 |  Test. Acc: 52.34%\nTEST TRAIN\n\t Test. Loss: 0.940 |  Test. Acc: 52.11%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\nGET METRICS\n--------------------------------------------------\nBEST METRICS VALID\nTest f1: 0.429 | Test Prec: 42.41% | Test Recall: 42.41% \nBEST METRICS TRAIN\nTest f1: 0.429 | Test Prec: 42.41% | Test Recall: 42.41% \n"
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(\"-\"*50)\n",
    "print(\"Running Sentiment Analysis, Random Embeddings, spanglish\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_RandomEmbeddings_spanglish\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : 300,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_HIDDEN\" : 100,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"BIDIR\": True,\n",
    "    \"DRPOUT\": 0.5,\n",
    "    \"LR\": 0.01\n",
    "}\n",
    "model = SentimentLSTM(config[\"N_VOCAB\"],config[\"N_EMBED\"], config[\"N_HIDDEN\"], config[\"N_OUTPUT\"], config[\"N_LAYERS\"], config[\"BIDIR\"], config[\"DRPOUT\"])\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)\n",
    "print(\"GET METRICS\")\n",
    "print(\"-\"*50)\n",
    "get_metrics(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = config[\"N_VOCAB\"]     # number of unique words in vocabulary\n",
    "        self.n_layers = config[\"N_LAYERS\"]   # number of LSTM layers \n",
    "        self.n_hidden = config[\"N_HIDDEN\"]   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.n_vocab, config[\"N_EMBED\"])\n",
    "        self.embedding.weight.data = torch.eye(self.n_vocab)\n",
    "        # make embedding untrainable\n",
    "        if not config[\"EMB_TRAIN\"]:\n",
    "            self.embedding.weight.requires_grad=False\n",
    "        self.lstm = nn.LSTM(config[\"N_EMBED\"], self.n_hidden, self.n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(self.n_hidden, config[\"N_OUTPUT\"])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Untrainable\n{   'EMB_TRAIN': False,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Untrainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 1,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 49.86%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Untrainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Untrainable\",\n",
    "    \"N_EPOCHS\": 1,\n",
    "    \"EMB_TRAIN\": False,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Trainable\n{   'EMB_TRAIN': True,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Trainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 9s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.82%\nEpoch: 02 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 03 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 50.08%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 04 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.02%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 05 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.78%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 06 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.20%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 07 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 48.80%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 08 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 46.21%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 09 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 44.87%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 10 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 44.31%\n\t Val. Loss: 0.693 |  Val. Acc: 49.78%\nEpoch: 11 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 41.84%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 12 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 40.35%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 13 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.64%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 14 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.10%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 15 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 16 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.21%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 17 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 34.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 18 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.87%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 19 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.33%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 20 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.41%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 21 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.17%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 22 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.15%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 23 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.02%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 24 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.90%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 25 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.00%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 26 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 27 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.96%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 28 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.93%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 29 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 30 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.94%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 31 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 32 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 33 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 34 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 35 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 36 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 37 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 38 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 39 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 40 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 41 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 42 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 43 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 44 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 45 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 46 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 47 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 48 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 49 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 50 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 51 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 52 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 53 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 54 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 55 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 56 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 57 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 58 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 59 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 60 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 61 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 62 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 63 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 64 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 65 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 66 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 67 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 68 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 69 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 70 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 71 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 72 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 73 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 74 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 75 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 76 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 77 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.41%\nEpoch: 78 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 79 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 80 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 81 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 82 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 83 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 84 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 85 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 86 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 87 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 88 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 89 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 90 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 91 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 92 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 93 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 94 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 95 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 96 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 97 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 98 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 99 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 100 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 34.29%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Trainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Trainable\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}