{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "random.seed(0)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/train_conll_hinglish.csv'\n",
    "import torchtext\n",
    "\n",
    "def label2int(label):\n",
    "    if label=='positive':\n",
    "        return 1\n",
    "    elif label=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x.lower(), # because are building a character-RNN\n",
    "                                  include_lengths=False, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "label_field = torchtext.data.Field(sequential=False,    # not a sequence\n",
    "                                   use_vocab=False,     # don't need to track vocabulary\n",
    "                                   is_target=True,      \n",
    "                                   batch_first=True,\n",
    "                                   preprocessing=lambda x: label2int(x)) # convert text to 0 and 1\n",
    "\n",
    "fields = [('id', None),('text', text_field), ('label', label_field)]\n",
    "dataset = torchtext.data.TabularDataset(filename, # name of the file\n",
    "                                        \"tsv\",               # fields are separated by a tab\n",
    "                                        fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "@ adilnisarbutt pakistan ka ghra tauq he pakistan israel ko tasleem nahein kerta isko palestine kehta he- occupied palestine --- 0\nmadarchod mulle ye mathura me nahi dikha tha jab mullo ne hindu ko iss liye mara ki vo lasse ki paise mag liye theâ€¦ https// t. co/ oxf8tr3bly --- 0\n@ narendramodi manya pradhan mantri mahoday shriman narendra modi ji pradhanmantri banne par hardik badhai tahe dilâ€¦ https// t. co/ prnomskkn1 --- 1\n@ atheist_ krishna jcb full trend me chal rahi aa --- 1\n@ abhisharsharma_@ ravishkumarblog loksabha me janta sirf modi ko vote de rahi thi na ki kisi mp or bjp ko without mâ€¦ https// t. co/ shtbwcb7fm --- 1\n@ noirnaveed@ angelahana6@ cricketworldcup bhosdike tum pechvade ki tatti hi rahoge bc --- 0\nlove u bhaijan...â™¥â™¥ father+ son..# bharat# iambharat# bharatthiseid best pic from entire# promotions... mashallahâ€¦ https// t. co/ s2xhwu6lud --- 1\n@ manojgajjar111 tumhara pass abh deemagh hai nahi islea google ko apna deemagh banaya hua hai. har koi tumhari tarhâ€¦ https// t. co/ bxueug3xsn --- 0\n@ mahlogo_ nolo weni ankere o gae this weekendğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ --- 1\n@ aurangzeb_ aimim@ sachins40805591 lage raho mullo tumhre issi quran faad gyan ki kayal hain duniya allah bhi khusâ€¦ https// t co/ gha9dwnz6u --- 0\n"
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(dataset[i].text, \"---\", dataset[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "12105 1513 1513 15131\n"
    }
   ],
   "source": [
    "print(len(train), len(val), len(test), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "47,\n             \"'\": 48,\n             '-': 49,\n             'â€™': 50,\n             'ï¸': 51,\n             'ğŸ™': 52,\n             'â¤': 53,\n             'ğŸ¤£': 54,\n             '&': 55,\n             'ğŸ˜': 56,\n             ')': 57,\n             '(': 58,\n             'ğŸ˜­': 59,\n             '*': 60,\n             'ğŸ˜˜': 61,\n             'ğŸ‡®': 62,\n             'ğŸ‡³': 63,\n             'ğŸ˜Š': 64,\n             'ğŸŒ¹': 65,\n             'ğŸ’œ': 66,\n             'ğŸ‚': 67,\n             'â€œ': 68,\n             'ğŸ’•': 69,\n             'à¤¾': 70,\n             'ğŸ˜': 71,\n             'ğŸ‘': 72,\n             'ğŸ‰': 73,\n             'ğŸ’–': 74,\n             '|': 75,\n             'ğŸ‘': 76,\n             'â€': 77,\n             'ğŸ‘Œ': 78,\n             'âœŒ': 79,\n             '%': 80,\n             'à¤°': 81,\n             'ğŸ˜œ': 82,\n             'ğŸ’': 83,\n             'ğŸ˜†': 84,\n             'ğŸ»': 85,\n             '+': 86,\n             'â™¥': 87,\n             'à¥': 88,\n             'ğŸ¤—': 89,\n             'ğŸ˜': 90,\n             'ğŸ˜¡': 91,\n             'ğŸ™„': 92,\n             'ğŸ”¥': 93,\n             '~': 94,\n             'ğŸ¤”': 95,\n             '\\u200d': 96,\n             'ğŸ’™': 97,\n             'ğŸ˜‰': 98,\n             'ğŸ˜ ': 99,\n             'ğŸ˜…': 100,\n             'ğŸ™Œ': 101,\n             'ğŸ¤ª': 102,\n             'à¤•': 103,\n             'à¤¨': 104,\n             'à¤®': 105,\n             'ğŸ˜€': 106,\n             ';': 107,\n             'ğŸŒ¸': 108,\n             'à¥€': 109,\n             'ğŸ’“': 110,\n             'à¤¸': 111,\n             'ğŸ’”': 112,\n             'ğŸ˜‹': 113,\n             'â€˜': 114,\n             'â˜º': 115,\n             'ğŸ˜Œ': 116,\n             'ğŸ˜¢': 117,\n             'à¥‡': 118,\n             'â€”': 119,\n             'ğŸ’': 120,\n             'ğŸ¥°': 121,\n             'ğŸ’ª': 122,\n             'à¤¤': 123,\n             'à¤¹': 124,\n             'ğŸ˜”': 125,\n             'ğŸ˜': 126,\n             'ğŸ’—': 127,\n             'ğŸ’¯': 128,\n             'ğŸ˜„': 129,\n             'ğŸ˜‘': 130,\n             'ğŸ˜’': 131,\n             '[': 132,\n             'ğŸ™ˆ': 133,\n             'ğŸŒ·': 134,\n             'ğŸŠ': 135,\n             'ğŸš©': 136,\n             'à¤‚': 137,\n             'ğŸ˜‡': 138,\n             '=': 139,\n             'ğŸ‘‡': 140,\n             'ğŸ˜ª': 141,\n             ']': 142,\n             'Ã©': 143,\n             'à¤¯': 144,\n             'ğŸŒº': 145,\n             'ğŸ˜¹': 146,\n             'à¤¿': 147,\n             'ğŸ¼': 148,\n             'ğŸ˜': 149,\n             'ğŸ˜›': 150,\n             '>': 151,\n             'ğŸ™‚': 152,\n             'ğŸ˜¬': 153,\n             'à¤—': 154,\n             'à¤ª': 155,\n             'à¤¬': 156,\n             'ğŸ': 157,\n             'ğŸ˜ƒ': 158,\n             'ğŸ’š': 159,\n             'ğŸ˜¤': 160,\n             'ğŸ¤©': 161,\n             '$': 162,\n             'Ã¡': 163,\n             'ğŸˆ': 164,\n             'ğŸ½': 165,\n             'ğŸ˜': 166,\n             'ğŸ˜•': 167,\n             'Ø§': 168,\n             'ğŸ†': 169,\n             'ğŸ’': 170,\n             '\"': 171,\n             '<': 172,\n             'à¤¦': 173,\n             'ğŸ‡°': 174,\n             'ğŸ‡µ': 175,\n             'ğŸ˜©': 176,\n             'ğŸ¤¦': 177,\n             'Ä': 178,\n             'à¤µ': 179,\n             'â™‚': 180,\n             'à¤œ': 181,\n             'à¤²': 182,\n             'à¥': 183,\n             'â˜¹': 184,\n             'â™€': 185,\n             'ğŸ’ƒ': 186,\n             'ğŸ™Š': 187,\n             'ğŸ¤¨': 188,\n             'à¥‹': 189,\n             'ğŸ¾': 190,\n             'ğŸ–•': 191,\n             'ğŸ¤§': 192,\n             'ğŸ¥º': 193,\n             'à¥‚': 194,\n             'âœ¨': 195,\n             'ğŸ’': 196,\n             'à¥ˆ': 197,\n             'Ù„': 198,\n             'à¤¶': 199,\n             'âœ…': 200,\n             'â£': 201,\n             'ğŸ’‹': 202,\n             'ğŸ¤¬': 203,\n             'Ã­': 204,\n             'Ù…': 205,\n             'ğŸŒˆ': 206,\n             'ğŸŒš': 207,\n             'Ø±': 208,\n             'ğŸ‘‰': 209,\n             'ğŸ’˜': 210,\n             'ğŸ’›': 211,\n             'ğŸ˜ˆ': 212,\n             'ğŸ˜¶': 213,\n             'ğŸ˜·': 214,\n             'Ø¹': 215,\n             'à¤­': 216,\n             'â€“': 217,\n             'ğŸ’®': 218,\n             'ğŸ”¯': 219,\n             'ğŸ˜£': 220,\n             'ğŸ˜³': 221,\n             'Ä±': 222,\n             'ÛŒ': 223,\n             'ğŸ‘‹': 224,\n             'ğŸ˜': 225,\n             'ğŸ˜¨': 226,\n             'ğŸ¤“': 227,\n             'ğŸ¥³': 228,\n             '`': 229,\n             'ÅŸ': 230,\n             'Ùˆ': 231,\n             'à¤…': 232,\n             'á´‡': 233,\n             'âš«': 234,\n             'ğŸŒ¼': 235,\n             'ğŸµ': 236,\n             'ğŸ‘€': 237,\n             'ğŸ‘‘': 238,\n             'ğŸ–¤': 239,\n             'ğŸ˜š': 240,\n             'ğŸ˜¥': 241,\n             'ğŸ˜´': 242,\n             'ğŸ¤˜': 243,\n             '\\U000e0067': 244,\n             'Ã§': 245,\n             'Ø¯': 246,\n             'à¤«': 247,\n             'à±': 248,\n             'â€¼': 249,\n             'ğŸŒ±': 250,\n             'ğŸŒ²': 251,\n             'ğŸ¶': 252,\n             'ğŸ': 253,\n             'ğŸ‘«': 254,\n             'ğŸ’¥': 255,\n             'ğŸ˜“': 256,\n             'ğŸ˜»': 257,\n             'ğŸ™‰': 258,\n             'Ã¥': 259,\n             'Ã±': 260,\n             'Ã¼': 261,\n             'à¤Ÿ': 262,\n             'à¤§': 263,\n             'ğŸ': 264,\n             'ğŸ‘»': 265,\n             'ğŸ’«': 266,\n             'ğŸ¤²': 267,\n             'ğŸ¤·': 268,\n             'ğŸ¦‹': 269,\n             'ğŸ§¡': 270,\n             'Ãª': 271,\n             'à¤–': 272,\n             'à¤š': 273,\n             'á´€': 274,\n             'âœŠ': 275,\n             'âœ–': 276,\n             'â¡': 277,\n             'ğŸ«': 278,\n             'ğŸµ': 279,\n             'ğŸ¼': 280,\n             'ğŸ‘ˆ': 281,\n             'ğŸ‘¦': 282,\n             'ğŸ‘©': 283,\n             'ğŸ’‘': 284,\n             'ğŸ˜«': 285,\n             'ğŸ™ƒ': 286,\n             'ğŸ™‡': 287,\n             'ğŸ¤': 288,\n             'ğŸ¤­': 289,\n             'Â´': 290,\n             'Ã£': 291,\n             'Ø©': 292,\n             'Ø¬': 293,\n             'Ù‚': 294,\n             'à¤': 295,\n             'à¤¡': 296,\n             'à¤¥': 297,\n             'à°°': 298,\n             'á´›': 299,\n             'â€¢': 300,\n             'â˜': 301,\n             'â›³': 302,\n             'âœ”': 303,\n             'ğŸ‡¦': 304,\n             'ğŸŒµ': 305,\n             'ğŸŒ¿': 306,\n             'ğŸ´': 307,\n             'ğŸ—¿': 308,\n             'ğŸ˜¯': 309,\n             'ğŸ˜±': 310,\n             'ğŸ™†': 311,\n             'ğŸ¤™': 312,\n             '\\U000e0062': 313,\n             '\\U000e007f': 314,\n             '^': 315,\n             '{': 316,\n             '}': 317,\n             'Â¡': 318,\n             'É©': 319,\n             'Éª': 320,\n             'É´': 321,\n             'Øª': 322,\n             'Ø³': 323,\n             'Ú©': 324,\n             'à¤ˆ': 325,\n             'à°¾': 326,\n             'à°¿': 327,\n             'à¹ˆ': 328,\n             'á´': 329,\n             'áµ‰': 330,\n             'áµ’': 331,\n             'â‚¹': 332,\n             'â•­': 333,\n             'â•®': 334,\n             'âœ‹': 335,\n             'âœ': 336,\n             'â“': 337,\n             'â—': 338,\n             'â¬‡': 339,\n             'ã®': 340,\n             'ğ—²': 341,\n             'ğŸŒ™': 342,\n             'ğŸ§': 343,\n             'ğŸ°': 344,\n             'ğŸ“': 345,\n             'ğŸ³': 346,\n             'ğŸ±': 347,\n             'ğŸ‘': 348,\n             'ğŸ‘¨': 349,\n             'ğŸ‘­': 350,\n             'ğŸ’©': 351,\n             'ğŸ“„': 352,\n             'ğŸ”«': 353,\n             'ğŸ–': 354,\n             'ğŸ˜Ÿ': 355,\n             'ğŸ˜®': 356,\n             'ğŸ˜°': 357,\n             'ğŸ¤': 358,\n             'ğŸ¤¯': 359,\n             '\\U000e0065': 360,\n             '\\U000e006e': 361,\n             'Ã¤': 362,\n             'Ã´': 363,\n             'Ã½': 364,\n             'Ä': 365,\n             'Å‚': 366,\n             'Ê€': 367,\n             'Ë¢': 368,\n             'Ù†': 369,\n             'Û’': 370,\n             'à¤†': 371,\n             'à¤‡': 372,\n             'à¤·': 373,\n             'à¤¼': 374,\n             'à¥Œ': 375,\n             'à¥¤': 376,\n             'à¨‚': 377,\n             'à°‚': 378,\n             'à°•': 379,\n             'à°¡': 380,\n             'à°ª': 381,\n             'à°¸': 382,\n             'à±': 383,\n             'à¸„': 384,\n             '\\u200b': 385,\n             '\\u2066': 386,\n             'âŒ': 387,\n             'â¬†': 388,\n             'ãƒ³': 389,\n             'ãƒ¼': 390,\n             'ğ„': 391,\n             'ğ˜€': 392,\n             'ğŸ‡©': 393,\n             'ğŸ‡«': 394,\n             'ğŸ‡¬': 395,\n             'ğŸ‡¸': 396,\n             'ğŸ‡º': 397,\n             'ğŸ‡¿': 398,\n             'ğŸŒŸ': 399,\n             'ğŸŒ»': 400,\n             'ğŸ¾': 401,\n             'ğŸ€': 402,\n             'ğŸ‡': 403,\n             'ğŸ¤': 404,\n             'ğŸ¿': 405,\n             'ğŸ·': 406,\n             'ğŸ‘Š': 407,\n             'ğŸ‘ ': 408,\n             'ğŸ‘°': 409,\n             'ğŸ‘¸': 410,\n             'ğŸ‘¿': 411,\n             'ğŸ’': 412,\n             'ğŸ’¸': 413,\n             'ğŸ“ƒ': 414,\n             'ğŸ”': 415,\n             'ğŸ”´': 416,\n             'ğŸ—£': 417,\n             'ğŸ˜—': 418,\n             'ğŸ˜™': 419,\n             'ğŸ™': 420,\n             'ğŸš¨': 421,\n             'ğŸ¤Ÿ': 422,\n             'ğŸ§': 423,\n             'ğŸ§š': 424,\n             'Â®': 425,\n             'Â¯': 426,\n             'Â·': 427,\n             'Âº': 428,\n             'Ã—': 429,\n             'Ã®': 430,\n             'Ã³': 431,\n             'Ã¶': 432,\n             'Ãº': 433,\n             'Ä‘': 434,\n             'Ä“': 435,\n             'Ä«': 436,\n             'Å™': 437,\n             'Å¯': 438,\n             'Å¼': 439,\n             'É£': 440,\n             'É²': 441,\n             'Ê¸': 442,\n             'Ë˜': 443,\n             'Ì©': 444,\n             'Ğ°': 445,\n             'Ñ”': 446,\n             'Ò“': 447,\n             'Õ¬': 448,\n             'Ø¨': 449,\n             'Ø´': 450,\n             'ÙŠ': 451,\n             'Ûƒ': 452,\n             'à¤': 453,\n             'à¤ƒ': 454,\n             'à¤›': 455,\n             'à¤£': 456,\n             'à¥ƒ': 457,\n             'à¨¤': 458,\n             'à¨°': 459,\n             'à¨¹': 460,\n             'à©€': 461,\n             'à©': 462,\n             'à°‰': 463,\n             'à°¤': 464,\n             'à°¦': 465,\n             'à°¨': 466,\n             'à°®': 467,\n             'à°¯': 468,\n             'à°²': 469,\n             'à°·': 470,\n             'à±‹': 471,\n             'à¸': 472,\n             'à¸—': 473,\n             'à¸™': 474,\n             'à¸š': 475,\n             'à¸›': 476,\n             'à¸œ': 477,\n             'à¸­': 478,\n             'à¸µ': 479,\n             'à¸¸': 480,\n             'á´…': 481,\n             'á´‹': 482,\n             'á´': 483,\n             'á´˜': 484,\n             'áµƒ': 485,\n             'áµ': 486,\n             'áµ': 487,\n             'áµ˜': 488,\n             'á¶¦': 489,\n             'áº¡': 490,\n             'áº½': 491,\n             'â€¿': 492,\n             '\\u2069': 493,\n             'âƒ‘': 494,\n             'âƒ£': 495,\n             'â–º': 496,\n             'â—‰': 497,\n             'â™¡': 498,\n             'âœˆ': 499,\n             'â €': 500,\n             'â¤´': 501,\n             'ã€£': 502,\n             'ã™': 503,\n             'ã£': 504,\n             'ã§': 505,\n             'ãƒ€': 506,\n             'ãƒ’': 507,\n             'ãƒ§': 508,\n             'ã… ': 509,\n             'å®š': 510,\n             'æ—¥': 511,\n             'é™': 512,\n             'ê•Š': 513,\n             'ê¸°': 514,\n             'ë‹¤': 515,\n             'ìŠ¤': 516,\n             'ï·º': 517,\n             'ï½“': 518,\n             'ï½¡': 519,\n             'ğ': 520,\n             'ğ¡': 521,\n             'ğ—°': 522,\n             'ğ—¶': 523,\n             'ğŸ‡§': 524,\n             'ğŸ‡­': 525,\n             'ğŸ‡±': 526,\n             'ğŸŒ': 527,\n             'ğŸŒ': 528,\n             'ğŸŒ': 529,\n             'ğŸŒ˜': 530,\n             'ğŸŒ': 531,\n             'ğŸŒ´': 532,\n             'ğŸŒ¶': 533,\n             'ğŸƒ': 534,\n             'ğŸ': 535,\n             'ğŸ': 536,\n             'ğŸ': 537,\n             'ğŸ¦': 538,\n             'ğŸ¨': 539,\n             'ğŸ¬': 540,\n             'ğŸ‹': 541,\n             'ğŸ§': 542,\n             'ğŸ¬': 543,\n             'ğŸ‚': 544,\n             'ğŸ–': 545,\n             'ğŸ¯': 546,\n             'ğŸ¼': 547,\n             'ğŸ‘§': 548,\n             'ğŸ‘º': 549,\n             'ğŸ‘¾': 550,\n             'ğŸ’€': 551,\n             'ğŸ’': 552,\n             'ğŸ’…': 553,\n             'ğŸ’Œ': 554,\n             'ğŸ’¡': 555,\n             'ğŸ’¬': 556,\n             'ğŸ“·': 557,\n             'ğŸ˜–': 558,\n             'ğŸ™‹': 559,\n             'ğŸš¬': 560,\n             'ğŸš´': 561,\n             'ğŸš¶': 562,\n             'ğŸ¤’': 563,\n             'ğŸ¤¤': 564,\n             'ğŸ¥µ': 565,\n             'ğŸ¦‰': 566,\n             'ğŸ¦': 567,\n             'Â£': 568,\n             'Â¥': 569,\n             'Â°': 570,\n             'Â¿': 571,\n             'Ã ': 572,\n             'Ã¬': 573,\n             'Ã²': 574,\n             'Ä‡': 575,\n             'Ä™': 576,\n             'Ä›': 577,\n             'ÄŸ': 578,\n             'Åˆ': 579,\n             'Å›': 580,\n             'Å¡': 581,\n             'Æ°': 582,\n             'É¢': 583,\n             'Ê': 584,\n             'Êœ': 585,\n             'ÊŸ': 586,\n             'Ê°': 587,\n             'Ê³': 588,\n             'Î´': 589,\n             'Î¼': 590,\n             'Ğ´': 591,\n             'Ğº': 592,\n             'Ğ¼': 593,\n             'Ğ¾': 594,\n             'Ñ€': 595,\n             'Ñ‚': 596,\n             'Ø¦': 597,\n             'Ø®': 598,\n             'Ø²': 599,\n             'Øµ': 600,\n             'Ø·': 601,\n             'Øº': 602,\n             'Ù‡': 603,\n             'Ú¯': 604,\n             'Úº': 605,\n             'Û': 606,\n             'à¤‘': 607,\n             'à¤”': 608,\n             'à¤ ': 609,\n             'à¥‰': 610,\n             'à¦ˆ': 611,\n             'à¦•': 612,\n             'à¦¦': 613,\n             'à¦¬': 614,\n             'à¦®': 615,\n             'à¦°': 616,\n             'à¦¾': 617,\n             'à§‹': 618,\n             'à¨•': 619,\n             'à¨—': 620,\n             'à¨£': 621,\n             'à¨¨': 622,\n             'à¨«': 623,\n             'à¨¸': 624,\n             'à¨¾': 625,\n             'à¨¿': 626,\n             'à©‡': 627,\n             'à©‹': 628,\n             'à°…': 629,\n             'à°‡': 630,\n             'à°š': 631,\n             'à°§': 632,\n             'à°¬': 633,\n             'à°µ': 634,\n             'à±‚': 635,\n             'à±‡': 636,\n             'à¸‚': 637,\n             'à¸‡': 638,\n             'à¸“': 639,\n             'à¸¢': 640,\n             'à¸£': 641,\n             'à¸¥': 642,\n             'à¸§': 643,\n             'à¸±': 644,\n             'à¸²': 645,\n             'à¸¶': 646,\n             'à¸¹': 647,\n             'à¹': 648,\n             'à¹„': 649,\n             'à¹‰': 650,\n             'á´„': 651,\n             'á´œ': 652,\n             'á´ ': 653,\n             'áµˆ': 654,\n             'áµ': 655,\n             'áµ—': 656,\n             'áµ›': 657,\n             'á¶œ': 658,\n             'áº¿': 659,\n             'á»': 660,\n             'á»‰': 661,\n             'á»‘': 662,\n             'á»“': 663,\n             'á»': 664,\n             'á»§': 665,\n             'â€•': 666,\n             'â‰': 667,\n             'â‚¬': 668,\n             'â„…': 669,\n             'â„¢': 670,\n             'â†‘': 671,\n             'â‡’': 672,\n             'âŒš': 673,\n             'â–‚': 674,\n             'â˜€': 675,\n             'â˜…': 676,\n             'â˜”': 677,\n             'â˜ª': 678,\n             'âš”': 679,\n             'âš•': 680,\n             'âš˜': 681,\n             'âšª': 682,\n             'âš½': 683,\n             'â›²': 684,\n             'â›½': 685,\n             'âœ‰': 686,\n             'âœ´': 687,\n             'â‡': 688,\n             'â”': 689,\n             'â•': 690,\n             'â­': 691,\n             'ã€': 692,\n             'ã„': 693,\n             'ãŒ': 694,\n             'ã“': 695,\n             'ã¤': 696,\n             'ã¦': 697,\n             'ã¨': 698,\n             'ã«': 699,\n             'ã­': 700,\n             'ã¯': 701,\n             'ã¹': 702,\n             'ã¾': 703,\n             'ã‚‹': 704,\n             'ã‚’': 705,\n             'ã‚¢': 706,\n             'ã‚¤': 707,\n             'ã‚¯': 708,\n             'ã‚³': 709,\n             'ã‚¸': 710,\n             'ã‚¹': 711,\n             'ã‚º': 712,\n             'ãƒ†': 713,\n             'ãƒˆ': 714,\n             'ãƒ': 715,\n             'ãƒ‘': 716,\n             'ãƒ“': 717,\n             'ãƒ™': 718,\n             'ãƒ ': 719,\n             'ãƒª': 720,\n             'ãƒ«': 721,\n             'å½“': 722,\n             'æ”¾': 723,\n             'çŒ«': 724,\n             'ç”Ÿ': 725,\n             'èª•': 726,\n             'é–‹': 727,\n             'ê²½': 728,\n             'ê³„': 729,\n             'êµ­': 730,\n             'ê»˜': 731,\n             'ë‚˜': 732,\n             'ëŠ”': 733,\n             'ë‹ˆ': 734,\n             'ëŒ„': 735,\n             'ëŸ¬': 736,\n             'ë¦½': 737,\n             'ë§Œ': 738,\n             'ë¨¸': 739,\n             'ë©€': 740,\n             'ë·°': 741,\n             'ìƒ': 742,\n             'ì‹œ': 743,\n             'ì‹¤': 744,\n             'ì•½': 745,\n             'ì™€': 746,\n             'ì€': 747,\n             'ì´': 748,\n             'ì¼': 749,\n             'ì': 750,\n             'ì œ': 751,\n             'ì¤‘': 752,\n             'ì¦': 753,\n             'í„°': 754,\n             'í‹°': 755,\n             'íŒ€': 756,\n             'í•œ': 757,\n             'í•¨': 758,\n             'í–‰': 759,\n             'í™©': 760,\n             'ï·»': 761,\n             'ï¸': 762,\n             'ï½…': 763,\n             'ï½': 764,\n             'ï½”': 765,\n             'ï½•': 766,\n             'ğ‚': 767,\n             'ğ‰': 768,\n             'ğ–': 769,\n             'ğš': 770,\n             'ğ¢': 771,\n             'ğ§': 772,\n             'ğ©': 773,\n             'ğ«': 774,\n             'ğ¬': 775,\n             'ğ­': 776,\n             'ğ®': 777,\n             'ğ¹': 778,\n             'ğ‘”': 779,\n             'ğ‘œ': 780,\n             'ğ’»': 781,\n             'ğ’½': 782,\n             'ğ’¾': 783,\n             'ğ“‡': 784,\n             'ğ“ˆ': 785,\n             'ğ“‰': 786,\n             'ğ“Š': 787,\n             'ğ—£': 788,\n             'ğ—®': 789,\n             'ğ—¯': 790,\n             'ğ—´': 791,\n             'ğ—¹': 792,\n             'ğ—º': 793,\n             'ğ—¿': 794,\n             'ğ˜‚': 795,\n             'ğ˜ƒ': 796,\n             'ğŸ': 797,\n             'ğŸ': 798,\n             'ğŸ”': 799,\n             'ğŸ‡¨': 800,\n             'ğŸ‡ª': 801,\n             'ğŸ‡²': 802,\n             'ğŸ‡·': 803,\n             'ğŸ‡¾': 804,\n             'ğŸŒ„': 805,\n             'ğŸŒŠ': 806,\n             'ğŸŒŒ': 807,\n             'ğŸŒ³': 808,\n             'ğŸ€': 809,\n             'ğŸ‚': 810,\n             'ğŸ‡': 811,\n             'ğŸŸ': 812,\n             'ğŸ±': 813,\n             'ğŸ†': 814,\n             'ğŸ—': 815,\n             'ğŸŸ': 816,\n             'ğŸ¥': 817,\n             'ğŸ©': 818,\n             'ğŸ': 819,\n             'ğŸ‹': 820,\n             'ğŸ–': 821,\n             'ğŸ¡': 822,\n             'ğŸˆ': 823,\n             'ğŸ': 824,\n             'ğŸ’': 825,\n             'ğŸ“': 826,\n             'ğŸ•': 827,\n             'ğŸ—': 828,\n             'ğŸ¥': 829,\n             'ğŸ®': 830,\n             'ğŸ¶': 831,\n             'ğŸ¸': 832,\n             'ğŸ¿': 833,\n             'ğŸ‘…': 834,\n             'ğŸ‘†': 835,\n             'ğŸ‘': 836,\n             'ğŸ‘’': 837,\n             'ğŸ‘': 838,\n             'ğŸ‘Ÿ': 839,\n             'ğŸ‘¥': 840,\n             'ğŸ‘¶': 841,\n             'ğŸ‘¹': 842,\n             'ğŸ‘¼': 843,\n             'ğŸ’': 844,\n             'ğŸ’Ÿ': 845,\n             'ğŸ’£': 846,\n             'ğŸ’¾': 847,\n             'ğŸ“ˆ': 848,\n             'ğŸ“': 849,\n             'ğŸ“£': 850,\n             'ğŸ“»': 851,\n             'ğŸ“¿': 852,\n             'ğŸ”': 853,\n             'ğŸ”ˆ': 854,\n             'ğŸ”': 855,\n             'ğŸ””': 856,\n             'ğŸ”š': 857,\n             'ğŸ”': 858,\n             'ğŸ”Ÿ': 859,\n             'ğŸ”¨': 860,\n             'ğŸ”ª': 861,\n             'ğŸ”¶': 862,\n             'ğŸ”·': 863,\n             'ğŸ•‹': 864,\n             'ğŸ•Œ': 865,\n             'ğŸ•¯': 866,\n             'ğŸ•º': 867,\n             'ğŸ˜¦': 868,\n             'ğŸ˜²': 869,\n             'ğŸ˜µ': 870,\n             'ğŸ˜¼': 871,\n             'ğŸ˜¿': 872,\n             'ğŸ™€': 873,\n             'ğŸ™…': 874,\n             'ğŸ›': 875,\n             'ğŸ›': 876,\n             'ğŸ¤': 877,\n             'ğŸ¤‘': 878,\n             'ğŸ¤•': 879,\n             'ğŸ¤š': 880,\n             'ğŸ¤ ': 881,\n             'ğŸ¤¡': 882,\n             'ğŸ¤®': 883,\n             'ğŸ¥‚': 884,\n             'ğŸ¥ƒ': 885,\n             'ğŸ¥‡': 886,\n             'ğŸ¥­': 887,\n             'ğŸ¥´': 888,\n             'ğŸ¥¾': 889,\n             'ğŸ¦': 890,\n             'ğŸ¦„': 891,\n             'ğŸ¦…': 892,\n             'ğŸ¦³': 893,\n             'ğŸ§': 894,\n             'ğŸ§¸': 895,\n             '\\U000e0063': 896,\n             '\\U000e0073': 897,\n             '\\U000e0074': 898})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(dataset)\n",
    "text_field.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['<unk>',\n '<pad>',\n ' ',\n 'a',\n 'i',\n 'e',\n 'h',\n 't',\n 'o',\n 'r',\n 's',\n 'n',\n 'k',\n 'm',\n 'l',\n 'd',\n 'u',\n 'p',\n 'b',\n 'y',\n 'c',\n 'g',\n '/',\n '@',\n '.',\n 'j',\n 'w',\n 'f',\n 'v',\n 'â€¦',\n 'z',\n '_',\n '1',\n '0',\n 'q',\n '2',\n 'x',\n '3',\n '9',\n '5',\n '4',\n '7',\n '8',\n '6',\n '#',\n '!',\n 'ğŸ˜‚',\n '?',\n \"'\",\n '-',\n 'â€™',\n 'ï¸',\n 'ğŸ™',\n 'â¤',\n 'ğŸ¤£',\n '&',\n 'ğŸ˜',\n ')',\n '(',\n 'ğŸ˜­',\n '*',\n 'ğŸ˜˜',\n 'ğŸ‡®',\n 'ğŸ‡³',\n 'ğŸ˜Š',\n 'ğŸŒ¹',\n 'ğŸ’œ',\n 'ğŸ‚',\n 'â€œ',\n 'ğŸ’•',\n 'à¤¾',\n 'ğŸ˜',\n 'ğŸ‘',\n 'ğŸ‰',\n 'ğŸ’–',\n '|',\n 'ğŸ‘',\n 'â€',\n 'ğŸ‘Œ',\n 'âœŒ',\n '%',\n 'à¤°',\n 'ğŸ˜œ',\n 'ğŸ’',\n 'ğŸ˜†',\n 'ğŸ»',\n '+',\n 'â™¥',\n 'à¥',\n 'ğŸ¤—',\n 'ğŸ˜',\n 'ğŸ˜¡',\n 'ğŸ™„',\n 'ğŸ”¥',\n '~',\n 'ğŸ¤”',\n '\\u200d',\n 'ğŸ’™',\n 'ğŸ˜‰',\n 'ğŸ˜ ',\n 'ğŸ˜…',\n 'ğŸ™Œ',\n 'ğŸ¤ª',\n 'à¤•',\n 'à¤¨',\n 'à¤®',\n 'ğŸ˜€',\n ';',\n 'ğŸŒ¸',\n 'à¥€',\n 'ğŸ’“',\n 'à¤¸',\n 'ğŸ’”',\n 'ğŸ˜‹',\n 'â€˜',\n 'â˜º',\n 'ğŸ˜Œ',\n 'ğŸ˜¢',\n 'à¥‡',\n 'â€”',\n 'ğŸ’',\n 'ğŸ¥°',\n 'ğŸ’ª',\n 'à¤¤',\n 'à¤¹',\n 'ğŸ˜”',\n 'ğŸ˜',\n 'ğŸ’—',\n 'ğŸ’¯',\n 'ğŸ˜„',\n 'ğŸ˜‘',\n 'ğŸ˜’',\n '[',\n 'ğŸ™ˆ',\n 'ğŸŒ·',\n 'ğŸŠ',\n 'ğŸš©',\n 'à¤‚',\n 'ğŸ˜‡',\n '=',\n 'ğŸ‘‡',\n 'ğŸ˜ª',\n ']',\n 'Ã©',\n 'à¤¯',\n 'ğŸŒº',\n 'ğŸ˜¹',\n 'à¤¿',\n 'ğŸ¼',\n 'ğŸ˜',\n 'ğŸ˜›',\n '>',\n 'ğŸ™‚',\n 'ğŸ˜¬',\n 'à¤—',\n 'à¤ª',\n 'à¤¬',\n 'ğŸ',\n 'ğŸ˜ƒ',\n 'ğŸ’š',\n 'ğŸ˜¤',\n 'ğŸ¤©',\n '$',\n 'Ã¡',\n 'ğŸˆ',\n 'ğŸ½',\n 'ğŸ˜',\n 'ğŸ˜•',\n 'Ø§',\n 'ğŸ†',\n 'ğŸ’',\n '\"',\n '<',\n 'à¤¦',\n 'ğŸ‡°',\n 'ğŸ‡µ',\n 'ğŸ˜©',\n 'ğŸ¤¦',\n 'Ä',\n 'à¤µ',\n 'â™‚',\n 'à¤œ',\n 'à¤²',\n 'à¥',\n 'â˜¹',\n 'â™€',\n 'ğŸ’ƒ',\n 'ğŸ™Š',\n 'ğŸ¤¨',\n 'à¥‹',\n 'ğŸ¾',\n 'ğŸ–•',\n 'ğŸ¤§',\n 'ğŸ¥º',\n 'à¥‚',\n 'âœ¨',\n 'ğŸ’',\n 'à¥ˆ',\n 'Ù„',\n 'à¤¶',\n 'âœ…',\n 'â£',\n 'ğŸ’‹',\n 'ğŸ¤¬',\n 'Ã­',\n 'Ù…',\n 'ğŸŒˆ',\n 'ğŸŒš',\n 'Ø±',\n 'ğŸ‘‰',\n 'ğŸ’˜',\n 'ğŸ’›',\n 'ğŸ˜ˆ',\n 'ğŸ˜¶',\n 'ğŸ˜·',\n 'Ø¹',\n 'à¤­',\n 'â€“',\n 'ğŸ’®',\n 'ğŸ”¯',\n 'ğŸ˜£',\n 'ğŸ˜³',\n 'Ä±',\n 'ÛŒ',\n 'ğŸ‘‹',\n 'ğŸ˜',\n 'ğŸ˜¨',\n 'ğŸ¤“',\n 'ğŸ¥³',\n '`',\n 'ÅŸ',\n 'Ùˆ',\n 'à¤…',\n 'á´‡',\n 'âš«',\n 'ğŸŒ¼',\n 'ğŸµ',\n 'ğŸ‘€',\n 'ğŸ‘‘',\n 'ğŸ–¤',\n 'ğŸ˜š',\n 'ğŸ˜¥',\n 'ğŸ˜´',\n 'ğŸ¤˜',\n '\\U000e0067',\n 'Ã§',\n 'Ø¯',\n 'à¤«',\n 'à±',\n 'â€¼',\n 'ğŸŒ±',\n 'ğŸŒ²',\n 'ğŸ¶',\n 'ğŸ',\n 'ğŸ‘«',\n 'ğŸ’¥',\n 'ğŸ˜“',\n 'ğŸ˜»',\n 'ğŸ™‰',\n 'Ã¥',\n 'Ã±',\n 'Ã¼',\n 'à¤Ÿ',\n 'à¤§',\n 'ğŸ',\n 'ğŸ‘»',\n 'ğŸ’«',\n 'ğŸ¤²',\n 'ğŸ¤·',\n 'ğŸ¦‹',\n 'ğŸ§¡',\n 'Ãª',\n 'à¤–',\n 'à¤š',\n 'á´€',\n 'âœŠ',\n 'âœ–',\n 'â¡',\n 'ğŸ«',\n 'ğŸµ',\n 'ğŸ¼',\n 'ğŸ‘ˆ',\n 'ğŸ‘¦',\n 'ğŸ‘©',\n 'ğŸ’‘',\n 'ğŸ˜«',\n 'ğŸ™ƒ',\n 'ğŸ™‡',\n 'ğŸ¤',\n 'ğŸ¤­',\n 'Â´',\n 'Ã£',\n 'Ø©',\n 'Ø¬',\n 'Ù‚',\n 'à¤',\n 'à¤¡',\n 'à¤¥',\n 'à°°',\n 'á´›',\n 'â€¢',\n 'â˜',\n 'â›³',\n 'âœ”',\n 'ğŸ‡¦',\n 'ğŸŒµ',\n 'ğŸŒ¿',\n 'ğŸ´',\n 'ğŸ—¿',\n 'ğŸ˜¯',\n 'ğŸ˜±',\n 'ğŸ™†',\n 'ğŸ¤™',\n '\\U000e0062',\n '\\U000e007f',\n '^',\n '{',\n '}',\n 'Â¡',\n 'É©',\n 'Éª',\n 'É´',\n 'Øª',\n 'Ø³',\n 'Ú©',\n 'à¤ˆ',\n 'à°¾',\n 'à°¿',\n 'à¹ˆ',\n 'á´',\n 'áµ‰',\n 'áµ’',\n 'â‚¹',\n 'â•­',\n 'â•®',\n 'âœ‹',\n 'âœ',\n 'â“',\n 'â—',\n 'â¬‡',\n 'ã®',\n 'ğ—²',\n 'ğŸŒ™',\n 'ğŸ§',\n 'ğŸ°',\n 'ğŸ“',\n 'ğŸ³',\n 'ğŸ±',\n 'ğŸ‘',\n 'ğŸ‘¨',\n 'ğŸ‘­',\n 'ğŸ’©',\n 'ğŸ“„',\n 'ğŸ”«',\n 'ğŸ–',\n 'ğŸ˜Ÿ',\n 'ğŸ˜®',\n 'ğŸ˜°',\n 'ğŸ¤',\n 'ğŸ¤¯',\n '\\U000e0065',\n '\\U000e006e',\n 'Ã¤',\n 'Ã´',\n 'Ã½',\n 'Ä',\n 'Å‚',\n 'Ê€',\n 'Ë¢',\n 'Ù†',\n 'Û’',\n 'à¤†',\n 'à¤‡',\n 'à¤·',\n 'à¤¼',\n 'à¥Œ',\n 'à¥¤',\n 'à¨‚',\n 'à°‚',\n 'à°•',\n 'à°¡',\n 'à°ª',\n 'à°¸',\n 'à±',\n 'à¸„',\n '\\u200b',\n '\\u2066',\n 'âŒ',\n 'â¬†',\n 'ãƒ³',\n 'ãƒ¼',\n 'ğ„',\n 'ğ˜€',\n 'ğŸ‡©',\n 'ğŸ‡«',\n 'ğŸ‡¬',\n 'ğŸ‡¸',\n 'ğŸ‡º',\n 'ğŸ‡¿',\n 'ğŸŒŸ',\n 'ğŸŒ»',\n 'ğŸ¾',\n 'ğŸ€',\n 'ğŸ‡',\n 'ğŸ¤',\n 'ğŸ¿',\n 'ğŸ·',\n 'ğŸ‘Š',\n 'ğŸ‘ ',\n 'ğŸ‘°',\n 'ğŸ‘¸',\n 'ğŸ‘¿',\n 'ğŸ’',\n 'ğŸ’¸',\n 'ğŸ“ƒ',\n 'ğŸ”',\n 'ğŸ”´',\n 'ğŸ—£',\n 'ğŸ˜—',\n 'ğŸ˜™',\n 'ğŸ™',\n 'ğŸš¨',\n 'ğŸ¤Ÿ',\n 'ğŸ§',\n 'ğŸ§š',\n 'Â®',\n 'Â¯',\n 'Â·',\n 'Âº',\n 'Ã—',\n 'Ã®',\n 'Ã³',\n 'Ã¶',\n 'Ãº',\n 'Ä‘',\n 'Ä“',\n 'Ä«',\n 'Å™',\n 'Å¯',\n 'Å¼',\n 'É£',\n 'É²',\n 'Ê¸',\n 'Ë˜',\n 'Ì©',\n 'Ğ°',\n 'Ñ”',\n 'Ò“',\n 'Õ¬',\n 'Ø¨',\n 'Ø´',\n 'ÙŠ',\n 'Ûƒ',\n 'à¤',\n 'à¤ƒ',\n 'à¤›',\n 'à¤£',\n 'à¥ƒ',\n 'à¨¤',\n 'à¨°',\n 'à¨¹',\n 'à©€',\n 'à©',\n 'à°‰',\n 'à°¤',\n 'à°¦',\n 'à°¨',\n 'à°®',\n 'à°¯',\n 'à°²',\n 'à°·',\n 'à±‹',\n 'à¸',\n 'à¸—',\n 'à¸™',\n 'à¸š',\n 'à¸›',\n 'à¸œ',\n 'à¸­',\n 'à¸µ',\n 'à¸¸',\n 'á´…',\n 'á´‹',\n 'á´',\n 'á´˜',\n 'áµƒ',\n 'áµ',\n 'áµ',\n 'áµ˜',\n 'á¶¦',\n 'áº¡',\n 'áº½',\n 'â€¿',\n '\\u2069',\n 'âƒ‘',\n 'âƒ£',\n 'â–º',\n 'â—‰',\n 'â™¡',\n 'âœˆ',\n 'â €',\n 'â¤´',\n 'ã€£',\n 'ã™',\n 'ã£',\n 'ã§',\n 'ãƒ€',\n 'ãƒ’',\n 'ãƒ§',\n 'ã… ',\n 'å®š',\n 'æ—¥',\n 'é™',\n 'ê•Š',\n 'ê¸°',\n 'ë‹¤',\n 'ìŠ¤',\n 'ï·º',\n 'ï½“',\n 'ï½¡',\n 'ğ',\n 'ğ¡',\n 'ğ—°',\n 'ğ—¶',\n 'ğŸ‡§',\n 'ğŸ‡­',\n 'ğŸ‡±',\n 'ğŸŒ',\n 'ğŸŒ',\n 'ğŸŒ',\n 'ğŸŒ˜',\n 'ğŸŒ',\n 'ğŸŒ´',\n 'ğŸŒ¶',\n 'ğŸƒ',\n 'ğŸ',\n 'ğŸ',\n 'ğŸ',\n 'ğŸ¦',\n 'ğŸ¨',\n 'ğŸ¬',\n 'ğŸ‹',\n 'ğŸ§',\n 'ğŸ¬',\n 'ğŸ‚',\n 'ğŸ–',\n 'ğŸ¯',\n 'ğŸ¼',\n 'ğŸ‘§',\n 'ğŸ‘º',\n 'ğŸ‘¾',\n 'ğŸ’€',\n 'ğŸ’',\n 'ğŸ’…',\n 'ğŸ’Œ',\n 'ğŸ’¡',\n 'ğŸ’¬',\n 'ğŸ“·',\n 'ğŸ˜–',\n 'ğŸ™‹',\n 'ğŸš¬',\n 'ğŸš´',\n 'ğŸš¶',\n 'ğŸ¤’',\n 'ğŸ¤¤',\n 'ğŸ¥µ',\n 'ğŸ¦‰',\n 'ğŸ¦',\n 'Â£',\n 'Â¥',\n 'Â°',\n 'Â¿',\n 'Ã ',\n 'Ã¬',\n 'Ã²',\n 'Ä‡',\n 'Ä™',\n 'Ä›',\n 'ÄŸ',\n 'Åˆ',\n 'Å›',\n 'Å¡',\n 'Æ°',\n 'É¢',\n 'Ê',\n 'Êœ',\n 'ÊŸ',\n 'Ê°',\n 'Ê³',\n 'Î´',\n 'Î¼',\n 'Ğ´',\n 'Ğº',\n 'Ğ¼',\n 'Ğ¾',\n 'Ñ€',\n 'Ñ‚',\n 'Ø¦',\n 'Ø®',\n 'Ø²',\n 'Øµ',\n 'Ø·',\n 'Øº',\n 'Ù‡',\n 'Ú¯',\n 'Úº',\n 'Û',\n 'à¤‘',\n 'à¤”',\n 'à¤ ',\n 'à¥‰',\n 'à¦ˆ',\n 'à¦•',\n 'à¦¦',\n 'à¦¬',\n 'à¦®',\n 'à¦°',\n 'à¦¾',\n 'à§‹',\n 'à¨•',\n 'à¨—',\n 'à¨£',\n 'à¨¨',\n 'à¨«',\n 'à¨¸',\n 'à¨¾',\n 'à¨¿',\n 'à©‡',\n 'à©‹',\n 'à°…',\n 'à°‡',\n 'à°š',\n 'à°§',\n 'à°¬',\n 'à°µ',\n 'à±‚',\n 'à±‡',\n 'à¸‚',\n 'à¸‡',\n 'à¸“',\n 'à¸¢',\n 'à¸£',\n 'à¸¥',\n 'à¸§',\n 'à¸±',\n 'à¸²',\n 'à¸¶',\n 'à¸¹',\n 'à¹',\n 'à¹„',\n 'à¹‰',\n 'á´„',\n 'á´œ',\n 'á´ ',\n 'áµˆ',\n 'áµ',\n 'áµ—',\n 'áµ›',\n 'á¶œ',\n 'áº¿',\n 'á»',\n 'á»‰',\n 'á»‘',\n 'á»“',\n 'á»',\n 'á»§',\n 'â€•',\n 'â‰',\n 'â‚¬',\n 'â„…',\n 'â„¢',\n 'â†‘',\n 'â‡’',\n 'âŒš',\n 'â–‚',\n 'â˜€',\n 'â˜…',\n 'â˜”',\n 'â˜ª',\n 'âš”',\n 'âš•',\n 'âš˜',\n 'âšª',\n 'âš½',\n 'â›²',\n 'â›½',\n 'âœ‰',\n 'âœ´',\n 'â‡',\n 'â”',\n 'â•',\n 'â­',\n 'ã€',\n 'ã„',\n 'ãŒ',\n 'ã“',\n 'ã¤',\n 'ã¦',\n 'ã¨',\n 'ã«',\n 'ã­',\n 'ã¯',\n 'ã¹',\n 'ã¾',\n 'ã‚‹',\n 'ã‚’',\n 'ã‚¢',\n 'ã‚¤',\n 'ã‚¯',\n 'ã‚³',\n 'ã‚¸',\n 'ã‚¹',\n 'ã‚º',\n 'ãƒ†',\n 'ãƒˆ',\n 'ãƒ',\n 'ãƒ‘',\n 'ãƒ“',\n 'ãƒ™',\n 'ãƒ ',\n 'ãƒª',\n 'ãƒ«',\n 'å½“',\n 'æ”¾',\n 'çŒ«',\n 'ç”Ÿ',\n 'èª•',\n 'é–‹',\n 'ê²½',\n 'ê³„',\n 'êµ­',\n 'ê»˜',\n 'ë‚˜',\n 'ëŠ”',\n 'ë‹ˆ',\n 'ëŒ„',\n 'ëŸ¬',\n 'ë¦½',\n 'ë§Œ',\n 'ë¨¸',\n 'ë©€',\n 'ë·°',\n 'ìƒ',\n 'ì‹œ',\n 'ì‹¤',\n 'ì•½',\n 'ì™€',\n 'ì€',\n 'ì´',\n 'ì¼',\n 'ì',\n 'ì œ',\n 'ì¤‘',\n 'ì¦',\n 'í„°',\n 'í‹°',\n 'íŒ€',\n 'í•œ',\n 'í•¨',\n 'í–‰',\n 'í™©',\n 'ï·»',\n 'ï¸',\n 'ï½…',\n 'ï½',\n 'ï½”',\n 'ï½•',\n 'ğ‚',\n 'ğ‰',\n 'ğ–',\n 'ğš',\n 'ğ¢',\n 'ğ§',\n 'ğ©',\n 'ğ«',\n 'ğ¬',\n 'ğ­',\n 'ğ®',\n 'ğ¹',\n 'ğ‘”',\n 'ğ‘œ',\n 'ğ’»',\n 'ğ’½',\n 'ğ’¾',\n 'ğ“‡',\n 'ğ“ˆ',\n 'ğ“‰',\n 'ğ“Š',\n 'ğ—£',\n 'ğ—®',\n 'ğ—¯',\n 'ğ—´',\n 'ğ—¹',\n 'ğ—º',\n 'ğ—¿',\n 'ğ˜‚',\n 'ğ˜ƒ',\n 'ğŸ',\n 'ğŸ',\n 'ğŸ”',\n 'ğŸ‡¨',\n 'ğŸ‡ª',\n 'ğŸ‡²',\n 'ğŸ‡·',\n 'ğŸ‡¾',\n 'ğŸŒ„',\n 'ğŸŒŠ',\n 'ğŸŒŒ',\n 'ğŸŒ³',\n 'ğŸ€',\n 'ğŸ‚',\n 'ğŸ‡',\n 'ğŸŸ',\n 'ğŸ±',\n 'ğŸ†',\n 'ğŸ—',\n 'ğŸŸ',\n 'ğŸ¥',\n 'ğŸ©',\n 'ğŸ',\n 'ğŸ‹',\n 'ğŸ–',\n 'ğŸ¡',\n 'ğŸˆ',\n 'ğŸ',\n 'ğŸ’',\n 'ğŸ“',\n 'ğŸ•',\n 'ğŸ—',\n 'ğŸ¥',\n 'ğŸ®',\n 'ğŸ¶',\n 'ğŸ¸',\n 'ğŸ¿',\n 'ğŸ‘…',\n 'ğŸ‘†',\n 'ğŸ‘',\n 'ğŸ‘’',\n 'ğŸ‘',\n 'ğŸ‘Ÿ',\n 'ğŸ‘¥',\n 'ğŸ‘¶',\n 'ğŸ‘¹',\n 'ğŸ‘¼',\n 'ğŸ’',\n 'ğŸ’Ÿ',\n 'ğŸ’£',\n 'ğŸ’¾',\n 'ğŸ“ˆ',\n 'ğŸ“',\n 'ğŸ“£',\n 'ğŸ“»',\n 'ğŸ“¿',\n 'ğŸ”',\n 'ğŸ”ˆ',\n 'ğŸ”',\n 'ğŸ””',\n 'ğŸ”š',\n 'ğŸ”',\n 'ğŸ”Ÿ',\n 'ğŸ”¨',\n 'ğŸ”ª',\n 'ğŸ”¶',\n 'ğŸ”·',\n 'ğŸ•‹',\n 'ğŸ•Œ',\n 'ğŸ•¯',\n 'ğŸ•º',\n 'ğŸ˜¦',\n 'ğŸ˜²',\n 'ğŸ˜µ',\n 'ğŸ˜¼',\n 'ğŸ˜¿',\n 'ğŸ™€',\n 'ğŸ™…',\n 'ğŸ›',\n 'ğŸ›',\n 'ğŸ¤',\n 'ğŸ¤‘',\n 'ğŸ¤•',\n 'ğŸ¤š',\n 'ğŸ¤ ',\n 'ğŸ¤¡',\n 'ğŸ¤®',\n 'ğŸ¥‚',\n 'ğŸ¥ƒ',\n 'ğŸ¥‡',\n 'ğŸ¥­',\n 'ğŸ¥´',\n 'ğŸ¥¾',\n 'ğŸ¦',\n 'ğŸ¦„',\n 'ğŸ¦…',\n 'ğŸ¦³',\n 'ğŸ§',\n 'ğŸ§¸',\n '\\U000e0063',\n '\\U000e0073',\n '\\U000e0074']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch = 128\n",
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                          )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "val_iter = torchtext.data.BucketIterator(val,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                        )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                         )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<torchtext.data.iterator.BucketIterator object at 0x7f8ee5833f10>\ntensor([[23,  2, 17,  ..., 27,  4,  7],\n        [15,  3, 10,  ...,  8, 16, 24],\n        [ 9,  7, 23,  ...,  4,  5, 10],\n        ...,\n        [ 9,  7, 23,  ..., 77,  1,  1],\n        [23,  2, 10,  ..., 21,  1,  1],\n        [ 9,  7, 23,  ..., 46,  1,  1]])\ntensor([1, 1, 1, 2, 1, 1, 2, 0, 1, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 2, 0, 1,\n        1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 0, 2, 2, 1,\n        1, 0, 1, 2, 1, 1, 2, 2, 2, 0, 0, 0, 2, 2, 1, 2, 2, 0, 1, 2, 2, 1, 2, 0,\n        2, 0, 1, 2, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 2, 0, 0, 2, 1, 1, 2, 1, 1, 2,\n        0, 2, 2, 0, 1, 1, 2, 2, 1, 0, 1, 2, 2, 2, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1,\n        2, 2, 1, 1, 0, 2, 1, 0])\ntensor([[23,  2, 28,  ..., 20, 14,  5],\n        [23,  2, 18,  ...,  2, 17,  5],\n        [ 9,  7, 23,  ...,  3, 14,  4],\n        ...,\n        [23,  2, 10,  ...,  1,  1,  1],\n        [23,  2, 10,  ...,  1,  1,  1],\n        [23,  2, 12,  ...,  1,  1,  1]])\ntensor([2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2,\n        2, 2, 1, 1, 1, 2, 2, 0, 0, 1, 2, 0, 2, 1, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0,\n        2, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1,\n        0, 2, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0,\n        2, 2, 0, 0, 1, 2, 2, 1, 1, 2, 1, 1, 0, 1, 0, 2, 2, 0, 0, 1, 2, 1, 2, 1,\n        0, 0, 0, 1, 0, 1, 1, 2])\ntensor([[23,  2, 15,  ..., 15,  4,  3],\n        [10,  8, 13,  ..., 16, 17, 24],\n        [23,  2, 10,  ..., 21,  3,  1],\n        ...,\n        [23,  2,  3,  ...,  1,  1,  1],\n        [23,  2,  6,  ...,  1,  1,  1],\n        [ 9,  3, 25,  ...,  1,  1,  1]])\ntensor([0, 2, 0, 2, 2, 2, 2, 1, 1, 1, 0, 1, 1, 0, 2, 2, 2, 1, 1, 2, 0, 0, 2, 1,\n        2, 2, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1, 2, 2, 0, 1, 1, 0, 1, 1, 0, 2, 1, 0,\n        1, 1, 2, 0, 2, 2, 2, 0, 2, 1, 2, 1, 2, 0, 0, 0, 1, 2, 1, 1, 1, 0, 2, 1,\n        2, 0, 0, 1, 1, 0, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2,\n        0, 1, 1, 0, 1, 0, 2, 2, 2, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 0, 0, 0, 2,\n        1, 2, 1, 1, 2, 1, 1, 1])\ntensor([[ 11,   2, 291,  ...,   9,  43,  10],\n        [ 10,   3,   9,  ...,  39,  13,  27],\n        [  9,   7,  23,  ...,  46,  46,  29],\n        ...,\n        [ 23,   2,   4,  ...,   9,  40,  42],\n        [ 23,   2,  12,  ...,   4,  21,  24],\n        [ 23,   2,  26,  ...,   5,   9,   4]])\ntensor([2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 0, 1, 2, 1, 2, 1, 1, 2, 2, 0, 2,\n        1, 1, 2, 1, 0, 2, 1, 2, 1, 2, 0, 0, 0, 2, 1, 1, 2, 0, 1, 1, 0, 1, 0, 0,\n        1, 1, 2, 1, 2, 1, 1, 0, 0, 2, 2, 1, 1, 0, 1, 2, 2, 2, 1, 0, 2, 2, 0, 2,\n        1, 1, 1, 1, 0, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2,\n        1, 0, 2, 2, 2, 1, 1, 2, 1, 0, 0, 1, 2, 2, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2,\n        0, 2, 1, 1, 0, 1, 0, 2])\ntensor([[23,  2,  3,  ...,  3,  6, 18],\n        [ 9,  7, 23,  ..., 20, 12, 29],\n        [ 9,  7, 23,  ..., 18,  3, 29],\n        ...,\n        [23,  2, 10,  ..., 27, 13, 17],\n        [23,  2,  3,  ..., 14,  5, 11],\n        [23,  2,  3,  ...,  9,  6,  3]])\ntensor([2, 1, 2, 1, 2, 2, 2, 0, 1, 1, 0, 1, 0, 0, 1, 2, 1, 2, 0, 1, 2, 2, 2, 1,\n        1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2, 1, 1, 0, 1, 1, 2, 2, 1,\n        0, 1, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 1, 2, 1, 2, 1, 2, 1, 0, 0, 2, 0,\n        1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 1,\n        0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2, 2, 1, 1, 0, 2, 2, 1, 0, 2,\n        1, 0, 0, 1, 0, 1, 0, 0])\ntensor([[ 23,   2,  26,  ...,  19,   8,  16],\n        [ 23,   2,   6,  ...,   5, 175, 396],\n        [ 23,   2,  17,  ...,   8,  98,  46],\n        ...,\n        [ 23,   2,   3,  ...,   1,   1,   1],\n        [ 23,   2,   4,  ...,   1,   1,   1],\n        [ 23,   2,  10,  ...,   1,   1,   1]])\ntensor([2, 1, 2, 0, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 0, 0, 2, 1,\n        1, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 1, 0, 2, 2,\n        0, 0, 0, 2, 0, 1, 2, 2, 1, 1, 1, 2, 2, 0, 0, 2, 1, 2, 0, 2, 0, 2, 1, 2,\n        0, 2, 0, 0, 0, 2, 1, 2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 2, 1, 1, 0, 0, 2, 2,\n        2, 1, 2, 0, 2, 2, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 1, 0, 0, 1, 2, 2, 1, 2,\n        2, 2, 2, 1, 2, 1, 2, 0])\ntensor([[ 9,  7, 23,  ...,  7, 31, 29],\n        [28,  8,  7,  ..., 27, 16, 15],\n        [23, 31,  2,  ..., 10,  7, 36],\n        ...,\n        [ 9,  7, 23,  ..., 25,  3, 29],\n        [ 9,  7, 23,  ...,  6,  5, 29],\n        [21, 16, 19,  ...,  8, 34, 35]])\ntensor([2, 1, 2, 1, 1, 1, 2, 0, 2, 1, 2, 1, 1, 2, 0, 2, 1, 1, 1, 1, 2, 1, 0, 2,\n        2, 2, 2, 2, 1, 1, 0, 2, 1, 0, 2, 2, 2, 0, 2, 0, 2, 1, 1, 1, 0, 1, 1, 2,\n        1, 0, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, 2, 2, 2, 1, 2, 1, 1,\n        1, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0,\n        0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n        2, 0, 1, 1, 0, 1, 2, 2])\ntensor([[23,  2, 13,  ..., 19,  5, 52],\n        [23,  2, 13,  ..., 51, 53, 51],\n        [23,  2,  3,  ...,  3,  4, 24],\n        ...,\n        [23,  2, 18,  ..., 46,  1,  1],\n        [ 9,  7, 23,  ...,  3,  1,  1],\n        [23,  2, 15,  ...,  3,  1,  1]])\ntensor([1, 1, 0, 1, 1, 1, 0, 0, 2, 2, 1, 0, 0, 1, 0, 1, 1, 0, 2, 0, 0, 0, 2, 2,\n        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 2, 2, 2, 2, 1, 0, 0, 2, 0, 2,\n        0, 0, 2, 0, 1, 1, 2, 0, 0, 1, 2, 1, 2, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 0, 2, 0, 0, 0, 1, 2, 2, 0, 2, 1,\n        2, 2, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 2, 1, 0, 2, 1, 0, 2, 2, 1, 1, 1, 1,\n        0, 1, 0, 1, 2, 2, 2, 2])\ntensor([[23,  2,  3,  ..., 43, 19,  3],\n        [23,  2, 11,  ..., 40,  4, 25],\n        [23,  2, 11,  ...,  6, 36,  8],\n        ...,\n        [23,  2, 13,  ..., 12, 18,  1],\n        [ 9,  7, 23,  ...,  4, 29,  1],\n        [ 6,  5,  2,  ..., 43,  7,  1]])\ntensor([2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 1,\n        2, 2, 0, 1, 0, 1, 1, 2, 1, 1, 0, 2, 2, 2, 1, 0, 2, 1, 0, 1, 0, 2, 2, 2,\n        1, 1, 1, 1, 0, 1, 0, 0, 0, 2, 0, 2, 2, 1, 0, 1, 2, 2, 1, 1, 1, 0, 0, 0,\n        1, 2, 2, 1, 0, 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0,\n        1, 1, 0, 2, 2, 1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1,\n        0, 1, 0, 2, 2, 2, 0, 2])\ntensor([[23,  2, 10,  ...,  6,  5, 24],\n        [23,  2,  4,  ...,  9, 38, 15],\n        [23,  2, 10,  ...,  4, 13,  5],\n        ...,\n        [27, 16, 20,  ...,  1,  1,  1],\n        [23,  2, 14,  ...,  1,  1,  1],\n        [23,  2, 13,  ...,  1,  1,  1]])\ntensor([0, 2, 0, 1, 1, 2, 2, 1, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 2,\n        0, 0, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 2,\n        2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 0, 2,\n        1, 1, 2, 2, 2, 0, 2, 1, 1, 0, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 0, 2, 1, 0,\n        2, 0, 1, 2, 0, 0, 2, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 2, 2, 0, 2, 0, 1, 0,\n        2, 2, 0, 1, 2, 0, 1, 2])\n"
    }
   ],
   "source": [
    "print(train_iter)\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(batch.text)\n",
    "    print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, bidir=False, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(2*n_hidden, n_output)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out[:,-1])\n",
    "        log_probs = F.log_softmax(out)\n",
    "\n",
    "        # sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        # sigmoid_out = fc_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        # sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        return log_probs\n",
    "        # return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(logits, labels):\n",
    "    correct, total = 0, 0\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "#     print(predicted, labels)\n",
    "#     print(predicted.shape, labels.shape)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        h = model.init_hidden(len(batch))\n",
    "        batch.text = batch.text.to(device)\n",
    "        # predictions, _ = model(batch.text)\n",
    "        predictions = model(batch.text)\n",
    "        # predictions = predictions.squeeze(1)\n",
    "        # print(batch.text.shape, predictions.shape, batch.label.shape)\n",
    "\n",
    "        # target = torch.tensor(batch.label, dtype=torch.float, device=device)\n",
    "        loss = criterion(predictions, batch.label.to(device))\n",
    "\n",
    "        acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label.to(device))\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_metrics(model, config):\n",
    "    train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])\n",
    "    exp_name = config[\"NAME\"]\n",
    "    test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=len(test),\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True)        # sort within each batch\n",
    "    print(\"BEST METRICS VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "            conf = confusion_matrix(batch.label.cpu(), preds.cpu())\n",
    "            pp.pprint(conf)\n",
    "            \n",
    "\n",
    "    print(\"BEST METRICS TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-train.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "            conf = confusion_matrix(batch.label.cpu(), preds.cpu())\n",
    "            pp.pprint(conf)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, config):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    exp_name = config[\"NAME\"]\n",
    "    print(\"TEST VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    print(\"TEST TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-train.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(model, config):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = config[\"LR\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = config[\"N_EPOCHS\"]\n",
    "    exp_name = config[\"NAME\"]\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    print(\"BEGIN TRAINING\")\n",
    "    print(\"-\"*50)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(\"SAVED VALID\")\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-valid.pt'.format(exp_name))\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            print(\"SAVED TRAIN\")\n",
    "            best_train_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-train.pt'.format(exp_name))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning Sentiment Analysis, Random Embeddings, hinglish\n{   'BIDIR': True,\n    'DRPOUT': 0.5,\n    'EMB_TRAIN': True,\n    'LR': 0.01,\n    'NAME': 'LSTM_RandomEmbeddings_hinglish',\n    'N_EMBED': 400,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 130,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 899}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nSAVED VALID\nSAVED TRAIN\nEpoch: 01 | Epoch Time: 0m 4s\n\tTrain Loss: 1.331 | Train Acc: 35.82%\n\t Val. Loss: 1.129 |  Val. Acc: 36.18%\nSAVED VALID\nSAVED TRAIN\nEpoch: 02 | Epoch Time: 0m 4s\n\tTrain Loss: 1.099 | Train Acc: 37.69%\n\t Val. Loss: 1.101 |  Val. Acc: 36.63%\nSAVED VALID\nSAVED TRAIN\nEpoch: 03 | Epoch Time: 0m 4s\n\tTrain Loss: 1.093 | Train Acc: 38.13%\n\t Val. Loss: 1.084 |  Val. Acc: 38.56%\nEpoch: 04 | Epoch Time: 0m 4s\n\tTrain Loss: 1.090 | Train Acc: 38.06%\n\t Val. Loss: 1.104 |  Val. Acc: 36.46%\nSAVED VALID\nEpoch: 05 | Epoch Time: 0m 4s\n\tTrain Loss: 1.086 | Train Acc: 38.61%\n\t Val. Loss: 1.084 |  Val. Acc: 36.79%\nEpoch: 06 | Epoch Time: 0m 4s\n\tTrain Loss: 1.085 | Train Acc: 38.57%\n\t Val. Loss: 1.097 |  Val. Acc: 36.09%\nSAVED TRAIN\nEpoch: 07 | Epoch Time: 0m 4s\n\tTrain Loss: 1.080 | Train Acc: 39.38%\n\t Val. Loss: 1.089 |  Val. Acc: 37.98%\nSAVED VALID\nSAVED TRAIN\nEpoch: 08 | Epoch Time: 0m 4s\n\tTrain Loss: 1.083 | Train Acc: 39.05%\n\t Val. Loss: 1.077 |  Val. Acc: 39.10%\nEpoch: 09 | Epoch Time: 0m 4s\n\tTrain Loss: 1.079 | Train Acc: 39.30%\n\t Val. Loss: 1.080 |  Val. Acc: 37.10%\nSAVED VALID\nSAVED TRAIN\nEpoch: 10 | Epoch Time: 0m 4s\n\tTrain Loss: 1.069 | Train Acc: 40.23%\n\t Val. Loss: 1.043 |  Val. Acc: 42.61%\nSAVED VALID\nEpoch: 11 | Epoch Time: 0m 4s\n\tTrain Loss: 1.051 | Train Acc: 42.80%\n\t Val. Loss: 1.036 |  Val. Acc: 43.59%\nSAVED VALID\nSAVED TRAIN\nEpoch: 12 | Epoch Time: 0m 4s\n\tTrain Loss: 1.043 | Train Acc: 44.08%\n\t Val. Loss: 1.031 |  Val. Acc: 42.45%\nSAVED VALID\nSAVED TRAIN\nEpoch: 13 | Epoch Time: 0m 4s\n\tTrain Loss: 1.031 | Train Acc: 45.27%\n\t Val. Loss: 1.019 |  Val. Acc: 46.04%\nEpoch: 14 | Epoch Time: 0m 4s\n\tTrain Loss: 1.022 | Train Acc: 45.37%\n\t Val. Loss: 1.026 |  Val. Acc: 46.69%\nSAVED TRAIN\nEpoch: 15 | Epoch Time: 0m 4s\n\tTrain Loss: 1.019 | Train Acc: 45.90%\n\t Val. Loss: 1.025 |  Val. Acc: 45.03%\nSAVED TRAIN\nEpoch: 16 | Epoch Time: 0m 4s\n\tTrain Loss: 1.016 | Train Acc: 46.36%\n\t Val. Loss: 1.023 |  Val. Acc: 45.98%\nSAVED TRAIN\nEpoch: 17 | Epoch Time: 0m 4s\n\tTrain Loss: 1.013 | Train Acc: 46.66%\n\t Val. Loss: 1.029 |  Val. Acc: 44.42%\nSAVED TRAIN\nEpoch: 18 | Epoch Time: 0m 4s\n\tTrain Loss: 1.011 | Train Acc: 46.81%\n\t Val. Loss: 1.030 |  Val. Acc: 43.98%\nSAVED VALID\nSAVED TRAIN\nEpoch: 19 | Epoch Time: 0m 4s\n\tTrain Loss: 1.005 | Train Acc: 47.88%\n\t Val. Loss: 1.001 |  Val. Acc: 47.06%\nEpoch: 20 | Epoch Time: 0m 4s\n\tTrain Loss: 1.003 | Train Acc: 46.81%\n\t Val. Loss: 1.012 |  Val. Acc: 46.82%\nEpoch: 21 | Epoch Time: 0m 4s\n\tTrain Loss: 1.002 | Train Acc: 48.00%\n\t Val. Loss: 1.016 |  Val. Acc: 45.23%\nSAVED TRAIN\nEpoch: 22 | Epoch Time: 0m 4s\n\tTrain Loss: 0.998 | Train Acc: 48.36%\n\t Val. Loss: 1.018 |  Val. Acc: 45.73%\nSAVED TRAIN\nEpoch: 23 | Epoch Time: 0m 4s\n\tTrain Loss: 1.002 | Train Acc: 47.38%\n\t Val. Loss: 1.006 |  Val. Acc: 46.19%\nSAVED VALID\nSAVED TRAIN\nEpoch: 24 | Epoch Time: 0m 4s\n\tTrain Loss: 0.997 | Train Acc: 48.29%\n\t Val. Loss: 1.000 |  Val. Acc: 47.49%\nSAVED VALID\nSAVED TRAIN\nEpoch: 25 | Epoch Time: 0m 4s\n\tTrain Loss: 0.998 | Train Acc: 48.08%\n\t Val. Loss: 0.997 |  Val. Acc: 47.96%\nEpoch: 26 | Epoch Time: 0m 4s\n\tTrain Loss: 1.000 | Train Acc: 47.64%\n\t Val. Loss: 1.001 |  Val. Acc: 46.12%\nSAVED TRAIN\nEpoch: 27 | Epoch Time: 0m 4s\n\tTrain Loss: 0.992 | Train Acc: 48.43%\n\t Val. Loss: 1.014 |  Val. Acc: 45.93%\nSAVED TRAIN\nEpoch: 28 | Epoch Time: 0m 4s\n\tTrain Loss: 0.988 | Train Acc: 49.41%\n\t Val. Loss: 1.017 |  Val. Acc: 45.92%\nSAVED TRAIN\nEpoch: 29 | Epoch Time: 0m 4s\n\tTrain Loss: 0.990 | Train Acc: 49.05%\n\t Val. Loss: 1.005 |  Val. Acc: 47.38%\nSAVED TRAIN\nEpoch: 30 | Epoch Time: 0m 4s\n\tTrain Loss: 0.987 | Train Acc: 49.00%\n\t Val. Loss: 1.004 |  Val. Acc: 48.87%\nSAVED VALID\nSAVED TRAIN\nEpoch: 31 | Epoch Time: 0m 4s\n\tTrain Loss: 0.985 | Train Acc: 49.94%\n\t Val. Loss: 0.993 |  Val. Acc: 47.79%\nSAVED TRAIN\nEpoch: 32 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.37%\n\t Val. Loss: 0.995 |  Val. Acc: 48.06%\nSAVED TRAIN\nEpoch: 33 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.30%\n\t Val. Loss: 1.019 |  Val. Acc: 44.74%\nSAVED VALID\nSAVED TRAIN\nEpoch: 34 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.62%\n\t Val. Loss: 0.993 |  Val. Acc: 48.74%\nSAVED VALID\nSAVED TRAIN\nEpoch: 35 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.13%\n\t Val. Loss: 0.988 |  Val. Acc: 50.32%\nSAVED TRAIN\nEpoch: 36 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 49.76%\n\t Val. Loss: 0.993 |  Val. Acc: 49.32%\nSAVED TRAIN\nEpoch: 37 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.31%\n\t Val. Loss: 1.013 |  Val. Acc: 47.24%\nSAVED VALID\nSAVED TRAIN\nEpoch: 38 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.69%\n\t Val. Loss: 0.984 |  Val. Acc: 50.32%\nSAVED VALID\nSAVED TRAIN\nEpoch: 39 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.43%\n\t Val. Loss: 0.980 |  Val. Acc: 51.45%\nSAVED TRAIN\nEpoch: 40 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.87%\n\t Val. Loss: 0.993 |  Val. Acc: 50.13%\nSAVED TRAIN\nEpoch: 41 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 50.13%\n\t Val. Loss: 0.989 |  Val. Acc: 49.15%\nSAVED TRAIN\nEpoch: 42 | Epoch Time: 0m 4s\n\tTrain Loss: 0.971 | Train Acc: 50.48%\n\t Val. Loss: 0.991 |  Val. Acc: 48.73%\nSAVED TRAIN\nEpoch: 43 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 49.78%\n\t Val. Loss: 0.994 |  Val. Acc: 48.51%\nSAVED TRAIN\nEpoch: 44 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.75%\n\t Val. Loss: 0.991 |  Val. Acc: 49.86%\nSAVED TRAIN\nEpoch: 45 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.11%\n\t Val. Loss: 0.985 |  Val. Acc: 49.48%\nSAVED TRAIN\nEpoch: 46 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.64%\n\t Val. Loss: 0.987 |  Val. Acc: 47.63%\nSAVED TRAIN\nEpoch: 47 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.31%\n\t Val. Loss: 0.997 |  Val. Acc: 48.46%\nSAVED TRAIN\nEpoch: 48 | Epoch Time: 0m 4s\n\tTrain Loss: 0.982 | Train Acc: 49.25%\n\t Val. Loss: 1.002 |  Val. Acc: 47.44%\nSAVED VALID\nSAVED TRAIN\nEpoch: 49 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.88%\n\t Val. Loss: 0.972 |  Val. Acc: 49.73%\nEpoch: 50 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 50.54%\n\t Val. Loss: 0.982 |  Val. Acc: 49.47%\nEpoch: 51 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 50.21%\n\t Val. Loss: 0.972 |  Val. Acc: 50.61%\nEpoch: 52 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 50.37%\n\t Val. Loss: 1.016 |  Val. Acc: 46.95%\nEpoch: 53 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.78%\n\t Val. Loss: 0.986 |  Val. Acc: 51.01%\nEpoch: 54 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.80%\n\t Val. Loss: 0.982 |  Val. Acc: 50.71%\nEpoch: 55 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 50.19%\n\t Val. Loss: 0.995 |  Val. Acc: 47.42%\nEpoch: 56 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.66%\n\t Val. Loss: 0.985 |  Val. Acc: 50.50%\nEpoch: 57 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.36%\n\t Val. Loss: 0.989 |  Val. Acc: 49.39%\nEpoch: 58 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.64%\n\t Val. Loss: 0.987 |  Val. Acc: 51.07%\nEpoch: 59 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 50.11%\n\t Val. Loss: 0.996 |  Val. Acc: 49.45%\nEpoch: 60 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.89%\n\t Val. Loss: 0.980 |  Val. Acc: 50.31%\nEpoch: 61 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.58%\n\t Val. Loss: 0.983 |  Val. Acc: 50.01%\nEpoch: 62 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 49.79%\n\t Val. Loss: 0.992 |  Val. Acc: 47.33%\nEpoch: 63 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.07%\n\t Val. Loss: 0.984 |  Val. Acc: 49.16%\nEpoch: 64 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.43%\n\t Val. Loss: 0.980 |  Val. Acc: 48.06%\nEpoch: 65 | Epoch Time: 0m 4s\n\tTrain Loss: 0.982 | Train Acc: 49.62%\n\t Val. Loss: 0.992 |  Val. Acc: 49.35%\nEpoch: 66 | Epoch Time: 0m 4s\n\tTrain Loss: 0.972 | Train Acc: 49.96%\n\t Val. Loss: 0.984 |  Val. Acc: 47.29%\nEpoch: 67 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.89%\n\t Val. Loss: 1.005 |  Val. Acc: 48.56%\nEpoch: 68 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.76%\n\t Val. Loss: 0.987 |  Val. Acc: 49.60%\nEpoch: 69 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.88%\n\t Val. Loss: 0.983 |  Val. Acc: 48.83%\nSAVED TRAIN\nEpoch: 70 | Epoch Time: 0m 4s\n\tTrain Loss: 0.972 | Train Acc: 50.46%\n\t Val. Loss: 0.995 |  Val. Acc: 47.98%\nSAVED TRAIN\nEpoch: 71 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.40%\n\t Val. Loss: 0.981 |  Val. Acc: 50.08%\nSAVED TRAIN\nEpoch: 72 | Epoch Time: 0m 4s\n\tTrain Loss: 0.973 | Train Acc: 50.15%\n\t Val. Loss: 0.983 |  Val. Acc: 49.38%\nSAVED TRAIN\nEpoch: 73 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.17%\n\t Val. Loss: 0.977 |  Val. Acc: 49.48%\nSAVED TRAIN\nEpoch: 74 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 49.79%\n\t Val. Loss: 0.976 |  Val. Acc: 50.76%\nEpoch: 75 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.99%\n\t Val. Loss: 0.975 |  Val. Acc: 52.47%\nSAVED TRAIN\nEpoch: 76 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.82%\n\t Val. Loss: 0.978 |  Val. Acc: 50.62%\nSAVED TRAIN\nEpoch: 77 | Epoch Time: 0m 4s\n\tTrain Loss: 0.971 | Train Acc: 49.97%\n\t Val. Loss: 0.994 |  Val. Acc: 50.14%\nSAVED TRAIN\nEpoch: 78 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.00%\n\t Val. Loss: 0.989 |  Val. Acc: 48.36%\nSAVED TRAIN\nEpoch: 79 | Epoch Time: 0m 4s\n\tTrain Loss: 0.974 | Train Acc: 49.75%\n\t Val. Loss: 0.981 |  Val. Acc: 50.28%\nSAVED VALID\nSAVED TRAIN\nEpoch: 80 | Epoch Time: 0m 4s\n\tTrain Loss: 0.970 | Train Acc: 50.70%\n\t Val. Loss: 0.968 |  Val. Acc: 50.70%\nEpoch: 81 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.11%\n\t Val. Loss: 0.985 |  Val. Acc: 50.34%\nEpoch: 82 | Epoch Time: 0m 4s\n\tTrain Loss: 0.977 | Train Acc: 49.71%\n\t Val. Loss: 0.985 |  Val. Acc: 50.20%\nEpoch: 83 | Epoch Time: 0m 4s\n\tTrain Loss: 0.969 | Train Acc: 50.64%\n\t Val. Loss: 0.990 |  Val. Acc: 49.98%\nEpoch: 84 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.52%\n\t Val. Loss: 0.981 |  Val. Acc: 50.60%\nEpoch: 85 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.86%\n\t Val. Loss: 0.978 |  Val. Acc: 50.26%\nEpoch: 86 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 49.66%\n\t Val. Loss: 0.990 |  Val. Acc: 48.04%\nEpoch: 87 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.92%\n\t Val. Loss: 0.991 |  Val. Acc: 48.74%\nEpoch: 88 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.79%\n\t Val. Loss: 0.984 |  Val. Acc: 49.05%\nEpoch: 89 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 50.02%\n\t Val. Loss: 0.990 |  Val. Acc: 49.24%\nEpoch: 90 | Epoch Time: 0m 4s\n\tTrain Loss: 0.980 | Train Acc: 49.55%\n\t Val. Loss: 0.980 |  Val. Acc: 49.63%\nEpoch: 91 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 48.95%\n\t Val. Loss: 1.003 |  Val. Acc: 48.40%\nEpoch: 92 | Epoch Time: 0m 4s\n\tTrain Loss: 0.975 | Train Acc: 49.59%\n\t Val. Loss: 0.991 |  Val. Acc: 47.87%\nEpoch: 93 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.61%\n\t Val. Loss: 0.987 |  Val. Acc: 47.64%\nEpoch: 94 | Epoch Time: 0m 4s\n\tTrain Loss: 0.981 | Train Acc: 49.48%\n\t Val. Loss: 0.981 |  Val. Acc: 49.26%\nEpoch: 95 | Epoch Time: 0m 4s\n\tTrain Loss: 0.978 | Train Acc: 49.16%\n\t Val. Loss: 0.974 |  Val. Acc: 49.72%\nEpoch: 96 | Epoch Time: 0m 4s\n\tTrain Loss: 0.979 | Train Acc: 49.58%\n\t Val. Loss: 0.979 |  Val. Acc: 51.01%\nEpoch: 97 | Epoch Time: 0m 4s\n\tTrain Loss: 0.976 | Train Acc: 49.29%\n\t Val. Loss: 0.978 |  Val. Acc: 50.25%\nEpoch: 98 | Epoch Time: 0m 4s\n\tTrain Loss: 0.984 | Train Acc: 49.36%\n\t Val. Loss: 0.970 |  Val. Acc: 51.71%\nEpoch: 99 | Epoch Time: 0m 4s\n\tTrain Loss: 0.983 | Train Acc: 49.74%\n\t Val. Loss: 0.970 |  Val. Acc: 51.46%\nEpoch: 100 | Epoch Time: 0m 4s\n\tTrain Loss: 0.986 | Train Acc: 49.40%\n\t Val. Loss: 0.980 |  Val. Acc: 50.39%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTEST VALID\n\t Test. Loss: 0.957 |  Test. Acc: 51.46%\nTEST TRAIN\n\t Test. Loss: 0.961 |  Test. Acc: 51.05%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\nGET METRICS\n--------------------------------------------------\nBEST METRICS VALID\nTest f1: 0.510 | Test Prec: 51.20% | Test Recall: 51.20% \narray([[263,  34, 144],\n       [ 78, 260, 168],\n       [201, 124, 241]])\nBEST METRICS TRAIN\nTest f1: 0.510 | Test Prec: 51.20% | Test Recall: 51.20% \narray([[263,  34, 144],\n       [ 78, 260, 168],\n       [201, 124, 241]])\n"
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(\"-\"*50)\n",
    "print(\"Running Sentiment Analysis, Random Embeddings, hinglish\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_RandomEmbeddings_hinglish\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : 400,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_HIDDEN\" : 130,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"BIDIR\": True,\n",
    "    \"DRPOUT\": 0.5,\n",
    "    \"LR\": 0.01\n",
    "}\n",
    "model = SentimentLSTM(config[\"N_VOCAB\"],config[\"N_EMBED\"], config[\"N_HIDDEN\"], config[\"N_OUTPUT\"], config[\"N_LAYERS\"], config[\"BIDIR\"], config[\"DRPOUT\"])\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)\n",
    "print(\"GET METRICS\")\n",
    "print(\"-\"*50)\n",
    "get_metrics(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = config[\"N_VOCAB\"]     # number of unique words in vocabulary\n",
    "        self.n_layers = config[\"N_LAYERS\"]   # number of LSTM layers \n",
    "        self.n_hidden = config[\"N_HIDDEN\"]   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.n_vocab, config[\"N_EMBED\"])\n",
    "        self.embedding.weight.data = torch.eye(self.n_vocab)\n",
    "        # make embedding untrainable\n",
    "        if not config[\"EMB_TRAIN\"]:\n",
    "            self.embedding.weight.requires_grad=False\n",
    "        self.lstm = nn.LSTM(config[\"N_EMBED\"], self.n_hidden, self.n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(self.n_hidden, config[\"N_OUTPUT\"])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Untrainable\n{   'EMB_TRAIN': False,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Untrainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 1,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 49.86%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Untrainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Untrainable\",\n",
    "    \"N_EPOCHS\": 1,\n",
    "    \"EMB_TRAIN\": False,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Trainable\n{   'EMB_TRAIN': True,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Trainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 9s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.82%\nEpoch: 02 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 03 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 50.08%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 04 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.02%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 05 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.78%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 06 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.20%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 07 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 48.80%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 08 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 46.21%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 09 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 44.87%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 10 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 44.31%\n\t Val. Loss: 0.693 |  Val. Acc: 49.78%\nEpoch: 11 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 41.84%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 12 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 40.35%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 13 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.64%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 14 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.10%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 15 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 16 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.21%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 17 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 34.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 18 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.87%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 19 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.33%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 20 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.41%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 21 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.17%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 22 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.15%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 23 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.02%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 24 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.90%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 25 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.00%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 26 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 27 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.96%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 28 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.93%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 29 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 30 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.94%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 31 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 32 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 33 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 34 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 35 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 36 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 37 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 38 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 39 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 40 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 41 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 42 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 43 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 44 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 45 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 46 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 47 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 48 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 49 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 50 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 51 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 52 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 53 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 54 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 55 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 56 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 57 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 58 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 59 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 60 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 61 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 62 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 63 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 64 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 65 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 66 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 67 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 68 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 69 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 70 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 71 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 72 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 73 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 74 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 75 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 76 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 77 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.41%\nEpoch: 78 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 79 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 80 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 81 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 82 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 83 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 84 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 85 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 86 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 87 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 88 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 89 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 90 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 91 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 92 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 93 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 94 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 95 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 96 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 97 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 98 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 99 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 100 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 34.29%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Trainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Trainable\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}