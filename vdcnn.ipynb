{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "random.seed(0)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/train_conll_spanglish.csv'\n",
    "import torchtext\n",
    "\n",
    "def label2int(label):\n",
    "    if label=='positive':\n",
    "        return 1\n",
    "    elif label=='negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x.lower(), # because are building a character-RNN\n",
    "                                  include_lengths=False, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "label_field = torchtext.data.Field(sequential=False,    # not a sequence\n",
    "                                   use_vocab=False,     # don't need to track vocabulary\n",
    "                                   is_target=True,      \n",
    "                                   batch_first=True,\n",
    "                                   preprocessing=lambda x: label2int(x)) # convert text to 0 and 1\n",
    "\n",
    "fields = [('id', None),('text', text_field), ('label', label_field)]\n",
    "dataset = torchtext.data.TabularDataset(filename, # name of the file\n",
    "                                        \"tsv\",               # fields are separated by a tab\n",
    "                                        fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "so that means tomorrow cruda segura lol --- 1\ntonight peda segura --- 2\neres tan mala vieja bruja interesada#jamming --- 0\nyo kiero pretzels lol --- 2\nfuck that ni ke el me vaya a mantener toda la vida lol --- 0\ni always tell my dad ke me kiero kasar con una vieja rika and me rega√±a telling me ke no sea interesada ha --- 0\nke me compre un carrito pa irme con mis friends and party lol --- 2\nwhy can i just find a rich bitch ke me mantenga y ya ha --- 2\nsince i started working ya ni disfruto la vida lol --- 0\nmy dad me regano cuzs i was telling that to my brother and lo andaba molestando lol --- 0\n"
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(dataset[i].text, \"---\", dataset[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "12000 1500 1500 15000\n"
    }
   ],
   "source": [
    "print(len(train), len(val), len(test), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f42806ce4d0>>,\n            {'<unk>': 0,\n             '<pad>': 1,\n             ' ': 2,\n             'e': 3,\n             'a': 4,\n             'o': 5,\n             't': 6,\n             's': 7,\n             'n': 8,\n             'i': 9,\n             'r': 10,\n             'l': 11,\n             'c': 12,\n             'd': 13,\n             'u': 14,\n             'm': 15,\n             'p': 16,\n             'h': 17,\n             'y': 18,\n             'g': 19,\n             '.': 20,\n             'b': 21,\n             '/': 22,\n             'v': 23,\n             'f': 24,\n             'j': 25,\n             ':': 26,\n             '@': 27,\n             '!': 28,\n             'q': 29,\n             'k': 30,\n             'w': 31,\n             '#': 32,\n             'z': 33,\n             ',': 34,\n             'x': 35,\n             '1': 36,\n             '0': 37,\n             '2': 38,\n             '_': 39,\n             '√≠': 40,\n             '\"': 41,\n             '3': 42,\n             '7': 43,\n             '?': 44,\n             '6': 45,\n             '4': 46,\n             '5': 47,\n             '8': 48,\n             '9': 49,\n             '√°': 50,\n             '√±': 51,\n             \"'\": 52,\n             '√©': 53,\n             '‚Ä¶': 54,\n             '√≥': 55,\n             '-': 56,\n             'üòÇ': 57,\n             ')': 58,\n             '(': 59,\n             'Ô∏è': 60,\n             '|': 61,\n             '‚ù§': 62,\n             'üòç': 63,\n             '√∫': 64,\n             '‚Äô': 65,\n             '‚Äú': 66,\n             '‚Äù': 67,\n             '^': 68,\n             '&': 69,\n             '¬ø': 70,\n             'üò≠': 71,\n             '*': 72,\n             '>': 73,\n             '$': 74,\n             ';': 75,\n             'üòÅ': 76,\n             '¬°': 77,\n             'üëå': 78,\n             '„Éª': 79,\n             'üî•': 80,\n             'üèº': 81,\n             'üò±': 82,\n             '‚ò∫': 83,\n             'üö®': 84,\n             'üéâ': 85,\n             'üëè': 86,\n             'üòò': 87,\n             'üòù': 88,\n             'üòé': 89,\n             'üèª': 90,\n             'üèΩ': 91,\n             'üôå': 92,\n             'üíï': 93,\n             'üòä': 94,\n             '~': 95,\n             '‚ô•': 96,\n             '‚Ä¢': 97,\n             '<': 98,\n             'üéµ': 99,\n             'üò©': 100,\n             'üíÄ': 101,\n             '+': 102,\n             'üòú': 103,\n             'üëç': 104,\n             'üòã': 105,\n             'üòí': 106,\n             '‚úå': 107,\n             'üòè': 108,\n             'üò°': 109,\n             '%': 110,\n             'üëó': 111,\n             'üíÉ': 112,\n             'üíã': 113,\n             'üí™': 114,\n             '=': 115,\n             '‚òÄ': 116,\n             'üíÅ': 117,\n             'üòî': 118,\n             'üé∂': 119,\n             'üòâ': 120,\n             'üò≥': 121,\n             'üòå': 122,\n             'ü§î': 123,\n             'üíØ': 124,\n             'üçª': 125,\n             'üëá': 126,\n             'üòà': 127,\n             'üôÑ': 128,\n             'üôã': 129,\n             'üë†': 130,\n             'üå∏': 131,\n             'üòÖ': 132,\n             'üò´': 133,\n             '‚ô°': 134,\n             'üéà': 135,\n             'üíî': 136,\n             'üòÉ': 137,\n             'üôà': 138,\n             '‚ú®': 139,\n             'üòû': 140,\n             'üåª': 141,\n             'üíô': 142,\n             '‚úã': 143,\n             '√º': 144,\n             'üî´': 145,\n             'üò¨': 146,\n             'üòª': 147,\n             'üòê': 148,\n             'üå¥': 149,\n             'üíõ': 150,\n             'üòì': 151,\n             'üéä': 152,\n             'üíú': 153,\n             'üí•': 154,\n             'üòõ': 155,\n             'üòÜ': 156,\n             'üò¥': 157,\n             'üôè': 158,\n             'ü§ò': 159,\n             '‚òÅ': 160,\n             'üåô': 161,\n             '‚Å∞': 162,\n             'üçÅ': 163,\n             'üéÅ': 164,\n             'üëä': 165,\n             'üòë': 166,\n             'üò¢': 167,\n             ']': 168,\n             'üíò': 169,\n             'üòÑ': 170,\n             'üò∞': 171,\n             'üòπ': 172,\n             '‚≠ê': 173,\n             'üå∑': 174,\n             'üéÇ': 175,\n             'üíÑ': 176,\n             'üòï': 177,\n             'ü§ó': 178,\n             'üåº': 179,\n             'üçπ': 180,\n             'üç∫': 181,\n             'üíì': 182,\n             'üíñ': 183,\n             'üì≤': 184,\n             'üôÇ': 185,\n             'üôÜ': 186,\n             '‚úà': 187,\n             '‚ùÑ': 188,\n             'üçÉ': 189,\n             'üèø': 190,\n             'üíÜ': 191,\n             'üò†': 192,\n             'üò∑': 193,\n             'üéÆ': 194,\n             'üí¶': 195,\n             'üí´': 196,\n             'üòü': 197,\n             '¬∞': 198,\n             '‚Äî': 199,\n             '‚îÄ': 200,\n             'üåû': 201,\n             'üé§': 202,\n             'üí∞': 203,\n             'üîù': 204,\n             'üî¥': 205,\n             'üò§': 206,\n             'üò™': 207,\n             'üôÉ': 208,\n             '‚ùó': 209,\n             'üáµ': 210,\n             'üá∑': 211,\n             'üëñ': 212,\n             'üëõ': 213,\n             'üíö': 214,\n             'üí£': 215,\n             'üí®': 216,\n             'üòÄ': 217,\n             'üòñ': 218,\n             '[': 219,\n             '\\\\': 220,\n             '…™': 221,\n             '‚òé': 222,\n             '‚òù': 223,\n             '‚ö°': 224,\n             '‚úñ': 225,\n             'üå≤': 226,\n             'üå∫': 227,\n             'üåæ': 228,\n             'üçÄ': 229,\n             'üéÄ': 230,\n             'üéß': 231,\n             'üê∂': 232,\n             'üëÄ': 233,\n             'üëÖ': 234,\n             'üëâ': 235,\n             'üëî': 236,\n             'üë∂': 237,\n             'üëø': 238,\n             'üíó': 239,\n             'üí§': 240,\n             'üòó': 241,\n             'üòö': 242,\n             'üò®': 243,\n             'üôÄ': 244,\n             'üôä': 245,\n             '¬¥': 246,\n             '√∑': 247,\n             '…ô': 248,\n             '‚òî': 249,\n             '‚òï': 250,\n             '‚õÑ': 251,\n             '‚úä': 252,\n             '‚ù£': 253,\n             'üåä': 254,\n             'üåö': 255,\n             'üëã': 256,\n             'üëë': 257,\n             'üëô': 258,\n             'üëú': 259,\n             'üë≠': 260,\n             'üëØ': 261,\n             'üëº': 262,\n             'üëæ': 263,\n             'üíå': 264,\n             'üíü': 265,\n             'üì©': 266,\n             'üì±': 267,\n             'üì∏': 268,\n             'üòá': 269,\n             'üôç': 270,\n             '¬£': 271,\n             '¬Æ': 272,\n             '‚Äì': 273,\n             '‚Äò': 274,\n             '‚òπ': 275,\n             '‚ö†': 276,\n             '‚õµ': 277,\n             '‚úÖ': 278,\n             'üåà': 279,\n             'üåå': 280,\n             'üåü': 281,\n             'üå§': 282,\n             'üåµ': 283,\n             'üåø': 284,\n             'üçÇ': 285,\n             'üçá': 286,\n             'üç°': 287,\n             'üçæ': 288,\n             'üéÑ': 289,\n             'üêÄ': 290,\n             'üêö': 291,\n             'üê§': 292,\n             'üê•': 293,\n             'üê∏': 294,\n             'üë∞': 295,\n             'üëª': 296,\n             'üíé': 297,\n             'üìç': 298,\n             'üñï': 299,\n             'üò•': 300,\n             'üòÆ': 301,\n             'üõç': 302,\n             'ü¶Ñ': 303,\n             '¬¢': 304,\n             '√ó': 305,\n             '√ß': 306,\n             'Àà': 307,\n             '‚åõ': 308,\n             '‚ï≠': 309,\n             '‚ïÆ': 310,\n             '‚öΩ': 311,\n             'üåé': 312,\n             'üåè': 313,\n             'üå†': 314,\n             'üçí': 315,\n             'üç¶': 316,\n             'üçß': 317,\n             'üç¨': 318,\n             'üéã': 319,\n             'üéì': 320,\n             'üé•': 321,\n             'üèÜ': 322,\n             'üêù': 323,\n             'üê∞': 324,\n             'üê±': 325,\n             'üëé': 326,\n             'üëö': 327,\n             'üíç': 328,\n             'üíê': 329,\n             'üíû': 330,\n             'üîÜ': 331,\n             'üîô': 332,\n             'üî™': 333,\n             'üî∫': 334,\n             'üò¶': 335,\n             'üò∂': 336,\n             'üòΩ': 337,\n             'üôé': 338,\n             '¬´': 339,\n             '¬ª': 340,\n             '¬Ω': 341,\n             '√®': 342,\n             '≈Ñ': 343,\n             '…õ': 344,\n             'Õ°': 345,\n             'ÿ™': 346,\n             '‚Ç¨': 347,\n             '‚àö': 348,\n             '‚è∞': 349,\n             '‚è±': 350,\n             '‚ñ∂': 351,\n             '‚ò¢': 352,\n             '‚öì': 353,\n             '‚öú': 354,\n             '‚ö™': 355,\n             '‚õÖ': 356,\n             '‚úÇ': 357,\n             '‚úè': 358,\n             '‚ú≥': 359,\n             '‚¨á': 360,\n             'üåÄ': 361,\n             'üåç': 362,\n             'üå±': 363,\n             'üå∂': 364,\n             'üçâ': 365,\n             'üçä': 366,\n             'üçç': 367,\n             'üçï': 368,\n             'üç≠': 369,\n             'üç¥': 370,\n             'üç∑': 371,\n             'üéÉ': 372,\n             'üé®': 373,\n             'üé©': 374,\n             'üéº': 375,\n             'üéæ': 376,\n             'üèÉ': 377,\n             'üèä': 378,\n             'üêü': 379,\n             'üê£': 380,\n             'üêß': 381,\n             'üê≥': 382,\n             'üêµ': 383,\n             'üê∑': 384,\n             'üëê': 385,\n             'üëï': 386,\n             'üë¢': 387,\n             'üë®': 388,\n             'üë´': 389,\n             'üíä': 390,\n             'üíè': 391,\n             'üí©': 392,\n             'üí≥': 393,\n             'üí¥': 394,\n             'üí∏': 395,\n             'üìö': 396,\n             'üì¢': 397,\n             'üì•': 398,\n             'üì®': 399,\n             'üì™': 400,\n             'üì∑': 401,\n             'üîÖ': 402,\n             'üîÆ': 403,\n             'üîµ': 404,\n             'üññ': 405,\n             'üôÖ': 406,\n             'üôâ': 407,\n             'üöå': 408,\n             'üöï': 409,\n             'üö¢': 410,\n             'üö∂': 411,\n             'ü§ì': 412,\n             '{': 413,\n             '¬∫': 414,\n             '√†': 415,\n             '√¢': 416,\n             '√§': 417,\n             'ƒì': 418,\n             '≈Ü': 419,\n             '≈õ': 420,\n             ' É': 421,\n             ' ñ': 422,\n             'Õú': 423,\n             '‚É£': 424,\n             '‚è≥': 425,\n             '‚îÇ': 426,\n             '‚ñ™': 427,\n             '‚ñ∫': 428,\n             '‚òÇ': 429,\n             '‚òÖ': 430,\n             '‚ô†': 431,\n             '‚ô©': 432,\n             '‚ô™': 433,\n             '‚ô¨': 434,\n             '‚ö´': 435,\n             '‚öæ': 436,\n             '‚úß': 437,\n             '‚ùå': 438,\n             '„Ä∞': 439,\n             '„ÉÑ': 440,\n             'üÜî': 441,\n             'üá®': 442,\n             'üáÆ': 443,\n             'üá±': 444,\n             'üá≤': 445,\n             'üá∏': 446,\n             'üáπ': 447,\n             'üá∫': 448,\n             'üáΩ': 449,\n             'üåÇ': 450,\n             'üåÉ': 451,\n             'üåÑ': 452,\n             'üåë': 453,\n             'üåù': 454,\n             'üåÆ': 455,\n             'üåØ': 456,\n             'üå≥': 457,\n             'üåπ': 458,\n             'üçé': 459,\n             'üçê': 460,\n             'üçì': 461,\n             'üçî': 462,\n             'üçó': 463,\n             'üçò': 464,\n             'üçö': 465,\n             'üç†': 466,\n             'üç¢': 467,\n             'üç£': 468,\n             'üç•': 469,\n             'üç´': 470,\n             'üç∞': 471,\n             'üç±': 472,\n             'üç≤': 473,\n             'üç∏': 474,\n             'üçº': 475,\n             'üçø': 476,\n             'üéÜ': 477,\n             'üéå': 478,\n             'üé¨': 479,\n             'üé∏': 480,\n             'üèÄ': 481,\n             'üèÅ': 482,\n             'üèà': 483,\n             'üèê': 484,\n             'üèù': 485,\n             'üè†': 486,\n             'üè°': 487,\n             'üè•': 488,\n             'üèæ': 489,\n             'üêñ': 490,\n             'üê¢': 491,\n             'üê¶': 492,\n             'üêØ': 493,\n             'üêª': 494,\n             'üêº': 495,\n             'üëÅ': 496,\n             'üëÜ': 497,\n             'üëì': 498,\n             'üëû': 499,\n             'üë£': 500,\n             'üë§': 501,\n             'üë©': 502,\n             'üë¥': 503,\n             'üë∏': 504,\n             'üëπ': 505,\n             'üë∫': 506,\n             'üëΩ': 507,\n             'üíÇ': 508,\n             'üíâ': 509,\n             'üíë': 510,\n             'üíí': 511,\n             'üíù': 512,\n             'üí≠': 513,\n             'üíÆ': 514,\n             'üíµ': 515,\n             'üí∂': 516,\n             'üí∑': 517,\n             'üíª': 518,\n             'üíº': 519,\n             'üìÄ': 520,\n             'üìÑ': 521,\n             'üìá': 522,\n             'üìí': 523,\n             'üìì': 524,\n             'üìù': 525,\n             'üì´': 526,\n             'üì¨': 527,\n             'üìπ': 528,\n             'üìª': 529,\n             'üîÑ': 530,\n             'üîä': 531,\n             'üîò': 532,\n             'üîú': 533,\n             'üîû': 534,\n             'üî¨': 535,\n             'üî≠': 536,\n             'üï∞': 537,\n             'üñä': 538,\n             'üñå': 539,\n             'üóª': 540,\n             'üóΩ': 541,\n             'üò≤': 542,\n             'üò∫': 543,\n             'üòº': 544,\n             'üòæ': 545,\n             'üöñ': 546,\n             'üöó': 547,\n             'üöô': 548,\n             'üö∑': 549,\n             'üõ≥': 550,\n             'ü§í': 551,\n             'ü¶É': 552,\n             '\\U000fe1d2': 553,\n             '\\U000fe343': 554})"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(dataset)\n",
    "text_field.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['<unk>',\n '<pad>',\n ' ',\n 'e',\n 'a',\n 'o',\n 't',\n 's',\n 'n',\n 'i',\n 'r',\n 'l',\n 'c',\n 'd',\n 'u',\n 'm',\n 'p',\n 'h',\n 'y',\n 'g',\n '.',\n 'b',\n '/',\n 'v',\n 'f',\n 'j',\n ':',\n '@',\n '!',\n 'q',\n 'k',\n 'w',\n '#',\n 'z',\n ',',\n 'x',\n '1',\n '0',\n '2',\n '_',\n '√≠',\n '\"',\n '3',\n '7',\n '?',\n '6',\n '4',\n '5',\n '8',\n '9',\n '√°',\n '√±',\n \"'\",\n '√©',\n '‚Ä¶',\n '√≥',\n '-',\n 'üòÇ',\n ')',\n '(',\n 'Ô∏è',\n '|',\n '‚ù§',\n 'üòç',\n '√∫',\n '‚Äô',\n '‚Äú',\n '‚Äù',\n '^',\n '&',\n '¬ø',\n 'üò≠',\n '*',\n '>',\n '$',\n ';',\n 'üòÅ',\n '¬°',\n 'üëå',\n '„Éª',\n 'üî•',\n 'üèº',\n 'üò±',\n '‚ò∫',\n 'üö®',\n 'üéâ',\n 'üëè',\n 'üòò',\n 'üòù',\n 'üòé',\n 'üèª',\n 'üèΩ',\n 'üôå',\n 'üíï',\n 'üòä',\n '~',\n '‚ô•',\n '‚Ä¢',\n '<',\n 'üéµ',\n 'üò©',\n 'üíÄ',\n '+',\n 'üòú',\n 'üëç',\n 'üòã',\n 'üòí',\n '‚úå',\n 'üòè',\n 'üò°',\n '%',\n 'üëó',\n 'üíÉ',\n 'üíã',\n 'üí™',\n '=',\n '‚òÄ',\n 'üíÅ',\n 'üòî',\n 'üé∂',\n 'üòâ',\n 'üò≥',\n 'üòå',\n 'ü§î',\n 'üíØ',\n 'üçª',\n 'üëá',\n 'üòà',\n 'üôÑ',\n 'üôã',\n 'üë†',\n 'üå∏',\n 'üòÖ',\n 'üò´',\n '‚ô°',\n 'üéà',\n 'üíî',\n 'üòÉ',\n 'üôà',\n '‚ú®',\n 'üòû',\n 'üåª',\n 'üíô',\n '‚úã',\n '√º',\n 'üî´',\n 'üò¨',\n 'üòª',\n 'üòê',\n 'üå¥',\n 'üíõ',\n 'üòì',\n 'üéä',\n 'üíú',\n 'üí•',\n 'üòõ',\n 'üòÜ',\n 'üò¥',\n 'üôè',\n 'ü§ò',\n '‚òÅ',\n 'üåô',\n '‚Å∞',\n 'üçÅ',\n 'üéÅ',\n 'üëä',\n 'üòë',\n 'üò¢',\n ']',\n 'üíò',\n 'üòÑ',\n 'üò∞',\n 'üòπ',\n '‚≠ê',\n 'üå∑',\n 'üéÇ',\n 'üíÑ',\n 'üòï',\n 'ü§ó',\n 'üåº',\n 'üçπ',\n 'üç∫',\n 'üíì',\n 'üíñ',\n 'üì≤',\n 'üôÇ',\n 'üôÜ',\n '‚úà',\n '‚ùÑ',\n 'üçÉ',\n 'üèø',\n 'üíÜ',\n 'üò†',\n 'üò∑',\n 'üéÆ',\n 'üí¶',\n 'üí´',\n 'üòü',\n '¬∞',\n '‚Äî',\n '‚îÄ',\n 'üåû',\n 'üé§',\n 'üí∞',\n 'üîù',\n 'üî¥',\n 'üò§',\n 'üò™',\n 'üôÉ',\n '‚ùó',\n 'üáµ',\n 'üá∑',\n 'üëñ',\n 'üëõ',\n 'üíö',\n 'üí£',\n 'üí®',\n 'üòÄ',\n 'üòñ',\n '[',\n '\\\\',\n '…™',\n '‚òé',\n '‚òù',\n '‚ö°',\n '‚úñ',\n 'üå≤',\n 'üå∫',\n 'üåæ',\n 'üçÄ',\n 'üéÄ',\n 'üéß',\n 'üê∂',\n 'üëÄ',\n 'üëÖ',\n 'üëâ',\n 'üëî',\n 'üë∂',\n 'üëø',\n 'üíó',\n 'üí§',\n 'üòó',\n 'üòö',\n 'üò®',\n 'üôÄ',\n 'üôä',\n '¬¥',\n '√∑',\n '…ô',\n '‚òî',\n '‚òï',\n '‚õÑ',\n '‚úä',\n '‚ù£',\n 'üåä',\n 'üåö',\n 'üëã',\n 'üëë',\n 'üëô',\n 'üëú',\n 'üë≠',\n 'üëØ',\n 'üëº',\n 'üëæ',\n 'üíå',\n 'üíü',\n 'üì©',\n 'üì±',\n 'üì∏',\n 'üòá',\n 'üôç',\n '¬£',\n '¬Æ',\n '‚Äì',\n '‚Äò',\n '‚òπ',\n '‚ö†',\n '‚õµ',\n '‚úÖ',\n 'üåà',\n 'üåå',\n 'üåü',\n 'üå§',\n 'üåµ',\n 'üåø',\n 'üçÇ',\n 'üçá',\n 'üç°',\n 'üçæ',\n 'üéÑ',\n 'üêÄ',\n 'üêö',\n 'üê§',\n 'üê•',\n 'üê∏',\n 'üë∞',\n 'üëª',\n 'üíé',\n 'üìç',\n 'üñï',\n 'üò•',\n 'üòÆ',\n 'üõç',\n 'ü¶Ñ',\n '¬¢',\n '√ó',\n '√ß',\n 'Àà',\n '‚åõ',\n '‚ï≠',\n '‚ïÆ',\n '‚öΩ',\n 'üåé',\n 'üåè',\n 'üå†',\n 'üçí',\n 'üç¶',\n 'üçß',\n 'üç¨',\n 'üéã',\n 'üéì',\n 'üé•',\n 'üèÜ',\n 'üêù',\n 'üê∞',\n 'üê±',\n 'üëé',\n 'üëö',\n 'üíç',\n 'üíê',\n 'üíû',\n 'üîÜ',\n 'üîô',\n 'üî™',\n 'üî∫',\n 'üò¶',\n 'üò∂',\n 'üòΩ',\n 'üôé',\n '¬´',\n '¬ª',\n '¬Ω',\n '√®',\n '≈Ñ',\n '…õ',\n 'Õ°',\n 'ÿ™',\n '‚Ç¨',\n '‚àö',\n '‚è∞',\n '‚è±',\n '‚ñ∂',\n '‚ò¢',\n '‚öì',\n '‚öú',\n '‚ö™',\n '‚õÖ',\n '‚úÇ',\n '‚úè',\n '‚ú≥',\n '‚¨á',\n 'üåÄ',\n 'üåç',\n 'üå±',\n 'üå∂',\n 'üçâ',\n 'üçä',\n 'üçç',\n 'üçï',\n 'üç≠',\n 'üç¥',\n 'üç∑',\n 'üéÉ',\n 'üé®',\n 'üé©',\n 'üéº',\n 'üéæ',\n 'üèÉ',\n 'üèä',\n 'üêü',\n 'üê£',\n 'üêß',\n 'üê≥',\n 'üêµ',\n 'üê∑',\n 'üëê',\n 'üëï',\n 'üë¢',\n 'üë®',\n 'üë´',\n 'üíä',\n 'üíè',\n 'üí©',\n 'üí≥',\n 'üí¥',\n 'üí∏',\n 'üìö',\n 'üì¢',\n 'üì•',\n 'üì®',\n 'üì™',\n 'üì∑',\n 'üîÖ',\n 'üîÆ',\n 'üîµ',\n 'üññ',\n 'üôÖ',\n 'üôâ',\n 'üöå',\n 'üöï',\n 'üö¢',\n 'üö∂',\n 'ü§ì',\n '{',\n '¬∫',\n '√†',\n '√¢',\n '√§',\n 'ƒì',\n '≈Ü',\n '≈õ',\n ' É',\n ' ñ',\n 'Õú',\n '‚É£',\n '‚è≥',\n '‚îÇ',\n '‚ñ™',\n '‚ñ∫',\n '‚òÇ',\n '‚òÖ',\n '‚ô†',\n '‚ô©',\n '‚ô™',\n '‚ô¨',\n '‚ö´',\n '‚öæ',\n '‚úß',\n '‚ùå',\n '„Ä∞',\n '„ÉÑ',\n 'üÜî',\n 'üá®',\n 'üáÆ',\n 'üá±',\n 'üá≤',\n 'üá∏',\n 'üáπ',\n 'üá∫',\n 'üáΩ',\n 'üåÇ',\n 'üåÉ',\n 'üåÑ',\n 'üåë',\n 'üåù',\n 'üåÆ',\n 'üåØ',\n 'üå≥',\n 'üåπ',\n 'üçé',\n 'üçê',\n 'üçì',\n 'üçî',\n 'üçó',\n 'üçò',\n 'üçö',\n 'üç†',\n 'üç¢',\n 'üç£',\n 'üç•',\n 'üç´',\n 'üç∞',\n 'üç±',\n 'üç≤',\n 'üç∏',\n 'üçº',\n 'üçø',\n 'üéÜ',\n 'üéå',\n 'üé¨',\n 'üé∏',\n 'üèÄ',\n 'üèÅ',\n 'üèà',\n 'üèê',\n 'üèù',\n 'üè†',\n 'üè°',\n 'üè•',\n 'üèæ',\n 'üêñ',\n 'üê¢',\n 'üê¶',\n 'üêØ',\n 'üêª',\n 'üêº',\n 'üëÅ',\n 'üëÜ',\n 'üëì',\n 'üëû',\n 'üë£',\n 'üë§',\n 'üë©',\n 'üë¥',\n 'üë∏',\n 'üëπ',\n 'üë∫',\n 'üëΩ',\n 'üíÇ',\n 'üíâ',\n 'üíë',\n 'üíí',\n 'üíù',\n 'üí≠',\n 'üíÆ',\n 'üíµ',\n 'üí∂',\n 'üí∑',\n 'üíª',\n 'üíº',\n 'üìÄ',\n 'üìÑ',\n 'üìá',\n 'üìí',\n 'üìì',\n 'üìù',\n 'üì´',\n 'üì¨',\n 'üìπ',\n 'üìª',\n 'üîÑ',\n 'üîä',\n 'üîò',\n 'üîú',\n 'üîû',\n 'üî¨',\n 'üî≠',\n 'üï∞',\n 'üñä',\n 'üñå',\n 'üóª',\n 'üóΩ',\n 'üò≤',\n 'üò∫',\n 'üòº',\n 'üòæ',\n 'üöñ',\n 'üöó',\n 'üöô',\n 'üö∑',\n 'üõ≥',\n 'ü§í',\n 'ü¶É',\n '\\U000fe1d2',\n '\\U000fe343']"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch = 128\n",
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                          )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "val_iter = torchtext.data.BucketIterator(val,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                        )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=small_batch,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                         )\n",
    "#                                            repeat=True)                   # repeat the iterator for multiple epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<torchtext.data.iterator.BucketIterator object at 0x7f4281474110>\ntensor([[  4,  18,   2,  ...,   4,   6,   5],\n        [ 10,   6,  27,  ...,  21,  37,  54],\n        [ 10,   6,  27,  ...,   7, 220,  54],\n        ...,\n        [ 10,   6,  27,  ...,  12,   4,  20],\n        [ 66,  27,  12,  ...,   9,  10,  67],\n        [ 27,  24,   4,  ...,   7,  44,  28]])\ntensor([0, 1, 1, 2, 1, 2, 0, 2, 0, 1, 1, 2, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1,\n        1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2,\n        1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1,\n        2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 2, 0, 1, 1, 1,\n        0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 2, 0, 2, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 0,\n        1, 1, 1, 2, 2, 0, 1, 0])\ntensor([[24,  3, 11,  ..., 15,  5, 10],\n        [27, 35, 35,  ...,  2,  4, 21],\n        [10,  6, 27,  ...,  7, 18, 54],\n        ...,\n        [10,  6, 27,  ..., 11,  8,  1],\n        [18,  2, 23,  ..., 26, 58,  1],\n        [11,  4,  2,  ..., 16, 24,  1]])\ntensor([2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n        0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,\n        1, 1, 1, 1, 0, 2, 2, 2, 0, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2,\n        2, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 0, 2, 2,\n        1, 1, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 0, 2,\n        2, 1, 1, 1, 1, 1, 1, 2])\ntensor([[10,  6, 27,  ..., 46,  3, 24],\n        [ 7,  9,  2,  ...,  8,  5, 28],\n        [10,  6, 27,  ..., 15,  9, 35],\n        ...,\n        [32,  6, 17,  ..., 28, 28,  1],\n        [25, 14,  7,  ...,  5, 20,  1],\n        [ 5, 21, 11,  ...,  6,  4,  1]])\ntensor([2, 0, 1, 2, 0, 2, 1, 1, 1, 2, 0, 2, 0, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2,\n        1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1,\n        1, 2, 2, 1, 1, 0, 1, 1, 2, 1, 2, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 2,\n        1, 2, 1, 0, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 2, 2, 1, 0, 0,\n        2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1,\n        2, 2, 2, 2, 0, 1, 2, 1])\ntensor([[  8,   3,  12,  ..., 225,  58,  44],\n        [  5,  25,   4,  ...,  76,  62,  60],\n        [ 27,  15,   9,  ...,  23,   3,   7],\n        ...,\n        [ 15,  18,   2,  ...,  14,  13,   1],\n        [ 41,  27,  25,  ...,   7,  87,   1],\n        [ 10,   6,  27,  ...,   6, 123,   1]])\ntensor([2, 1, 2, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2,\n        1, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2, 2, 0, 1, 0, 1, 1, 1, 1,\n        2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0,\n        1, 0, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 1, 0, 1, 2, 1, 1, 0, 1,\n        0, 1, 1, 1, 2, 0, 0, 2, 1, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1,\n        1, 1, 0, 2, 1, 1, 1, 1])\ntensor([[27, 16,  4,  ..., 18, 18, 28],\n        [15,  3,  2,  ...,  5, 58, 20],\n        [27, 39,  9,  ..., 11,  5, 11],\n        ...,\n        [ 5, 30,  3,  ..., 19,  1,  1],\n        [24,  4, 15,  ..., 33,  1,  1],\n        [32, 12,  5,  ..., 33,  1,  1]])\ntensor([1, 2, 1, 1, 2, 2, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 2, 0, 2, 1, 0,\n        0, 1, 1, 0, 1, 2, 2, 1, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2,\n        1, 1, 1, 2, 1, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n        1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n        0, 2, 2, 1, 1, 1, 1, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1,\n        1, 1, 2, 1, 1, 2, 1, 2])\ntensor([[  3,   8,   4,  ..., 171, 171, 132],\n        [ 32,   6,  21,  ...,  45,  19,   5],\n        [  8,   3,   5,  ...,  18,  18,  18],\n        ...,\n        [  6,   3,   8,  ...,   3,  20,   1],\n        [ 18,   5,  14,  ...,   7,  20,   1],\n        [ 10,   6,  27,  ...,   8,   8,   1]])\ntensor([0, 1, 1, 0, 2, 1, 2, 1, 1, 0, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 0, 1, 2, 2,\n        1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 0, 0, 2, 1, 1, 2, 2, 2, 2, 0,\n        1, 1, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 2, 1, 0, 0, 2, 1, 2, 2, 1, 1, 2, 0,\n        1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 1, 2, 2, 1, 1, 2, 0, 1, 2, 1, 1, 0, 1,\n        1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1,\n        1, 2, 1, 0, 1, 0, 1, 1])\ntensor([[ 10,   5,   4,  ...,   7, 201, 141],\n        [ 27,  12,  10,  ...,  75,   2,  21],\n        [ 27,  24,   4,  ...,  11,   3,  18],\n        ...,\n        [ 27,  12,  10,  ...,   4,   1,   1],\n        [ 18,   4,   2,  ...,   7,   1,   1],\n        [  6,  17,   3,  ...,   8,   1,   1]])\ntensor([2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 0, 1, 1, 2, 2, 1, 2, 0, 1, 1, 1, 0,\n        1, 1, 1, 2, 0, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 0, 1, 1, 0, 2, 2, 1,\n        2, 1, 1, 2, 1, 0, 2, 0, 1, 0, 1, 1, 2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0,\n        1, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n        1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 1, 0, 1, 1, 0, 2, 1, 1, 0, 2, 2, 1, 2, 1,\n        1, 1, 2, 2, 1, 1, 2, 1])\ntensor([[ 17,   5,  10,  ..., 175, 333, 333],\n        [ 27,  12,   4,  ...,   4,  19,   3],\n        [  6,  31,   2,  ...,  35,   6,  20],\n        ...,\n        [ 10,   6,  27,  ...,   8,   7,   1],\n        [ 32,   6,  10,  ...,   9,  12,   1],\n        [ 27,  25,   4,  ...,  18,   3,   1]])\ntensor([1, 1, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 2, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1,\n        2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 2, 1, 1, 2, 1, 2, 0, 1, 1,\n        1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 0, 1, 2, 2, 1, 2, 2, 2, 1, 1, 0, 1, 1, 2,\n        1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0,\n        1, 0, 2, 2, 1, 1, 0, 2, 2, 2, 0, 1, 1, 2, 0, 1, 1, 2, 2, 1, 2, 2, 0, 1,\n        2, 1, 0, 1, 0, 1, 1, 2])\ntensor([[ 9,  2, 31,  ..., 12,  3, 28],\n        [ 7,  9,  2,  ...,  5, 23, 31],\n        [ 8,  5,  2,  ..., 20, 20, 20],\n        ...,\n        [13,  5, 15,  ...,  6,  5,  1],\n        [27, 12, 17,  ..., 57, 78,  1],\n        [ 3, 11,  2,  ..., 35, 13,  1]])\ntensor([1, 2, 0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 2, 1, 1, 2, 1,\n        0, 2, 0, 2, 2, 1, 1, 2, 2, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1,\n        2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 0, 1, 2, 2, 0, 1, 1, 2, 1, 0, 0, 0, 1,\n        0, 0, 1, 0, 2, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0, 2, 1, 0, 0, 1, 1, 1,\n        1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 2, 0, 2, 1, 1, 1,\n        1, 2, 2, 2, 1, 1, 1, 2])\ntensor([[ 15,   3,   2,  ...,   2,  19,   5],\n        [ 11,   3,   2,  ...,   4,  10, 106],\n        [  3,   8,   4,  ...,   5,   8,  20],\n        ...,\n        [ 15,   3,  13,  ...,  30,  30,   1],\n        [ 27,   4,   8,  ...,   3,   3,   1],\n        [ 15,   4,  51,  ...,  26,  58,   1]])\ntensor([1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 0, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1,\n        1, 0, 0, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2, 2, 2, 2, 2,\n        1, 2, 1, 1, 2, 2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n        1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1,\n        1, 2, 1, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1, 1,\n        2, 2, 0, 2, 2, 1, 2, 2])\n"
    }
   ],
   "source": [
    "print(train_iter)\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(batch.text)\n",
    "    print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, bidir=False, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(2*n_hidden, n_output)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out[:,-1])\n",
    "        log_probs = F.log_softmax(out)\n",
    "\n",
    "        # sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        # sigmoid_out = fc_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        # sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        return log_probs\n",
    "        # return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(logits, labels):\n",
    "    correct, total = 0, 0\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "#     print(predicted, labels)\n",
    "#     print(predicted.shape, labels.shape)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        h = model.init_hidden(len(batch))\n",
    "        batch.text = batch.text.to(device)\n",
    "        # predictions, _ = model(batch.text)\n",
    "        predictions = model(batch.text)\n",
    "        # predictions = predictions.squeeze(1)\n",
    "        # print(batch.text.shape, predictions.shape, batch.label.shape)\n",
    "\n",
    "        # target = torch.tensor(batch.label, dtype=torch.float, device=device)\n",
    "        loss = criterion(predictions, batch.label.to(device))\n",
    "\n",
    "        acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-121-358ba46187ca>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-121-358ba46187ca>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    predictions = model(batch.text))\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text))\n",
    "\n",
    "            loss = criterion(predictions, batch.label.to(device))\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label.to(device))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, config):\n",
    "    train, val, test = dataset.split(split_ratio=[0.8,0.1,0.1])\n",
    "    exp_name = config[\"NAME\"]\n",
    "    test_iter = torchtext.data.BucketIterator(test,\n",
    "                                           batch_size=len(test),\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True)        # sort within each batch\n",
    "    print(\"BEST METRICS VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "\n",
    "    print(\"BEST METRICS TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            h = model.init_hidden(len(batch))\n",
    "            batch.text = batch.text.to(device)\n",
    "            predictions = model(batch.text)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            precision, recall, f_1, _ = precision_recall_fscore_support(batch.label.cpu(),preds.cpu(),average='macro')\n",
    "            print(f'Test f1: {f_1:.3f} | Test Prec: {recall*100:.2f}% | Test Recall: {recall*100:.2f}% ')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, config):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    exp_name = config[\"NAME\"]\n",
    "    print(\"TEST VALID\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    print(\"TEST TRAIN\")\n",
    "    model.load_state_dict(torch.load('{}-valid.pt'.format(exp_name)))\n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "    print(f'\\t Test. Loss: {valid_loss:.3f} |  Test. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(model, config):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = config[\"LR\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = config[\"N_EPOCHS\"]\n",
    "    exp_name = config[\"NAME\"]\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    print(\"BEGIN TRAINING\")\n",
    "    print(\"-\"*50)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(\"SAVED VALID\")\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-valid.pt'.format(exp_name))\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            print(\"SAVED TRAIN\")\n",
    "            best_train_loss = valid_loss\n",
    "            torch.save(model.state_dict(), '{}-train.pt'.format(exp_name))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning Sentiment Analysis, Random Embeddings, spanglish\n{   'BIDIR': True,\n    'DRPOUT': 0.5,\n    'EMB_TRAIN': True,\n    'LR': 0.01,\n    'NAME': 'LSTM_RandomEmbeddings_spanglish',\n    'N_EMBED': 300,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 100,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nSAVED VALID\nSAVED TRAIN\nEpoch: 01 | Epoch Time: 0m 2s\n\tTrain Loss: 1.158 | Train Acc: 46.76%\n\t Val. Loss: 1.017 |  Val. Acc: 49.70%\nSAVED VALID\nEpoch: 02 | Epoch Time: 0m 2s\n\tTrain Loss: 1.018 | Train Acc: 48.81%\n\t Val. Loss: 1.008 |  Val. Acc: 49.79%\nSAVED VALID\nSAVED TRAIN\nEpoch: 03 | Epoch Time: 0m 2s\n\tTrain Loss: 1.004 | Train Acc: 49.55%\n\t Val. Loss: 0.994 |  Val. Acc: 49.61%\nSAVED VALID\nEpoch: 04 | Epoch Time: 0m 2s\n\tTrain Loss: 0.995 | Train Acc: 49.42%\n\t Val. Loss: 0.978 |  Val. Acc: 50.53%\nSAVED VALID\nSAVED TRAIN\nEpoch: 05 | Epoch Time: 0m 2s\n\tTrain Loss: 0.983 | Train Acc: 49.81%\n\t Val. Loss: 0.974 |  Val. Acc: 49.96%\nEpoch: 06 | Epoch Time: 0m 2s\n\tTrain Loss: 0.978 | Train Acc: 50.02%\n\t Val. Loss: 0.980 |  Val. Acc: 49.13%\nSAVED VALID\nSAVED TRAIN\nEpoch: 07 | Epoch Time: 0m 2s\n\tTrain Loss: 0.974 | Train Acc: 49.74%\n\t Val. Loss: 0.970 |  Val. Acc: 49.10%\nSAVED TRAIN\nEpoch: 08 | Epoch Time: 0m 2s\n\tTrain Loss: 0.967 | Train Acc: 50.38%\n\t Val. Loss: 0.974 |  Val. Acc: 50.81%\nSAVED VALID\nSAVED TRAIN\nEpoch: 09 | Epoch Time: 0m 2s\n\tTrain Loss: 0.963 | Train Acc: 50.63%\n\t Val. Loss: 0.969 |  Val. Acc: 49.86%\nSAVED VALID\nSAVED TRAIN\nEpoch: 10 | Epoch Time: 0m 2s\n\tTrain Loss: 0.955 | Train Acc: 50.72%\n\t Val. Loss: 0.964 |  Val. Acc: 50.65%\nSAVED VALID\nSAVED TRAIN\nEpoch: 11 | Epoch Time: 0m 2s\n\tTrain Loss: 0.953 | Train Acc: 51.10%\n\t Val. Loss: 0.962 |  Val. Acc: 50.11%\nSAVED TRAIN\nEpoch: 12 | Epoch Time: 0m 2s\n\tTrain Loss: 0.955 | Train Acc: 50.76%\n\t Val. Loss: 0.973 |  Val. Acc: 48.10%\nSAVED VALID\nSAVED TRAIN\nEpoch: 13 | Epoch Time: 0m 2s\n\tTrain Loss: 0.956 | Train Acc: 50.90%\n\t Val. Loss: 0.959 |  Val. Acc: 51.12%\nSAVED VALID\nSAVED TRAIN\nEpoch: 14 | Epoch Time: 0m 2s\n\tTrain Loss: 0.952 | Train Acc: 50.83%\n\t Val. Loss: 0.954 |  Val. Acc: 50.82%\nSAVED TRAIN\nEpoch: 15 | Epoch Time: 0m 2s\n\tTrain Loss: 0.945 | Train Acc: 51.34%\n\t Val. Loss: 0.979 |  Val. Acc: 45.16%\nSAVED TRAIN\nEpoch: 16 | Epoch Time: 0m 2s\n\tTrain Loss: 0.947 | Train Acc: 51.12%\n\t Val. Loss: 0.959 |  Val. Acc: 48.26%\nSAVED TRAIN\nEpoch: 17 | Epoch Time: 0m 2s\n\tTrain Loss: 0.942 | Train Acc: 51.07%\n\t Val. Loss: 0.955 |  Val. Acc: 49.92%\nSAVED TRAIN\nEpoch: 18 | Epoch Time: 0m 2s\n\tTrain Loss: 0.945 | Train Acc: 51.11%\n\t Val. Loss: 0.955 |  Val. Acc: 51.45%\nSAVED VALID\nSAVED TRAIN\nEpoch: 19 | Epoch Time: 0m 2s\n\tTrain Loss: 0.940 | Train Acc: 51.74%\n\t Val. Loss: 0.953 |  Val. Acc: 49.58%\nSAVED VALID\nSAVED TRAIN\nEpoch: 20 | Epoch Time: 0m 2s\n\tTrain Loss: 0.940 | Train Acc: 51.34%\n\t Val. Loss: 0.948 |  Val. Acc: 51.34%\nSAVED TRAIN\nEpoch: 21 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 52.46%\n\t Val. Loss: 0.961 |  Val. Acc: 49.21%\nSAVED TRAIN\nEpoch: 22 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.93%\n\t Val. Loss: 0.948 |  Val. Acc: 50.52%\nSAVED TRAIN\nEpoch: 23 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.89%\n\t Val. Loss: 0.976 |  Val. Acc: 46.11%\nSAVED TRAIN\nEpoch: 24 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.35%\n\t Val. Loss: 0.963 |  Val. Acc: 48.91%\nSAVED TRAIN\nEpoch: 25 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 52.12%\n\t Val. Loss: 0.958 |  Val. Acc: 48.52%\nSAVED TRAIN\nEpoch: 26 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.68%\n\t Val. Loss: 0.965 |  Val. Acc: 50.51%\nSAVED TRAIN\nEpoch: 27 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.90%\n\t Val. Loss: 0.964 |  Val. Acc: 51.24%\nSAVED TRAIN\nEpoch: 28 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.52%\n\t Val. Loss: 0.970 |  Val. Acc: 49.58%\nSAVED TRAIN\nEpoch: 29 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.19%\n\t Val. Loss: 0.966 |  Val. Acc: 50.21%\nSAVED TRAIN\nEpoch: 30 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.80%\n\t Val. Loss: 0.974 |  Val. Acc: 50.85%\nSAVED TRAIN\nEpoch: 31 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.44%\n\t Val. Loss: 0.968 |  Val. Acc: 50.53%\nSAVED TRAIN\nEpoch: 32 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.64%\n\t Val. Loss: 0.965 |  Val. Acc: 49.67%\nSAVED TRAIN\nEpoch: 33 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.76%\n\t Val. Loss: 0.966 |  Val. Acc: 50.23%\nSAVED TRAIN\nEpoch: 34 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 52.14%\n\t Val. Loss: 0.963 |  Val. Acc: 50.50%\nSAVED TRAIN\nEpoch: 35 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.63%\n\t Val. Loss: 0.955 |  Val. Acc: 50.42%\nSAVED TRAIN\nEpoch: 36 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.11%\n\t Val. Loss: 0.962 |  Val. Acc: 47.81%\nSAVED TRAIN\nEpoch: 37 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 52.06%\n\t Val. Loss: 0.966 |  Val. Acc: 51.68%\nSAVED TRAIN\nEpoch: 38 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.63%\n\t Val. Loss: 0.971 |  Val. Acc: 51.46%\nSAVED TRAIN\nEpoch: 39 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.21%\n\t Val. Loss: 0.979 |  Val. Acc: 49.60%\nSAVED TRAIN\nEpoch: 40 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 52.78%\n\t Val. Loss: 0.967 |  Val. Acc: 50.05%\nSAVED TRAIN\nEpoch: 41 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.93%\n\t Val. Loss: 0.983 |  Val. Acc: 50.33%\nSAVED TRAIN\nEpoch: 42 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 52.39%\n\t Val. Loss: 0.964 |  Val. Acc: 49.34%\nSAVED TRAIN\nEpoch: 43 | Epoch Time: 0m 2s\n\tTrain Loss: 0.927 | Train Acc: 52.31%\n\t Val. Loss: 1.004 |  Val. Acc: 47.11%\nSAVED TRAIN\nEpoch: 44 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.87%\n\t Val. Loss: 0.958 |  Val. Acc: 50.90%\nSAVED TRAIN\nEpoch: 45 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.60%\n\t Val. Loss: 0.962 |  Val. Acc: 50.72%\nSAVED TRAIN\nEpoch: 46 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.60%\n\t Val. Loss: 0.961 |  Val. Acc: 51.21%\nSAVED TRAIN\nEpoch: 47 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.05%\n\t Val. Loss: 0.969 |  Val. Acc: 50.27%\nSAVED TRAIN\nEpoch: 48 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.85%\n\t Val. Loss: 0.990 |  Val. Acc: 49.24%\nSAVED TRAIN\nEpoch: 49 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 51.84%\n\t Val. Loss: 0.978 |  Val. Acc: 48.66%\nSAVED TRAIN\nEpoch: 50 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.76%\n\t Val. Loss: 0.974 |  Val. Acc: 49.85%\nSAVED TRAIN\nEpoch: 51 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.59%\n\t Val. Loss: 0.964 |  Val. Acc: 50.76%\nSAVED TRAIN\nEpoch: 52 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.04%\n\t Val. Loss: 0.986 |  Val. Acc: 50.12%\nSAVED TRAIN\nEpoch: 53 | Epoch Time: 0m 3s\n\tTrain Loss: 0.934 | Train Acc: 52.42%\n\t Val. Loss: 0.957 |  Val. Acc: 49.26%\nSAVED TRAIN\nEpoch: 54 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.48%\n\t Val. Loss: 0.983 |  Val. Acc: 47.70%\nSAVED TRAIN\nEpoch: 55 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.12%\n\t Val. Loss: 0.981 |  Val. Acc: 46.07%\nSAVED TRAIN\nEpoch: 56 | Epoch Time: 0m 2s\n\tTrain Loss: 0.924 | Train Acc: 52.47%\n\t Val. Loss: 0.969 |  Val. Acc: 47.75%\nSAVED TRAIN\nEpoch: 57 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 52.17%\n\t Val. Loss: 0.983 |  Val. Acc: 45.34%\nSAVED TRAIN\nEpoch: 58 | Epoch Time: 0m 2s\n\tTrain Loss: 0.927 | Train Acc: 52.29%\n\t Val. Loss: 0.976 |  Val. Acc: 46.82%\nSAVED TRAIN\nEpoch: 59 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.71%\n\t Val. Loss: 0.995 |  Val. Acc: 47.34%\nSAVED TRAIN\nEpoch: 60 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.29%\n\t Val. Loss: 0.975 |  Val. Acc: 50.04%\nSAVED TRAIN\nEpoch: 61 | Epoch Time: 0m 2s\n\tTrain Loss: 0.926 | Train Acc: 52.58%\n\t Val. Loss: 0.963 |  Val. Acc: 49.44%\nSAVED TRAIN\nEpoch: 62 | Epoch Time: 0m 2s\n\tTrain Loss: 0.929 | Train Acc: 52.08%\n\t Val. Loss: 0.997 |  Val. Acc: 46.55%\nSAVED TRAIN\nEpoch: 63 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.62%\n\t Val. Loss: 0.977 |  Val. Acc: 48.27%\nSAVED TRAIN\nEpoch: 64 | Epoch Time: 0m 2s\n\tTrain Loss: 0.926 | Train Acc: 52.12%\n\t Val. Loss: 0.974 |  Val. Acc: 49.68%\nSAVED TRAIN\nEpoch: 65 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 52.26%\n\t Val. Loss: 0.988 |  Val. Acc: 49.51%\nSAVED TRAIN\nEpoch: 66 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.76%\n\t Val. Loss: 0.982 |  Val. Acc: 48.22%\nSAVED TRAIN\nEpoch: 67 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 52.04%\n\t Val. Loss: 0.971 |  Val. Acc: 49.39%\nSAVED TRAIN\nEpoch: 68 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 51.89%\n\t Val. Loss: 0.979 |  Val. Acc: 47.89%\nSAVED TRAIN\nEpoch: 69 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 50.91%\n\t Val. Loss: 0.969 |  Val. Acc: 50.43%\nSAVED TRAIN\nEpoch: 70 | Epoch Time: 0m 3s\n\tTrain Loss: 0.929 | Train Acc: 51.61%\n\t Val. Loss: 0.987 |  Val. Acc: 50.55%\nSAVED TRAIN\nEpoch: 71 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 51.30%\n\t Val. Loss: 0.983 |  Val. Acc: 49.58%\nSAVED TRAIN\nEpoch: 72 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.66%\n\t Val. Loss: 0.975 |  Val. Acc: 48.51%\nSAVED TRAIN\nEpoch: 73 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 51.69%\n\t Val. Loss: 0.965 |  Val. Acc: 50.16%\nSAVED TRAIN\nEpoch: 74 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 50.93%\n\t Val. Loss: 0.972 |  Val. Acc: 46.93%\nSAVED TRAIN\nEpoch: 75 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.83%\n\t Val. Loss: 0.968 |  Val. Acc: 51.08%\nSAVED TRAIN\nEpoch: 76 | Epoch Time: 0m 2s\n\tTrain Loss: 0.928 | Train Acc: 52.18%\n\t Val. Loss: 0.986 |  Val. Acc: 45.89%\nSAVED TRAIN\nEpoch: 77 | Epoch Time: 0m 2s\n\tTrain Loss: 0.930 | Train Acc: 52.10%\n\t Val. Loss: 0.979 |  Val. Acc: 49.32%\nSAVED TRAIN\nEpoch: 78 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.84%\n\t Val. Loss: 0.965 |  Val. Acc: 49.71%\nSAVED TRAIN\nEpoch: 79 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.79%\n\t Val. Loss: 0.972 |  Val. Acc: 48.83%\nSAVED TRAIN\nEpoch: 80 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.77%\n\t Val. Loss: 0.957 |  Val. Acc: 50.96%\nSAVED TRAIN\nEpoch: 81 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.72%\n\t Val. Loss: 0.971 |  Val. Acc: 49.54%\nSAVED TRAIN\nEpoch: 82 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.86%\n\t Val. Loss: 0.982 |  Val. Acc: 47.81%\nSAVED TRAIN\nEpoch: 83 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 51.56%\n\t Val. Loss: 0.980 |  Val. Acc: 50.46%\nSAVED TRAIN\nEpoch: 84 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.22%\n\t Val. Loss: 0.976 |  Val. Acc: 50.26%\nSAVED TRAIN\nEpoch: 85 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.54%\n\t Val. Loss: 0.977 |  Val. Acc: 51.13%\nSAVED TRAIN\nEpoch: 86 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.02%\n\t Val. Loss: 0.971 |  Val. Acc: 49.48%\nSAVED TRAIN\nEpoch: 87 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.49%\n\t Val. Loss: 0.975 |  Val. Acc: 50.50%\nSAVED TRAIN\nEpoch: 88 | Epoch Time: 0m 2s\n\tTrain Loss: 0.938 | Train Acc: 51.45%\n\t Val. Loss: 0.977 |  Val. Acc: 49.67%\nSAVED TRAIN\nEpoch: 89 | Epoch Time: 0m 2s\n\tTrain Loss: 0.931 | Train Acc: 52.20%\n\t Val. Loss: 0.975 |  Val. Acc: 47.89%\nSAVED TRAIN\nEpoch: 90 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.36%\n\t Val. Loss: 0.982 |  Val. Acc: 51.30%\nSAVED TRAIN\nEpoch: 91 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.15%\n\t Val. Loss: 0.984 |  Val. Acc: 50.52%\nSAVED TRAIN\nEpoch: 92 | Epoch Time: 0m 2s\n\tTrain Loss: 0.937 | Train Acc: 51.67%\n\t Val. Loss: 0.973 |  Val. Acc: 50.35%\nSAVED TRAIN\nEpoch: 93 | Epoch Time: 0m 2s\n\tTrain Loss: 0.933 | Train Acc: 51.87%\n\t Val. Loss: 0.982 |  Val. Acc: 50.69%\nSAVED TRAIN\nEpoch: 94 | Epoch Time: 0m 2s\n\tTrain Loss: 0.935 | Train Acc: 51.99%\n\t Val. Loss: 0.976 |  Val. Acc: 51.11%\nSAVED TRAIN\nEpoch: 95 | Epoch Time: 0m 2s\n\tTrain Loss: 0.936 | Train Acc: 51.68%\n\t Val. Loss: 0.979 |  Val. Acc: 49.36%\nSAVED TRAIN\nEpoch: 96 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.68%\n\t Val. Loss: 0.994 |  Val. Acc: 47.72%\nSAVED TRAIN\nEpoch: 97 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 51.64%\n\t Val. Loss: 0.981 |  Val. Acc: 49.83%\nSAVED TRAIN\nEpoch: 98 | Epoch Time: 0m 2s\n\tTrain Loss: 0.938 | Train Acc: 51.69%\n\t Val. Loss: 0.982 |  Val. Acc: 49.68%\nSAVED TRAIN\nEpoch: 99 | Epoch Time: 0m 2s\n\tTrain Loss: 0.934 | Train Acc: 52.22%\n\t Val. Loss: 0.974 |  Val. Acc: 50.40%\nSAVED TRAIN\nEpoch: 100 | Epoch Time: 0m 2s\n\tTrain Loss: 0.932 | Train Acc: 51.72%\n\t Val. Loss: 0.979 |  Val. Acc: 49.21%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTEST VALID\n\t Test. Loss: 0.941 |  Test. Acc: 52.34%\nTEST TRAIN\n\t Test. Loss: 0.940 |  Test. Acc: 52.11%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\nGET METRICS\n--------------------------------------------------\nBEST METRICS VALID\nTest f1: 0.429 | Test Prec: 42.41% | Test Recall: 42.41% \nBEST METRICS TRAIN\nTest f1: 0.429 | Test Prec: 42.41% | Test Recall: 42.41% \n"
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(\"-\"*50)\n",
    "print(\"Running Sentiment Analysis, Random Embeddings, spanglish\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_RandomEmbeddings_spanglish\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : 300,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_HIDDEN\" : 100,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"BIDIR\": True,\n",
    "    \"DRPOUT\": 0.5,\n",
    "    \"LR\": 0.01\n",
    "}\n",
    "model = SentimentLSTM(config[\"N_VOCAB\"],config[\"N_EMBED\"], config[\"N_HIDDEN\"], config[\"N_OUTPUT\"], config[\"N_LAYERS\"], config[\"BIDIR\"], config[\"DRPOUT\"])\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)\n",
    "print(\"GET METRICS\")\n",
    "print(\"-\"*50)\n",
    "get_metrics(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = config[\"N_VOCAB\"]     # number of unique words in vocabulary\n",
    "        self.n_layers = config[\"N_LAYERS\"]   # number of LSTM layers \n",
    "        self.n_hidden = config[\"N_HIDDEN\"]   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.n_vocab, config[\"N_EMBED\"])\n",
    "        self.embedding.weight.data = torch.eye(self.n_vocab)\n",
    "        # make embedding untrainable\n",
    "        if not config[\"EMB_TRAIN\"]:\n",
    "            self.embedding.weight.requires_grad=False\n",
    "        self.lstm = nn.LSTM(config[\"N_EMBED\"], self.n_hidden, self.n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(self.n_hidden, config[\"N_OUTPUT\"])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Untrainable\n{   'EMB_TRAIN': False,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Untrainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 1,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 49.86%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Untrainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Untrainable\",\n",
    "    \"N_EPOCHS\": 1,\n",
    "    \"EMB_TRAIN\": False,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--------------------------------------------------\nRunning LSTM, One Hot Embeddings, Trainable\n{   'EMB_TRAIN': True,\n    'LR': 0.001,\n    'NAME': 'LSTM_OneHot_Trainable',\n    'N_EMBED': 555,\n    'N_EPOCHS': 100,\n    'N_HIDDEN': 512,\n    'N_LAYERS': 2,\n    'N_OUTPUT': 3,\n    'N_VOCAB': 555}\n--------------------------------------------------\nBEGIN TRAINING\n--------------------------------------------------\nEpoch: 01 | Epoch Time: 0m 9s\n\tTrain Loss: 0.694 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.82%\nEpoch: 02 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.07%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 03 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 50.08%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 04 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 50.02%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 05 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.78%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 06 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 49.20%\n\t Val. Loss: 0.693 |  Val. Acc: 49.79%\nEpoch: 07 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 48.80%\n\t Val. Loss: 0.693 |  Val. Acc: 49.81%\nEpoch: 08 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 46.21%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 09 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 44.87%\n\t Val. Loss: 0.693 |  Val. Acc: 49.80%\nEpoch: 10 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 44.31%\n\t Val. Loss: 0.693 |  Val. Acc: 49.78%\nEpoch: 11 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 41.84%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 12 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 40.35%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 13 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.64%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 14 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 37.10%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 15 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 16 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 35.21%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 17 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 34.53%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 18 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.87%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 19 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.33%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 20 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.41%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 21 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.17%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 22 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.15%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 23 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.02%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 24 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.90%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 25 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 33.00%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 26 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 27 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.96%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 28 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.93%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 29 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 30 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.94%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 31 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 32 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 33 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 34 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 35 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 36 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 37 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 38 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 39 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 40 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 41 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 42 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 43 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 44 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 45 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 46 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 47 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 48 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 49 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 50 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 51 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 52 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 53 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 54 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 55 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 56 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 57 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 58 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 59 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 60 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 61 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 62 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 63 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 64 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 65 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 66 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 67 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 68 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 69 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 70 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 71 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 72 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 73 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 74 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 75 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 76 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 77 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.41%\nEpoch: 78 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 79 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 80 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 81 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 82 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 83 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 84 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 85 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 86 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 87 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 88 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 89 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 90 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 91 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 92 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.45%\nEpoch: 93 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.42%\nEpoch: 94 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 95 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 96 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 97 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 98 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\nEpoch: 99 | Epoch Time: 0m 8s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.44%\nEpoch: 100 | Epoch Time: 0m 9s\n\tTrain Loss: 0.693 | Train Acc: 32.92%\n\t Val. Loss: 0.693 |  Val. Acc: 33.43%\n--------------------------------------------------\nEND TRAINING\n--------------------------------------------------\nSTART TESTING\n--------------------------------------------------\nTest Loss: 0.693 | Test Acc: 34.29%\n--------------------------------------------------\nEND TESTING\n--------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"-\"*50)\n",
    "print(\"Running LSTM, One Hot Embeddings, Trainable\")\n",
    "\n",
    "config = {\n",
    "    \"NAME\": \"LSTM_OneHot_Trainable\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"EMB_TRAIN\": True,\n",
    "    \"N_VOCAB\": len(text_field.vocab.itos),\n",
    "    \"N_EMBED\" : len(text_field.vocab.itos),\n",
    "    \"N_HIDDEN\" : 512,\n",
    "    \"N_OUTPUT\" : 3,\n",
    "    \"N_LAYERS\" : 2,\n",
    "    \"LR\": 0.001\n",
    "}\n",
    "model = OneHotLSTM(config)\n",
    "pp.pprint(config)\n",
    "print(\"-\"*50)\n",
    "run_loop(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TRAINING\")\n",
    "print(\"-\"*50)\n",
    "print(\"START TESTING\")\n",
    "print(\"-\"*50)\n",
    "test(model, config)\n",
    "print(\"-\"*50)\n",
    "print(\"END TESTING\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        # self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.lstm = nn.GRU(n_embed, n_hidden, n_layers, batch_first = True, bidirectional=True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        batch_size = len(input_words)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1].view([batch_size,1])               # (batch_size, 1)\n",
    "        # print(\"this\",sigmoid_last.shape, sigmoid_out.shape, input_words.shape)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}